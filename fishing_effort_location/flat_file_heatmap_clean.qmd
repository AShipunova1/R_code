---
title: "Fishing effort locations heatmap"
---


```{r no cache setup, results='hide', message=FALSE, warning=FALSE, cache=FALSE, include=FALSE}
library(knitr)
```

# Prepare data
##### Current file: useful_functions_module.r 

```{r Current file: useful_functions_module.r}
##### Current file: useful_functions_module.r ----

# nolint: commented_code_linter
# useful functions

```
```{r start functions }
# How to use:
# my_paths <- set_work_dir()

#install.packages("tidyverse")
## Load the 'tidyverse' library, which is a collection of R packages for data manipulation and visualization.
library(tidyverse)

## Load the 'magrittr' library, which provides piping data and functions.
library(magrittr)

## Load the 'readxl' library, used for reading Excel (.xlsx) files.
library(readxl)

## Load the 'rbenchmark' library, which is used for benchmarking code performance.
library(rbenchmark)

## Load the 'ROracle' library, which provides an interface for working with Oracle databases in R.
library(ROracle)

## Load the 'tictoc' library, which allows measuring code execution time.
library(tictoc)

## Do not show warnings about groups
options(dplyr.summarise.inform = FALSE)
## Turn off the scientific notation
options(scipen = 999)

## Use my function in case we want to change the case in all functions
my_headers_case_function <- tolower

## current user name
get_username <- function(){
    return(as.character(Sys.info()["user"]))
}
```

## set working directories in useful functions 

```{r set working directories in useful functions}
## set working directories in useful functions ----

## change main_r_dir, in_dir, out_dir, git_r_dir to your local environment
  ## then you can use it in the code like my_paths$input etc.
set_work_dir <- function() {

  ## Set the working directory to the user's home directory (~)
  setwd("~/")
  base_dir <- getwd()

  ## Initialize 'add_dir' as an empty string (for others)
  add_dir <- ""

  ## Check if the username is "anna.shipunova" (Anna's computer)
  if (get_username() == "anna.shipunova") {
    ## Set 'add_dir' to a specific directory path for Anna
    add_dir <- "R_files_local/test_dir"
  }

  ## Construct the path to the main R directory
  main_r_dir <- file.path(add_dir, "SEFHIER/R code")

  ## Define directory names for 'Inputs' and 'Outputs'
  in_dir <- "Inputs"
  out_dir <- "Outputs"

  ## Construct full paths to 'Inputs' and 'Outputs' directories using 'file.path'
  ## file.path is a function used to create platform-independent file paths by joining its arguments using the appropriate path separator (e.g., "\" on Windows, "/" on Unix-like systems).
  #
  ## base_dir is the base directory obtained from the user's home directory.
  #
  ## main_r_dir is the path to the main R directory, which may vary depending on whether the user is Anna or not.
  #
  ## in_dir is the name of the 'Inputs' directory.
  #
  ## So, this line effectively combines these components to create the full path to the 'Inputs' directory, ensuring that the path is correctly formatted for the user's operating system.

  full_path_to_in_dir <- file.path(base_dir, main_r_dir, in_dir)
  full_path_to_out_dir <- file.path(base_dir, main_r_dir, out_dir)

  ## Change the working directory to the main R directory
  setwd(file.path(base_dir, main_r_dir))

  ## Create a list of directory paths for 'inputs' and 'outputs'
  my_paths <- list("inputs" = full_path_to_in_dir,
                   "outputs" = full_path_to_out_dir)
  return(my_paths)
}

## Define a function named 'set_work_dir_local'
## This function sets the working directory to the user's home directory, defines paths to 'my_inputs,' 'my_outputs,' and 'R_code_github' directories, and returns these directory paths as a list. The use of file.path ensures that the path construction is platform-independent.

set_work_dir_local <- function() {

  ## Set the working directory to the user's home directory (~)
  setwd("~/")
  base_dir <- getwd()

  ## Define 'main_r_dir' as "R_files_local"
  main_r_dir <- "R_files_local"

  ## Define 'in_dir' as "my_inputs"
  in_dir <- "my_inputs"

  ## Construct the full path to 'my_inputs' directory
  full_path_to_in_dir <- file.path(base_dir, main_r_dir, in_dir)

  ## Define 'out_dir' as "my_outputs"
  out_dir <- "my_outputs"

  ## Construct the full path to 'my_outputs' directory
  full_path_to_out_dir <- file.path(base_dir, main_r_dir, out_dir)

  ## Define 'git_r_dir' as "R_code_github"
  git_r_dir <- "R_code_github"

  ## Construct the full path to 'R_code_github' directory
  full_path_to_r_git_dir <- file.path(base_dir, git_r_dir)

  ## Change the working directory to 'R_files_local'
  setwd(file.path(base_dir, main_r_dir))

  ## Create a list of directory paths for 'inputs,' 'outputs,' and 'git_r'
  my_paths <- list("inputs" = full_path_to_in_dir,
                   "outputs" = full_path_to_out_dir,
                   "git_r" = full_path_to_r_git_dir)

  ## Return the list of directory paths
  return(my_paths)
}

## ===
## Change the behavior of the set_work_dir function based on the username. If the username matches "anna.shipunova," it reassigns set_work_dir to the set_work_dir_local function, effectively using a different directory structure for Anna compared to other users.

## Check if the current username is "anna.shipunova"
if (get_username() == "anna.shipunova") {
  ## If the condition is true, assign the 'set_work_dir_local' function to 'set_work_dir'
  set_work_dir <- set_work_dir_local
}

## Define a function named 'load_csv_names' that takes two parameters: 'my_paths' and 'csv_names_list'
load_csv_names <- function(my_paths, csv_names_list) {

  ## Extract the 'inputs' directory path from 'my_paths' and store it in 'my_inputs'
  my_inputs <- my_paths$inputs

  ## Use 'lapply' to prepend the 'my_inputs' directory path to each file name in 'csv_names_list'
  ## This creates a list of full file paths for the CSV files
  myfiles <- lapply(csv_names_list, function(x) file.path(my_inputs, x))

  ## Use 'lapply' again to read all CSV files listed in 'myfiles'
  ## The 'read_csv' function from the 'readr' package is used, specifying default column types as 'c' ('character')
  contents <- lapply(myfiles, read_csv, col_types = cols(.default = 'c'))

  ## Return the contents of the CSV files as a list
  return(contents)
}

## Define a function named 'load_csv_names_in_one_df' that takes two parameters: 'path_to_files' and 'csv_names_list'
load_csv_names_in_one_df <- function(path_to_files, csv_names_list) {

    ## Initialize 'myfiles' with 'csv_names_list'
    myfiles <- csv_names_list

    ## Check if 'path_to_files' (input directory path) is provided
    if (length(path_to_files) > 0) {

        ## If provided, use 'lapply' to prepend 'path_to_files' to each file name in 'csv_names_list'
        myfiles <- lapply(csv_names_list, function(x) file.path(path_to_files, x))
    }

    ## Read all CSV files listed in 'myfiles' into a single data frame using 'map_df'
    csv_content <- purrr::map_df(myfiles, function(file_name) {

        ## Use 'read_csv' from the 'readr' package to read each CSV file
        readr::read_csv(
            file_name,
            col_types = cols(.default = 'c'),  ## Set default column type to 'character'
            trim_ws = TRUE,  ## Trim whitespace from values
            na = c("", "NA", "NaN"),  ## Treat empty strings, "NA," and "NaN" as NA values
            name_repair = "universal"  ## Repair column names
        )
    })

    ## Return the concatenated data frame containing all CSV file contents
    return(csv_content)
}

#
## Explanation:
#
## 1. The function `load_csv_names_in_one_df` takes two parameters: `path_to_files`, which is an optional input directory path, and `csv_names_list`, a list of CSV file names.
#
## 2. Initially, the `myfiles` variable is assigned the `csv_names_list`.
#
## 3. If `path_to_files` is provided (its length is greater than 0), the function uses `lapply` to prepend `path_to_files` to each file name in `csv_names_list`. This ensures that the full file paths are correctly constructed.
#
## 4. The `map_df` function is used to read all CSV files listed in `myfiles` and concatenate them into a single data frame (`csv_content`).
#
## 5. Within the `map_df` function, each CSV file is read using `read_csv` from the `readr` package. Various options are set, including the default column type as 'character', trimming whitespace, specifying NA values, and repairing column names.
#
## 6. Finally, the function returns the concatenated data frame containing the contents of all CSV files, making it easier to work with them as a single data structure.

## ===
## The function load_xls_names returns the concatenated data frame containing data from all Excel files. This allows you to work with the combined data more easily.
load_xls_names <- function(my_paths, xls_names_list, sheet_n = 1) {

  ## Extract the 'inputs' directory path from 'my_paths' and store it in 'my_inputs'
  my_inputs <- my_paths$inputs

  ## Use 'lapply' to prepend 'my_inputs' directory path to each Excel file name in 'xls_names_list'
  myfiles <- lapply(xls_names_list, function(x) file.path(my_inputs, x))

  ## Read Excel files listed in 'myfiles' into one data frame using 'map_df'
  contents <- map_df(myfiles, ~read_excel(
    .x,                           ## File path
    sheet = sheet_n,              ## Sheet number to read (default is 1)
    .name_repair = fix_names,     ## Repair column names
    guess_max = 21474836,         ## Maximum number of rows to guess data types
    col_types = "text"           ## Specify all columns as 'text' type
  ))

  ## Return the concatenated data frame containing data from all Excel files
  return(contents)
}

## The clean_headers function is designed to clean and fix the column names of a given dataframe (my_df).
clean_headers <- function(my_df) {
    ## Use the 'fix_names' function to clean and fix the column names of the dataframe.
    colnames(my_df) %<>%
        fix_names()

    ## Return the dataframe with cleaned and fixed column names.
    return(my_df)
}

## ===
## The fix_names function is used to clean and standardize column names to make them suitable for use in data analysis or further processing.
## to use in a function,
## e.g. read_csv(name_repair = fix_names)
fix_names <- function(x) {
  ## Use the pipe operator %>%
  x %>%

    ## Remove dots from column names
    str_replace_all("\\.", "") %>%

    ## Replace all characters that are not letters or numbers with underscores
    str_replace_all("[^A-z0-9]", "_") %>%

    ## Ensure that letters are only in the beginning of the column name
    str_replace_all("^(_*)(.+)", "\\2\\1") %>%

    ## Convert column names to lowercase using 'my_headers_case_function'
    my_headers_case_function()
}
```

## sf functions 

```{r sf functions}
## sf functions ----

## to use on download from db
## Define a function named 'vessels_permits_id_clean' to clean a dataframe.
vessels_permits_id_clean <- function(my_df) {
    ## Create a new dataframe 'vessels_permits' by renaming two specific columns.
    vessels_permits <- my_df |>
        rename("PERMIT_VESSEL_ID" = "QCSJ_C000000000300000") |>
        rename("VESSEL_VESSEL_ID" = "QCSJ_C000000000300001")

    ## Return the cleaned dataframe.
    return(vessels_permits)
}

## ===
## to use with toc(log = TRUE, quiet = TRUE)
## Define a function named print_toc_log that takes a 'variables' parameter
 ## It's useful for monitoring and debugging code execution time when using the tic and toc functions to measure time intervals.
print_toc_log <- function(variables) {
  ## Retrieve the log from the tic package with formatting enabled and store it in 'log.txt'
  log.txt <- tic.log(format = TRUE)

  ## Write the lines of 'log.txt' to the console
  writeLines(unlist(log.txt))

  ## Clear the log, removing its contents
  tic.clearlog()
}

my_paths <- set_work_dir()
# Uncomment and change to your path for future use
# my_paths$inputs <- "path_input"
# my_paths$outputs <- "path_output"
```

##### Current file: get_srhs_vessels.R 

```{r Current file: get_srhs_vessels.R}
##### Current file: get_srhs_vessels.R ----
```

## get SRHS vessels to exclude 

```{r get SRHS vessels to exclude}
## get SRHS vessels to exclude ----
## The file is provided by Kenneth Brennan

srhs_vessels_2022 <- file.path(my_paths$inputs, "2022_SRHS_Vessels_08_18_2023.xlsx")

file.exists(srhs_vessels_2022)

srhs_vessels_2022_info <-
  read_excel(
  srhs_vessels_2022,
  ## add the sheet name if needed and uncomment the next line
  ## sheet = sheet_n,
  ## use my fix_names function for col names
  .name_repair = fix_names,
  ## if omitted, the algorithm uses only the few first lines and sometimes guesses it wrong
  guess_max = 21474836,
  ## read all columns as text
  col_types = "text"
)
```

##### Current file: get_metrics_tracking.R 

```{r Current file: get_metrics_tracking.R}
##### Current file: get_metrics_tracking.R ----
```

## help functions (in metric tracking) 

```{r help functions (in metric tracking)}
## help functions (in metric tracking) ----

## ===
## Download from FHIER / Reports / Metrics Tracking
## Put dates in, e.g. 01/01/2022 - 12/31/2022
## Click search
## Under "Detail Report - via Valid and Renewable Permits Filter (SERO_NEW Source)	" section below click "Actions", then "Download"

fhier_reports_metrics_tracking_file_names <-
  c("Detail_Report_12312021_12312022__08_23_2023.csv",
    "Detail_Report_12312022_12312023__08_23_2023.csv")

common_dir <-
  file.path(my_paths$inputs, 
  r"(from_Fhier\Detail Report - via Valid and Renewable Permits Filter (SERO_NEW Source))")

file.exists(common_dir)

## save all file names to a list
## Create a vector named 'fhier_reports_metrics_tracking_file_path' using the purrr::map function.
## This vector will store file paths based on the 'fhier_reports_metrics_tracking_file_names' vector.
fhier_reports_metrics_tracking_file_path <-
  purrr::map(
    ## Iterate over each element in the 'fhier_reports_metrics_tracking_file_names' vector.
    fhier_reports_metrics_tracking_file_names,
    ## For each file name ('x'), create a file path by combining it with 'common_dir'.
    ~ file.path(common_dir, .x)
  )

## test
## Use the purrr::map function to check if files exist at the specified paths.
## The result will be a logical vector indicating file existence for each path.
purrr::map(fhier_reports_metrics_tracking_file_path, file.exists)
## T

## read each csv in a list of dfs
## Use the purrr::map function to read multiple CSV files into a list of data frames.
fhier_reports_metrics_tracking_list <- purrr::map(
  fhier_reports_metrics_tracking_file_path,
  ## A vector of file paths to CSV files.
  ~ readr::read_csv(
    ## The current file path being processed in the iteration.
    .x,
    ## Specify column types; here, all columns are read as characters.
    col_types = cols(.default = 'c'),
    name_repair = fix_names  ## Automatically repair column names to be syntactically valid.
  )
)
```

## check how many in diff years 

```{r check how many in diff years}
## check how many in diff years ----
## Use the 'dplyr::setdiff' function to find the set difference between two vectors.
## (1 minus 2)
dplyr::setdiff(
  fhier_reports_metrics_tracking_list[[1]]$vessel_official_number,
  fhier_reports_metrics_tracking_list[[2]]$vessel_official_number
) |>
  length()  ## Calculate the length of the resulting set difference.
## [1] 669

## (2 minus 1)
dplyr::setdiff(
  fhier_reports_metrics_tracking_list[[2]]$vessel_official_number,
  fhier_reports_metrics_tracking_list[[1]]$vessel_official_number
) |>
  length()
## [1] 493

## in both years
## Use the 'dplyr::intersect' function to find the intersection of two vectors.
## In this case, we're finding the common unique values between the two vectors.
dplyr::intersect(
  fhier_reports_metrics_tracking_list[[1]]$vessel_official_number,
  fhier_reports_metrics_tracking_list[[2]]$vessel_official_number
) |>
  length()  ## Calculate the length of the resulting intersection.
## 2965
```

##### Current file: metric_tracking_no_srhs.R 

```{r Current file: metric_tracking_no_srhs.R}
##### Current file: metric_tracking_no_srhs.R ----

get_data_from_fhier_dir <- "get_data/get_data_from_fhier"

get_metrics_tracking_path <-
  file.path(my_paths$git_r,
            get_data_from_fhier_dir,
            "get_metrics_tracking.R")
## source(get_metrics_tracking_path)

get_srhs_vessels_path <-
  file.path(my_paths$git_r,
            get_data_from_fhier_dir,
            "get_srhs_vessels.R")
## source(get_srhs_vessels_path)
```

### exclude srhs vessels from metric traking 

```{r exclude srhs vessels from metric traking}
### exclude srhs vessels from metric traking ----
fhier_reports_metrics_tracking_not_srhs_ids <-
  ## create a data frame
  purrr::map_df(
    fhier_reports_metrics_tracking_list,
    ## for each df from the list
    ~ .x |>
      ## exclude SRHS vessels
      dplyr::filter(!vessel_official_number %in% srhs_vessels_2022_info$uscg__)
  ) |>
  ## keep only the vessel_official_numbers, remove all other columns
  dplyr::select(vessel_official_number) |>
  ## remove duplicates
  distinct()

dim(fhier_reports_metrics_tracking_not_srhs_ids)
## [1] 2981    1

## the same, but result kept in a list
## Create a list named 'fhier_reports_metrics_tracking_not_srhs_ids_list'
fhier_reports_metrics_tracking_not_srhs_ids_list <-
  purrr::map(
    fhier_reports_metrics_tracking_list,
    ## Iterate over each data frame in this list
    ~ .x |>
      ## Exclude SRHS vessels:
      ## Filter rows where 'vessel_official_number' is not in 'uscg__' column of 'srhs_vessels_2022_info'
      filter(!vessel_official_number %in% srhs_vessels_2022_info$uscg__) |>
      ## Select only the 'vessel_official_number' column
      select(vessel_official_number) |>
      ## Remove duplicate values from the selected column
      distinct()
  )


## check
## Use 'map' to apply the 'dim' function to each data frame in 'fhier_reports_metrics_tracking_list'
purrr::map(fhier_reports_metrics_tracking_list, dim)
## [[1]]
## [1] 3634   13
#
## [[2]]
## [1] 3460   13

purrr::map(fhier_reports_metrics_tracking_not_srhs_ids_list, dim)
## [[1]]
## [1] 3571    1
#
## [[2]]
## [1] 3399    1
```

##### Current file: get_db_data.R 

```{r Current file: get_db_data.R}
##### Current file: get_db_data.R ----
```
## setup (in get data) 
```{r setup (in get data)}
## setup (in get data) ----

## err msg if no connection, but keep running
try(con <- connect_to_secpr())
```

## get data from db 

```{r get data from db}
## get data from db ----
## RDS (R Data Serialization) files are a common format for saving R objects in RStudio, and they allow you to preserve the state of an object between R sessions.
```

### logbooks as in FHIER 

```{r logbooks as in FHIER}
### logbooks as in FHIER ----
## mv_safis_trip_download

## Create a file path by combining 'my_paths$input' with the filename "mv_safis_trip_download.rds."
file_name_mv_safis_trip_download <-
  file.path(my_paths$input, "mv_safis_trip_download.rds")

mv_safis_trip_download_query <-
  "select * from
srh.mv_safis_trip_download@secapxdv_dblk.sfsc.noaa.gov
"

## Define a function 'mv_safis_trip_download_fun' to retrieve data from the database using a specified query.
mv_safis_trip_download_fun <-
  function(mv_safis_trip_download_query) {
  ## Use 'dbGetQuery' to execute the query on the database connection 'con' and return the result.
  result <- dbGetQuery(con, mv_safis_trip_download_query)

  ## Return the result of the database query.
  return(result)
}

get_mv_safis_trip_download <-
  function() {
    ## Use 'read_rds_or_run' to either read permit information from an RDS file or execute a query to obtain it.
    read_rds_or_run(file_name_mv_safis_trip_download,
                    mv_safis_trip_download_query,
                    mv_safis_trip_download_fun,
                    force_from_db)
  }

## to use alone:
## mv_safis_trip_download_data <- get_mv_safis_trip_download()
## 2023-10-19 run for mv_safis_trip_download.rds: 746.39 sec elapsed
```

### SEFHIER declarations as in FHIER 

```{r SEFHIER declarations as in FHIER}
### SEFHIER declarations as in FHIER ----
## MV_TMS_TRIP_NOTIFICATIONS

## Create a file path by combining 'my_paths$input' with the filename "mv_tms_trip_notifications.rds."
file_name_mv_tms_trip_notifications <- file.path(my_paths$input, "mv_tms_trip_notifications.rds")

mv_tms_trip_notifications_query <-
  "select * from
srh.mv_tms_trip_notifications@secapxdv_dblk.sfsc.noaa.gov
"

## Define a function 'mv_tms_trip_notifications_fun' to retrieve data from the database using a specified query.
mv_tms_trip_notifications_fun <- function(mv_tms_trip_notifications_query) {
  ## Use 'dbGetQuery' to execute the query on the database connection 'con' and return the result.
  result <- dbGetQuery(con, mv_tms_trip_notifications_query)

  ## Return the result of the database query.
  return(result)
}

get_mv_tms_trip_notifications <-
  function() {
    ## Use 'read_rds_or_run' to either read permit information from an RDS file or execute a query to obtain it.
    read_rds_or_run(file_name_mv_tms_trip_notifications,
                    mv_tms_trip_notifications_query,
                    mv_tms_trip_notifications_fun,
                    force_from_db)
  }

## to use alone:
## mv_tms_trip_notifications_data <-
##   get_mv_tms_trip_notifications()
## 2023-10-19 run for mv_tms_trip_notifications.rds: 11.04 sec elapsed
```

### permit 

```{r permit}
### permit ----
## Create a file path by combining 'my_paths$input' with the filename "permit_info.rds."
file_name_permits <- file.path(my_paths$input, "permit_info.rds")

mv_sero_fh_permits_his_query <-
  "select * from
srh.mv_sero_fh_permits_his@secapxdv_dblk.sfsc.noaa.gov
"

## Define a function 'permit_info_fun' to retrieve data from the database using a specified query.
permit_info_fun <- function(mv_sero_fh_permits_his_query) {
  ## Use 'dbGetQuery' to execute the query on the database connection 'con' and return the result.
  result <- dbGetQuery(con, mv_sero_fh_permits_his_query)

  ## Return the result of the database query.
  return(result)
}

get_permit_info <-
  function() {
    ## Use 'read_rds_or_run' to either read permit information from an RDS file or execute a query to obtain it.
    read_rds_or_run(file_name_permits,
                    mv_sero_fh_permits_his_query,
                    permit_info_fun,
                    force_from_db)
  }
## 2023-09-20 run the function: 40.74 sec elapsed
```

#### permit + vessel from db 

```{r permit + vessel from db}
#### permit + vessel from db ----
## Doesn't work
## permit_vessel_query_exp21_query <-
## "select * from srh.mv_sero_fh_permits_his@secapxdv_dblk.sfsc.noaa.gov p
## join safis.vessels@secapxdv_dblk.sfsc.noaa.gov v
## ON (v.sero_official_number = p.vessel_id)
## where expiration_date > TO_DATE('01-JAN-21')
## "
## and top in ('CDW', 'CHS', 'SC')

## permit_vessel_query_exp21 <- dbGetQuery(con,
                          ## permit_vessel_query_exp21_query)

## View(permit_vessel_query_exp21)

## add sa gom field

## names(permit_vessel_query_exp21) <-
  ## make.unique(names(permit_vessel_query_exp21), sep = "_")

## print_df_names(permit_vessel_query_exp21)

## permit_vessel_query_exp21 %>%
  ## filter(!(VESSEL_ID == SERO_OFFICIAL_NUMBER)) %>%
  ## dim()
## 0

## Logbooks
```

## get trips info 

```{r get trips info}
## get trips info ----
trips_file_name <-
    file.path(my_paths$input, "trips.rds")

trips_query <-
  "SELECT
  *
FROM
  safis.trips@secapxdv_dblk.sfsc.noaa.gov
WHERE
  ( trip_start_date BETWEEN TO_DATE('01-JAN-22', 'dd-mon-yy') AND CURRENT_DATE
  )
ORDER BY
  trip_end_date DESC
"

trips_fun <- function(trips_query) {
  return(dbGetQuery(con,
             trips_query))
}

get_trips_info <-
  function() {
      read_rds_or_run(trips_file_name,
                      trips_query,
                      trips_fun,
                      force_from_db)
  }
## 2023-09-20 run the function: 33.02 sec elapsed

## grep("long", names(trips_info), ignore.case = T, value = T)
## 0
```

## latitude/longitude 

```{r latitude_longitude}
## latitude/longitude ----
## select * from safis.EFFORTS@secapxdv_dblk.sfsc.noaa.gov;

trip_coord_query <- "
  SELECT
  trip_id,
  area_code,
  sub_area_code,
  distance_code,
  fishing_hours,
  latitude,
  longitude,
  local_area_code,
  in_state,
  avg_depth_in_fathoms,
  e.de e_de,
  e.ue e_ue,
  e.dc e_dc,
  e.uc e_uc,
  anything_caught_flag,
  depth,
  minimum_bottom_depth,
  maximum_bottom_depth,
  fishing_gear_depth,
  ten_minute_square_list,
  trip_type,
  supplier_trip_id,
  days_at_sea,
  t.de t_de,
  t.ue t_ue,
  t.dc t_dc,
  t.uc t_uc,
  vessel_id,
  cf_permit_id,
  trip_start_date,
  port,
  state,
  trip_end_date,
  trip_end_time,
  trip_start_time,
  submit_method,
  activity_type,
  end_port,
  start_port,
  sero_vessel_permit,
  sea_time
FROM
       safis.efforts@secapxdv_dblk.sfsc.noaa.gov e
  JOIN safis.trips@secapxdv_dblk.sfsc.noaa.gov t
  USING ( trip_id )
WHERE
  ( trip_start_date BETWEEN TO_DATE('01-JAN-22', 'dd-mon-yy') AND CURRENT_DATE
  )
"

trip_coord_file_name <-
    file.path(my_paths$input, "trip_coord.rds")

trip_coord_fun <- function(trip_coord_query) {
  return(dbGetQuery(con,
             trip_coord_query))
}

get_trip_coord_info <-
  function() {
      read_rds_or_run(trip_coord_file_name,
                      trip_coord_query,
                      trip_coord_fun,
                      force_from_db)
  }

## 2023-09-20 run the function: 30.94 sec elapsed

## DNF reports
```

## get trip neg 

```{r get trip neg}
## get trip neg ----

trip_neg_2022_file_path <-
  file.path(my_paths$input, "trip_neg_2022.rds")

trip_neg_2022_query <-
  "SELECT *
  FROM
    safis.trips_neg@secapxdv_dblk.sfsc.noaa.gov
WHERE
  ( trip_date BETWEEN TO_DATE('01-JAN-22', 'dd-mon-yy') AND TO_DATE('01-JAN-23'
  , 'dd-mon-yy') )"

## 1495929

trip_neg_2022_fun <-
  function(trip_neg_2022_query) {
    return(dbGetQuery(con, trip_neg_2022_query))
  }

## trip_neg_query_2022: 201.21 sec elapsed
## trip_neg_query_2022: 60.06 sec elapsed
## trip_neg_query_2022: 89.38 sec elapsed

get_trip_neg_2022 <-
  function() {
    read_rds_or_run(trip_neg_2022_file_path,
                    trip_neg_2022_query,
                    trip_neg_2022_fun,
                    force_from_db)
  }
## run the function: 98.23 sec elapsed

## Declarations
```

## trips_notifications 

```{r trips_notifications}
## trips_notifications ----
trips_notifications_2022_query <-
  "SELECT
 *
FROM
  safis.trip_notifications@secapxdv_dblk.sfsc.noaa.gov
WHERE
  ( trip_start_date BETWEEN TO_DATE('01-JAN-22', 'dd-mon-yy') AND TO_DATE('01-JAN-23'
  , 'dd-mon-yy') )
  OR ( trip_end_date BETWEEN TO_DATE('01-JAN-22', 'dd-mon-yy') AND TO_DATE('01-JAN-23'
  , 'dd-mon-yy') )
"

trips_notifications_2022_file_path <-
  file.path(my_paths$input, "trips_notifications_2022.rds")

trips_notifications_2022_fun <-
  function(trips_notifications_2022_query) {
    return(dbGetQuery(con, trips_notifications_2022_query))
  }
## trips_notifications_query: 52.08 sec elapsed
## 97279
## trips_notifications_query: 7.65 sec elapsed

get_trips_notifications_2022 <-
  function() {
    read_rds_or_run(
      trips_notifications_2022_file_path,
      trips_notifications_2022_query,
      trips_notifications_2022_fun,
      force_from_db
    )
  }
## 2023-07-15 run the function: 13.41 sec elapsed
```

## get_vessels with permits 2021+ 

```{r get_vessels with permits 2021+}
## get_vessels with permits 2021+ ----

dates_filter <- " (end_date >= TO_DATE('01-JAN-21', 'dd-mon-yy')
    OR expiration_date >= TO_DATE('01-JAN-21', 'dd-mon-yy') )
  AND effective_date <= CURRENT_DATE
"
## Use that "dates_filter" in all parts of the union below.

## The 3 part union is needed because while the permit table has only one vessel id, the vessel table has 3 different columns for that (sero_official_number, coast_guard_nbr and state_reg_nbr) and we want to join tables by all 3 in turn.
## stringr::str_glue is a function that allows you to create strings with placeholders for variable values. It works by using curly braces {} to enclose variable names within a string.
vessels_permits_query <-
  stringr::str_glue("SELECT
  *
FROM
       srh.mv_sero_fh_permits_his@secapxdv_dblk.sfsc.noaa.gov p
  JOIN safis.vessels@secapxdv_dblk.sfsc.noaa.gov
  ON ( p.vessel_id = sero_official_number )
WHERE {dates_filter}
UNION ALL
SELECT
  *
FROM
       srh.mv_sero_fh_permits_his@secapxdv_dblk.sfsc.noaa.gov p
  JOIN safis.vessels@secapxdv_dblk.sfsc.noaa.gov
  ON ( p.vessel_id = coast_guard_nbr )
WHERE
  {dates_filter}
UNION ALL
SELECT
  *
FROM
       srh.mv_sero_fh_permits_his@secapxdv_dblk.sfsc.noaa.gov p
  JOIN safis.vessels@secapxdv_dblk.sfsc.noaa.gov
  ON ( p.vessel_id = state_reg_nbr )
WHERE
{dates_filter}
")

vessels_permits_file_path <- file.path(my_paths$input, "vessels_permits.rds")

vessels_permits_fun <-
  function(vessels_permits_query) {
    return(dbGetQuery(con,
                      vessels_permits_query))
  }

get_vessels_permits <-
  function() {
    read_rds_or_run(vessels_permits_file_path,
                    vessels_permits_query,
                    vessels_permits_fun,
                    force_from_db) |>
      vessels_permits_id_clean()
  }
## 2023-09-20 run the function: 14.08 sec elapsed

## an additional procedure, usually is not needed
```

## find 0 column 

```{r find 0 column}
## find 0 column ----
## get vessels
## can't because of "\\0"
## use:
## replace(VESSEL_NAME, chr(0), '') VESSEL_NAME,

#
## field_names <-
##   c("VESSEL_ID",
##     "COUNTY_CODE",
##     "STATE_CODE",
##     "ENTRY_DATE",
##     "SUPPLIER_VESSEL_ID",
##     "PORT_CODE",
##     "HULL_ID_NBR",
##     "COAST_GUARD_NBR",
##     "STATE_REG_NBR",
##     "REGISTERING_STATE",
##     ## "VESSEL_NAME",
##     "PASSENGER_CAPACITY",
##     "VESSEL_TYPE",
##     "YEAR_BUILT",
##     "UPDATE_DATE",
##     "PRIMARY_GEAR",
##     "OWNER_ID",
##     "EVENT_ID",
##     "DE",
##     "UE",
##     "DC",
##     "UC",
##     "STATUS",
##     "SER_ID",
##     "UPDATED_FLAG",
##     "SERO_HOME_PORT_CITY",
##     "SERO_HOME_PORT_COUNTY",
##     "SERO_HOME_PORT_STATE",
##     "SERO_OFFICIAL_NUMBER")
#
## vessels_zero_query <-
##   "select
##   distinct {field_name}
##   from
##   safis.vessels@secapxdv_dblk.sfsc.noaa.gov"
#
## rr <-
##   map(field_names,
##     function(field_name) {
##       print(str_glue("field_name = {field_name}"))
##       q <- str_glue(vessels_zero_query)
##       tic("vessels_all1")
##       vessels_all <- dbGetQuery(con,
##                                 q)
##       toc()
##       return(dim(vessels_all))
##     }
## )
#
## ## \\0 err:
## ##   field_name = VESSEL_NAME
#
#
## ## distinct vessel_id ok
## tic("vessels_all1")
## vessels_all <- dbGetQuery(con,
##                           vessels_zero_query)
## toc()
#
#
#
## dim(permit_info)
## dim(trip_neg_2022)
## dim(trips_notifications_2022)
## dim(trips_info_2022)
## dim(vessels_all)
```

## dates_2022 

```{r dates_2022}
## dates_2022 ----
dates_2022_query <-
  "SELECT
  dd.year,
  dd.month_of_year,
  dd.week_of_year,
  dd.complete_date
FROM
  srh.dim_dates@secapxdv_dblk.sfsc.noaa.gov dd
WHERE
  dd.complete_date BETWEEN '01-DEC-2021' AND '31-JAN-2023'
"

dates_2022_file_path <- file.path(my_paths$input, "dates_2022.rds")

dates_2022_fun <-
  function(dates_2022_query) {
    return(dbGetQuery(con,
                      dates_2022_query))
  }

get_dates_2022 <- function() {
  read_rds_or_run(dates_2022_file_path,
                  dates_2022_query,
                  dates_2022_fun,
                  force_from_db)
}
```

## get override data 

```{r get override data}
## get override data ----
compl_err_query <-
    "SELECT
  *
FROM
       srh.srfh_vessel_comp_err@secapxdv_dblk.sfsc.noaa.gov
  INNER JOIN srh.srfh_vessel_comp@secapxdv_dblk.sfsc.noaa.gov
  USING ( srh_vessel_comp_id )
WHERE
  comp_year > '2020'
"
## common fields
##   SRH_VESSEL_COMP_ID
## CREATED_DT
## CREATED_USER_ID
## LU_DT
## LU_USER_ID

## Define a function 'get_compl_err_data_from_db' to retrieve compliance error data from the database.
get_compl_err_data_from_db <- function(compl_err_query) {

  ## Use 'ROracle::dbGetQuery' to execute the 'compl_err_query' on the database connection 'con'
  ## and store the result in 'compl_err_db_data_0'.
  compl_err_db_data_0 =
    ROracle::dbGetQuery(con, compl_err_query)

  compl_err_db_data_1 <-
    compl_err_db_data_0 %>%
    ## remove duplicated columns
    dplyr::select(-c(CREATED_DT,
                     CREATED_USER_ID,
                     LU_DT,
                     LU_USER_ID))

  return(compl_err_db_data_1)
}

file_name_overr <-
  file.path(my_paths$input, "compl_err_db_data_raw.rds")

get_compl_err_db_data <- function() {
  compl_err_db_data_raw <-
    read_rds_or_run(file_name_overr,
                    compl_err_query,
                    get_compl_err_data_from_db,
                    force_from_db)
  ## 2023-09-20 run the function: 14.99 sec elapsed

  ## Clean the column names of the 'compl_err_db_data_raw' data frame using the 'clean_headers' function defined above.
  compl_err_db_data <- clean_headers(compl_err_db_data_raw)

  return(compl_err_db_data)
}
```

## get metric_tracking_no_srhs 

```{r get metric_tracking_no_srhs}
## get metric_tracking_no_srhs ----

## Use the 'source' function to execute an R script file located at the specified path.
get_metrics_tracking_path <-
  file.path(my_paths$git_r,
                 get_data_from_fhier_dir,
                 "get_metrics_tracking.R")
## source(get_metrics_tracking_path)

## or source separate files instead of the flat one:
## ## source(file.path(my_paths$git_r,
##                 get_data_from_fhier_dir,
##                  "metric_tracking_no_srhs.R"))

## How to create a flat file:
## flat_file_name <- "make_metric_tracking_no_srhs.R"
## files_to_combine_list <-
##   c(
##     file.path(my_paths$git_r,
##                 get_data_from_fhier_dir,
##                  "get_srhs_vessels.R"),
##     file.path(my_paths$git_r,
##                 get_data_from_fhier_dir,
##                  "get_metrics_tracking.R"),
##     file.path(my_paths$git_r,
##                 get_data_from_fhier_dir,
##                  "metric_tracking_no_srhs.R")
##   )
#
## make_a_flat_file(flat_file_name,
##            files_to_combine_list)

## dim(fhier_reports_metrics_tracking_not_srhs_ids)
## 4063
```

## --- main 

```{r --- main}
## --- main ----
## Define a function 'run_all_get_db_data' to fetch data from the database and store it in a result list.

run_all_get_db_data <-
  function() {
    ## Initialize an empty list to store the results.
    result_l = list()

    ## 1) Call the 'get_permit_info' function to retrieve permit information from the database.
    mv_sero_fh_permits_his <- get_permit_info()

    ## 2) Store the retrieved data in the result list under the name "mv_sero_fh_permits_his."
    result_l[["mv_sero_fh_permits_his"]] <- mv_sero_fh_permits_his
    ## dim(mv_sero_fh_permits_his)
    ## [1] 183204     22

    ## Repeat the steps 1 and 2 for all other types of data using the predefined functions.

    mv_safis_trip_download_data <- get_mv_safis_trip_download()
    result_l[["mv_safis_trip_download"]] <-
      mv_safis_trip_download_data
    ## dim(mv_safis_trip_download_data)
    ## [1] 735666    149

    mv_tms_trip_notifications_data <-
      get_mv_tms_trip_notifications()
    result_l[["mv_tms_trip_notifications"]] <-
      mv_tms_trip_notifications_data
    ## dim(mv_tms_trip_notifications_data)
    ## [1] 118730     41

    trips_info <- get_trips_info()
    result_l[["trips_info"]] <- trips_info
    ## dim(trips_info)
    ## [1] 98528    72 2022
    ## [1] 142037   72 2021+

    trip_coord_info <- get_trip_coord_info()
    result_l[["trip_coord_info"]] <- trip_coord_info
    ## dim(trip_coord_info)
    ## [1] 141350     41

    trip_neg_2022 <- get_trip_neg_2022()
    result_l[["trip_neg_2022"]] <- trip_neg_2022
    ## dim(trip_neg_2022)
    ## Rows: 1,495,929
    ## [1] 746087     12
    ## [1] 747173     12

    trips_notifications_2022 <- get_trips_notifications_2022()
    result_l[["trips_notifications_2022"]] <-
      trips_notifications_2022
    ## dim(trips_notifications_2022)
    ## Rows: 129,701
    ## [1] 70056    33

    vessels_permits <- get_vessels_permits()
    result_l[["vessels_permits"]] <- vessels_permits
    ## dim(vessels_permits)
    ## [1] 78438    51

    dates_2022 <- get_dates_2022()
    result_l[["dates_2022"]] <- dates_2022
    ## dim(dates_2022)
    ## 427 4

    compl_err_db_data <- get_compl_err_db_data()
    result_l[["compl_err_db_data"]] <- compl_err_db_data
    ## dim(compl_err_db_data)
    ## [1] 99832    38

    return(result_l)
  }

force_from_db <- NULL ## read data from files if exist
## force_from_db <- "YES"

## How to use:
## Add to your code, uncomment and run:
## tic("run_all_get_db_data()")
## all_get_db_data_result_l <- run_all_get_db_data()
## toc()

## Then use like this, for example:
## View(all_get_db_data_result_l)
## mv_safis_trip_download <- all_get_db_data_result_l$mv_safis_trip_download

## Benchmark:
## reading RDS
## run_all_get_db_data(): 1.69 sec elapsed
## reading from db
## run_all_get_db_data(): 259.81 sec elapsed ~ 4 min
## run_all_get_db_data(): 606.66 sec elapsed ~ 10 min with MVs reading

## str(all_get_db_data_result_l[["compl_err_db_data"]])
## 'data.frame':	99832 obs. of  38 variables:
```

#### check 

```{r check}
#### check ----
## for each df print its name and dim()
## names(all_get_db_data_result_l) |>
##   map(\(df_name) {
##     c(df_name, dim(all_get_db_data_result_l[[df_name]]))
##   })

## force_from_db <- "NULL"
## dates_2022 <- get_dates_2022()
```

## close the db connection 

```{r close the db connection}
## close the db connection ----
# try(ROracle::dbDisconnect(con))
```

##### Current file: vessel_permit_corrected_list.R 

```{r Current file: vessel_permit_corrected_list.R}
##### Current file: vessel_permit_corrected_list.R ----

```

## read Jeannette's file 

```{r read Jeannette_s file}
## read Jeannette's file ----
library(readxl)  ## reading in .xlsx
## the file is from Jeannette Oct 17 2023
v_list_file_name <-
  file.path(
    my_paths$inputs,
    "SA.Permitted.Vessels.Among_revised.Lists.xlsx"
  )


file.exists('C:/Users/anna.shipunova/Documents/R_files_local/my_inputs/SA.Permitted.Vessels.Among_revised.Lists.xlsx')

## Create a sequence of sheet numbers from 1 to 4. These sheet numbers will be used to specify which sheets to read from the Excel file.
sheets <- seq(1:4)

## Use 'purrr::map' to read data from multiple sheets of an Excel file.
## For each sheet, read the data and store it in a list.
all_sheets_l <-
  purrr::map(sheets,
             function(sheet_num) {
               readxl::read_excel(
                 ## The name of the Excel file.
                 v_list_file_name,
                 ## Specify the sheet number to read.
                 sheet = sheet_num,
                 ## Maximum number of rows to guess data types
                 guess_max = 21474836,
                 ## Read all columns as text to preserve data integrity.
                 col_types = "text",
                 ## Use my function for name repair for column names.
                 .name_repair = fix_names,
                 ## Do not use the first row as column names
                 col_names = FALSE
               )
             })
```

## Check what's inside 

```{r Check what_s inside}
## Check what's inside ----
## map(all_sheets_l, dim)
## [[1]]
## [1] 2215    1
#
## [[2]]
## [1] 55  2
#
## [[3]]
## [1] 126   1
#
## [[4]]
## [1] 130   2

## map(all_sheets_l, glimpse)
```

## Clean up dfs 

```{r Clean up dfs}
## Clean up dfs ----
```

### add sheet names to the df list 

```{r add sheet names to the df list}
### add sheet names to the df list ----
## Extract the sheet names from an Excel file specified by 'v_list_file_name'.
## - 'excel_sheets' function is used to get the sheet names.
## - 'str_replace_all' replaces any periods with underscores in sheet names.
## - 'tolower' converts all sheet names to lowercase.
all_sheets_l_names <-
  readxl::excel_sheets(v_list_file_name) |>
  str_replace_all("\\.", "_") |>
  tolower()

## Rename the elements of the 'all_sheets_l' object to match the modified sheet names.
## - 'names' function is used to access and modify the names of the object.
## - 'all_sheets_l_names' is a character vector containing the modified sheet names.
## - Use the sheet names for all dataframes in the 'all_sheets_l'  list.
##   The names of 'all_sheets_l' are updated to match the corresponding sheet names.
names(all_sheets_l) <- all_sheets_l_names[1:length(all_sheets_l)]
```

### Remove old column names 

```{r Remove old column names}
### Remove old column names ----
## like "Vessel id"
## Define a function called 'remove_first_row_if_vessel'.
## This function takes a data frame 'my_df' as its input parameter.
remove_first_row_if_vessel <- function(my_df) {

  ## Check if the first cell (1,1) of 'my_df' contains the word "vessel"
  ## (case-insensitive search).
  if (grepl("vessel", my_df[1, 1], ignore.case = TRUE)) {
    ## If the word "vessel" is found, remove the first row (-1) from 'my_df'.
    my_df <- my_df[-1,]
  }
  ## Return the modified 'my_df'. If no changes were made, the original
  ## 'my_df' is returned unchanged.
  return(my_df)
}

## Use the 'map' function to apply the 'remove_first_row_if_vessel' function
## to each data frame in the 'all_sheets_l' list.
## The result is a modified 'all_sheets_l' list with the undesired header rows removed from each data frame.

all_sheets_l <-
  purrr::map(all_sheets_l, remove_first_row_if_vessel)

## View(all_sheets_l)
```

### add names to the columns 

```{r add names to the columns}
### add names to the columns ----
```

#### 1) 

```{r 1)}
#### 1) ----
## Set the names of the column in the first data frame in the list to "vessel_official_number".
names(all_sheets_l[[1]]) <- c("vessel_official_number")
```

#### 2) 

```{r 2)}
#### 2) ----
## Set the names of the column in the second data frame in the list.
names(all_sheets_l[[2]]) <- c("vessel_official_number",
                              "comments")
```

#### 3) 

```{r 3)}
#### 3) ----
## Set the names of the column in the third data frame in the list.
names(all_sheets_l[[3]]) <- c("vessel_official_number")
```

#### 4) 

```{r 4)}
#### 4) ----
## Set the temporary names of the column in the forth data frame in the list.
names(all_sheets_l[[4]]) <- c("num",
                              "vessel_official_number-comments")
```

##### Split comments 

```{r Split comments}
##### Split comments ----
## Create a new data frame 'sheet_4_temp_1' by splitting the 'vessel_official_number-comments'
## column of the fourth data frame in 'all_sheets_l' using the 'separate_wider_delim'
## function.
sheet_4_temp_1 <- all_sheets_l[[4]] |>
  separate_wider_delim(
    "vessel_official_number-comments",  ## Column to split
    delim = " - ",                     ## Delimiter used for splitting
    names = c("vessel_official_number", "comments"),  ## Names of the new columns
    too_few = "align_end"               ## How to handle if fewer columns are created
  )


## Create a new data frame 'sheet_4_temp_2' by performing a series of operations on the
## data frame 'sheet_4_temp_1':

## 1. Use 'group_split' to split 'sheet_4_temp_1' into multiple groups.
##    - 'grp' parameter creates groups based on cumulative sums of NA values
##      in each row. A new group starts when a row contains all NA values.
##    - '.keep = TRUE' means that the grouping column is retained in each group.

## 2. Use 'purrr::map_at' to apply a function to specific columns in each group.
##    - '.at = -1' specifies the position of the columns to be operated on
##      (all columns except the last one).
##    - 'tail' function is applied to each specified column within each group.
##    - '-1' indicates that the last element (row) of each column should be extracted.

## View(sheet_4_temp_1)
sheet_4_temp_2 <-
  sheet_4_temp_1 %>%
  group_split(grp = cumsum(rowSums(is.na(.)) == ncol(.)), .keep = TRUE) %>%
  purrr::map_at(.at = -1, tail, -1)

## change the last group columns
## sheet_4_temp_2[[3]] |> head()
## num vessel_official_number comments grp
## <chr> <chr> <chr> <int>
## 1 NA NA 99 vessels did not have the isLatest flag … 2
## 2 1.0 NA 555341.0 2
## 3 2.0 NA 556601.0 2

## Create a new data frame 'sheet_4_temp_3' by applying mutations to the third group
## in 'sheet_4_temp_2' using the 'dplyr::mutate' function.

sheet_4_temp_2[[3]] <- sheet_4_temp_2[[3]] |>
  dplyr::mutate(vessel_official_number =
                  dplyr::case_when(
                    ## If 'comments' contains a space, it is Janette's comment, set 'vessel_official_number' to NA.
                    grepl(" ", comments) ~ NA,
                    ## For all other cases, remove '.0' from 'comments' and assign it to 'vessel_official_number'.
                    .default = stringr::str_replace(comments, "\\.0", "")
                  ))

## Create a new data frame 'sheet_4' by binding rows from a list of data frames 'sheet_4_temp_2'.
## The 'dplyr::bind_rows' function combines the data frames in the list into a single data frame.
sheet_4 <- dplyr::bind_rows(sheet_4_temp_2)

## put sheet_4 back to the common list
all_sheets_l[[4]] <- sheet_4
## View(all_sheets_l[[4]])
```

## vessels_22_sa 

```{r vessels_22_sa}
## vessels_22_sa ----
## Create a new data frame 'vessels_22_sa' by combining data from two sheets.
vessels_22_sa <-

  ## Extract data from the fourth sheet of 'all_sheets_l' (sheet number 4).
  all_sheets_l[[4]] |>

  ## Filter rows where the 'grp' column contains values 0 or 2.
  dplyr::filter(grp %in% c(0, 2)) |>

  ## Select only the 'vessel_official_number' column.
  dplyr::select(vessel_official_number) |>

  ## Combine the filtered data with data from the first sheet (sheet number 1) of 'all_sheets_l'.
  rbind(all_sheets_l[[1]]) |>
  ## na.omit returns the object with incomplete cases removed.
  stats::na.omit()

## vessels_22_sa |>
  ## dim()
## [1] 2321    1
```

## remove wrong ids from FHIER results 

```{r remove wrong ids from FHIER results}
## remove wrong ids from FHIER results ----
vessels_to_remove_from_ours <-
  all_sheets_l$in_ours_not_jeannettes$vessel_official_number

## length(vessels_to_remove_from_ours)
## 55

cat("all_sheets_l",
      "vessels_22_sa",
      "vessels_to_remove_from_ours",
    sep = '\n')
```

##### Current file: all_logbooks_db_data_2022_short_p_region_prep.R 

```{r Current file: all_logbooks_db_data_2022_short_p_region_prep.R}
##### Current file: all_logbooks_db_data_2022_short_p_region_prep.R ----

## Prepare all_logbooks_db_data_2022_short_p_region
## 1) download all db data
## 2) use "all logbooks = mv_safis_trip_download
## 3) Filter 2022 only
## 4) Remove unused columns
## 5) Mark sa_only vs. gom and dual for 2022 using vessel list from Jeanette’s comparison

rm_columns <- c("ACTIVITY_TYPE",
"ANYTHING_CAUGHT_FLAG",
"APP_VERSION",
"APPROVAL_DATE",
"APPROVED_BY",
"AVG_DEPTH_IN_FATHOMS",
"CAPT_NAME_FIRST",
"CAPT_NAME_LAST",
"CATCH_DC",
"CATCH_DE",
"CATCH_SEQ",
"CATCH_SOURCE_NAME",
"CATCH_SOURCE",
"CATCH_SPECIES_ITIS",
"CATCH_UE",
"COMMON_NAME",
"CONFIRMATION_SIGNATURE",
"DC",
"DE",
"DEA_PERMIT_ID",
"DEPTH",
"DISPOSITION_CODE",
"DISPOSITION_NAME",
"EFFORT_SEQ",
"EFFORT_TARGET_COMMON_NAMES",
"EFFORT_TARGET_SPECIES_LIST",
"EVENT_ID",
"FISHING_GEAR_DEPTH",
"FISHING_HOURS",
"FORM_VERSION",
"FUEL_DIESEL_GALLON_PRICE",
"FUEL_DIESEL_GALLONS",
"FUEL_GALLON_PRICE",
"FUEL_GALLONS",
"FUEL_GAS_GALLON_PRICE",
"FUEL_GAS_GALLONS",
"GEAR_CATEGORY_CODE",
"GEAR_CATEGORY_NAME",
"GEAR_CODE",
"GEAR_DESC",
"GEAR_NAME",
"GEAR_QUANTITY",
"GEAR_SIZE",
"GEAR_TYPE_CODE",
"GEAR_TYPE_NAME",
"GEARS_FISHING",
"GRADE_CODE",
"GRADE_NAME",
"HOOKS_PER_LINE",
"HOURS_DAYS_FLAG",
"IN_STATE",
"LANDING_SEQ",
"LMA_CODE",
"MARKET_CATEGORY_CODE",
"MARKET_CATEGORY_NAME",
"MAXIMUM_BOTTOM_DEPTH",
"MESH_RING_LENGTH",
"MESH_RING_WIDTH",
"MINIMUM_BOTTOM_DEPTH",
"NBR_OF_CREW",
"NBR_PAYING_PASSENGERS",
"NUM_ANGLERS",
"REPORTED_QUANTITY",
"REPORTING_SOURCE",
"RIG_CODE",
"SPECIES_ITIS",
"SPLIT_TRIP",
"STRETCH_SIZE",
"SUB_TRIP_TYPE",
"SUBMIT_METHOD",
"SUBMITTED_BY_CORPORATE_NAME",
"SUBMITTED_BY_FIRST_NAME",
"SUBMITTED_BY_LAST_NAME",
"SUBMITTED_BY_MIDDEL_NAME",
"SUBMITTED_BY_PARTICIPANT",
"SUPPLIER_EFFCAT_ID",
"SUPPLIER_TRIP_ID",
"TICKET_TYPE",
"TRANSMISSION_DATE",
"TRIP_END_TIME",
"TRIP_FEE",
"TRIP_NBR",
"TRIP_START_TIME",
"TRIP_TYPE",
"UC",
"UE",
"UNIT_MEASURE")

## source(file.path(my_paths$git_r, r"(get_data\get_db_data\get_db_data.R)"))

con
tic("run_all_get_db_data()")
all_get_db_data_result_l <- run_all_get_db_data()
toc()

## dim(all_get_db_data_result_l$mv_safis_trip_download)
## [1] 735666    149
```

## get 2022 only 

```{r get 2022 only}
## get 2022 only ----
## Create a new variable 'all_logbooks_db_data_2022' using the pipe operator.
## This variable will store the filtered data for the year 2022.
all_logbooks_db_data_2022 <-
  ## Take the data from 'all_get_db_data_result_l$mv_safis_trip_download'
  all_get_db_data_result_l$mv_safis_trip_download |>

  ## Use the dplyr::filter function to filter rows based on a condition
  dplyr::filter(
    ## Check if the 'TRIP_START_DATE' is between "2022-01-01" and "2022-12-31"
    dplyr::between(
      TRIP_START_DATE,                 ## Column to check
      as.Date("2022-01-01"),          ## Start date for the range
      as.Date("2022-12-31")           ## End date for the range
    )
  )

dim(all_logbooks_db_data_2022)
## [1] 326670    149
```

## Remove unused columns 

```{r Remove unused columns}
## Remove unused columns ----

## Create a new variable 'all_logbooks_db_data_2022_short' by further processing the 'all_logbooks_db_data_2022' data.
all_logbooks_db_data_2022_short <-
  ## Take the data from 'all_logbooks_db_data_2022'
  all_logbooks_db_data_2022 |>

  ## Use dplyr::select to remove columns specified in 'rm_columns'
  dplyr::select(-any_of(rm_columns)) |>

  ## Use dplyr::distinct to retain only distinct rows
  dplyr::distinct()

dim(all_logbooks_db_data_2022_short)
## [1] 94471    72
```

## Mark sa_only vs. gom and dual for 2022 

```{r Mark sa_only vs. gom and dual for 2022}
## Mark sa_only vs. gom and dual for 2022 ----
## Get vessel list from Jeanette’s comparison
script_path <-
  file.path(my_paths$git_r,
            "vessel_permit_list/vessel_permit_corrected_list.R")

## Source (run) the R script using the constructed script path.
## source(script_path)

## Rows are filtered to keep only vessels whose 'VESSEL_OFFICIAL_NBR' is in the
## 'vessels_22_sa' vector.
all_logbooks_db_data_2022_short_p_region <-
  all_logbooks_db_data_2022_short |>
  ## Use the dplyr::mutate function to add a new column 'permit_region' to the dataset
  dplyr::mutate(
    permit_region =
      ## Use the case_when function to conditionally assign values to 'permit_region'
      ## If vessel number is in 'vessels_22_sa', set to "sa_only"
      dplyr::case_when(VESSEL_OFFICIAL_NBR %in% vessels_22_sa$vessel_official_number ~ "sa_only",
                ## For all other cases, set to "gom_and_dual"
                .default = "gom_and_dual")
  )

dim(all_logbooks_db_data_2022_short_p_region)
## [1] 94471    73

names(all_logbooks_db_data_2022_short_p_region) <-
  names(all_logbooks_db_data_2022_short_p_region) |>
  my_headers_case_function()

## Output the objects, concatenating the representations. cat performs much less conversion than print.
cat("all_logbooks_db_data_2022_short_p_region",
    sep = '\n')
```

##### Current file: fishing_effort_locations_get_data.R 

```{r Current file: fishing_effort_locations_get_data.R}
##### Current file: fishing_effort_locations_get_data.R ----

library(mapview)
library(sf)
```

## get area data 

```{r get area data}
## get area data ----
rm_columns <- c("ACTIVITY_TYPE",
"ANYTHING_CAUGHT_FLAG",
"APP_VERSION",
"APPROVAL_DATE",
"APPROVED_BY",
"AVG_DEPTH_IN_FATHOMS",
"CAPT_NAME_FIRST",
"CAPT_NAME_LAST",
"CATCH_DC",
"CATCH_DE",
"CATCH_SEQ",
"CATCH_SOURCE_NAME",
"CATCH_SOURCE",
"CATCH_SPECIES_ITIS",
"CATCH_UE",
"COMMON_NAME",
"CONFIRMATION_SIGNATURE",
"DC",
"DE",
"DEA_PERMIT_ID",
"DEPTH",
"DISPOSITION_CODE",
"DISPOSITION_NAME",
"EFFORT_SEQ",
"EFFORT_TARGET_COMMON_NAMES",
"EFFORT_TARGET_SPECIES_LIST",
"EVENT_ID",
"FISHING_GEAR_DEPTH",
"FISHING_HOURS",
"FORM_VERSION",
"FUEL_DIESEL_GALLON_PRICE",
"FUEL_DIESEL_GALLONS",
"FUEL_GALLON_PRICE",
"FUEL_GALLONS",
"FUEL_GAS_GALLON_PRICE",
"FUEL_GAS_GALLONS",
"GEAR_CATEGORY_CODE",
"GEAR_CATEGORY_NAME",
"GEAR_CODE",
"GEAR_DESC",
"GEAR_NAME",
"GEAR_QUANTITY",
"GEAR_SIZE",
"GEAR_TYPE_CODE",
"GEAR_TYPE_NAME",
"GEARS_FISHING",
"GRADE_CODE",
"GRADE_NAME",
"HOOKS_PER_LINE",
"HOURS_DAYS_FLAG",
"IN_STATE",
"LANDING_SEQ",
"LMA_CODE",
"MARKET_CATEGORY_CODE",
"MARKET_CATEGORY_NAME",
"MAXIMUM_BOTTOM_DEPTH",
"MESH_RING_LENGTH",
"MESH_RING_WIDTH",
"MINIMUM_BOTTOM_DEPTH",
"NBR_OF_CREW",
"NBR_PAYING_PASSENGERS",
"NUM_ANGLERS",
"REPORTED_QUANTITY",
"REPORTING_SOURCE",
"RIG_CODE",
"SPECIES_ITIS",
"SPLIT_TRIP",
"STRETCH_SIZE",
"SUB_TRIP_TYPE",
"SUBMIT_METHOD",
"SUBMITTED_BY_CORPORATE_NAME",
"SUBMITTED_BY_FIRST_NAME",
"SUBMITTED_BY_LAST_NAME",
"SUBMITTED_BY_MIDDEL_NAME",
"SUBMITTED_BY_PARTICIPANT",
"SUPPLIER_EFFCAT_ID",
"SUPPLIER_TRIP_ID",
"TICKET_TYPE",
"TRANSMISSION_DATE",
"TRIP_END_TIME",
"TRIP_FEE",
"TRIP_NBR",
"TRIP_START_TIME",
"TRIP_TYPE",
"UC",
"UE",
"UNIT_MEASURE")

## From DB ====

## file.exists(file.path(my_paths$git_r,
##                       r"(get_data\all_logbooks_db_data_2022_short_p_region_prep.R)"))

## source(file.path(my_paths$git_r, r"(get_data\all_logbooks_db_data_2022_short_p_region_prep.R)"))
```

## Data from FHIER 

```{r Data from FHIER}
## Data from FHIER ----
```

### Reports / SAFIS Efforts Extended 

```{r Reports _ SAFIS Efforts Extended}
### Reports / SAFIS Efforts Extended ----

upload_effort_files <- function(add_path) {
  full_path_to_files <-
    file.path(my_paths$inputs,
              add_path)

  csv_names_list <-
    list.files(path = full_path_to_files,
               pattern = "SAFIS EFFORTS EXTENDED *",
               full.names = T)

  efforts_extended <-
    load_csv_names_in_one_df(NULL, csv_names_list) |>
    distinct()

  return(efforts_extended)
}
```

#### 2022 

```{r 2022}
#### 2022 ----
add_path <- r"(from_Fhier\SAFIS Efforts Extended\2022__09_08_2023)"
safis_efforts_extended_2022 <- upload_effort_files(add_path)

dim(safis_efforts_extended_2022)
## [1] 101038     42
## [1] 97970    42 distinct()

## safis_efforts_extended_2022 |> select(LOCAL_AREA_NAME) |> distinct()

## data_overview(safis_efforts_extended_2022)
## TRIP_ID              97848
## VESSEL_OFFICIAL_NBR   1943
## LATITUDE             69913
## LONGITUDE            70682
## LOCAL_AREA_CODE         49
```

#### 2023 

```{r 2023}
#### 2023 ----
add_path <- r"(from_Fhier\SAFIS Efforts Extended\2023__09_13_2023)"
safis_efforts_extended_2023 <- upload_effort_files(add_path)

dim(safis_efforts_extended_2023)
## [1] 42378    42
```

#### clean fhier effort data 

```{r clean fhier effort data}
#### clean fhier effort data ----
## safis_efforts_extended_2022_short0 <-
##   safis_efforts_extended_2022 |>
##   select(-all_of(names(empty_cols)))
## dim(safis_efforts_extended_2022_short0)

safis_efforts_extended_2022_short <-
  safis_efforts_extended_2022 |>
  select(-any_of(rm_columns)) |>
  distinct()
dim(safis_efforts_extended_2022_short)
## [1] 97970    17
```

## get other geographical data 

```{r get other geographical data}
## get other geographical data ----
read_shapefile <- function(filename) {
  shapefile_file_name <- file.path(my_paths$inputs, "shapefiles", filename)

  x <- sf::read_sf(shapefile_file_name)
  return(x)
}

## https://www.fisheries.noaa.gov/resource/map/defined-fishery-management-areas-south-atlantic-states-map-gis-data

sa_shp <- read_shapefile(r"(shapefiles_sa_eez_off_states\SA_EEZ_off_states.shp)"
)
```

## all GOM 

```{r all GOM}
## all GOM ----
# gom_reef_shp <- read_shapefile(r"(gom\ReefFish_EFH_GOM\ReefFish_EFH_GOM.shp)")

gom_fed <- read_shapefile(r"(GOM_FedWatersBoundary\MSA_FMC_GOM_FedWaters.shp)")

## plot(gom_fed)
## mapview(gom_fed)

## doesn't work:
## gom_only <- st_difference(atmx_eez_shp, sa_shp)
## gom_only <- st_difference(atmx_eez_shp, sa_shp$geometry)
## Warning message:
## attribute variables are assumed to be spatially constant throughout all geometries
## mapview(gom_only,
        ## legend = F)

## gom_bath_shp <- read_shapefile(r"(gom\bathyc\bathyc.shp)")
## plot(gom_bath_shp)
## mapview(gom_bath_shp)

## Protraction Diagram Polygons for the Gulf of Mexico OCS
## gom_protrac_shp <- read_shapefile(r"(gom\protrac_nad83\protrac_nad83.shp)")
## mapview(gom_protrac_shp)

# Sys.setenv(SHAPE_RESTORE_SHX = "YES")
## works Atlantic + GOM:
# atmx_eez_shp <- read_shapefile(r"(atmx_eez/atmx_eez.shp)")
## mapview(atmx_eez_shp, legend = F)

# tic("all_atlantic_n_gom_map")
# all_atlantic_n_gom_map <-
#   mapview(atmx_eez_shp, legend = F) +
#   mapview(sa_shp) +
#   mapview(gom_reef_shp)
# toc()

## gom_depth_shp <- read_shapefile("gom/w98e78n31s18_isobath_selected_5-4000m/w98e78n31s18_isobath_selected_5-4000m.shp")
## plot(gom_depth_shp)

## useez <- read_shapefile("Downloads/useez.shp")

## gom_depth_shp5_100 <- read_shapefile("gom/w98e78n31s18_isobath_5-100m/w98e78n31s18_isobath_5-100m.shp")
## plot(gom_depth_shp5_100)

## gom_depth_shp100_1000 <- read_shapefile("gom/w98e78n31s18_isobath_100-1000m/w98e78n31s18_isobath_100-1000m.shp")
## plot(gom_depth_shp100_1000)

## gom_depth_shp500_4000 <- read_shapefile("gom/w98e78n31s18_isobath_500-4000m/w98e78n31s18_isobath_500-4000m.shp")

## ===
## fl_state_land_waters <- read_shapefile("Florida_State_Waters_and_Land_Boundary/Florida_State_Waters_and_Land_Boundary.shp")

## mapview(fl_state_land_waters)
```

#### atl_state_waters 

```{r atl_state_waters}
#### atl_state_waters ----
## https://catalog.data.gov/dataset/outer-continental-shelf-submerged-lands-act-boundary-atlantic-region-nad83
## Outer Continental Shelf Submerged Lands Act Boundary - Atlantic Region NAD83

## atl_state_waters <- read_shapefile("ATL_SLA/ATL_SLA.shp")
## mapview(atl_state_waters)
```

#### fl_state_w_counties 

```{r fl_state_w_counties}
#### fl_state_w_counties ----
fl_state_w_counties_shp <- read_shapefile(r"(GOVTUNIT_Florida_State_Shape\Shape\GU_CountyOrEquivalent.shp)")

## mapview(fl_state_w_counties_shp)
```

##### Current file: fishing_effort_location_by_permit.R 

```{r Current file: fishing_effort_location_by_permit.R}
##### Current file: fishing_effort_location_by_permit.R ----
```

## Requirements 

```{r Requirements}
## Requirements ----
## information on location of relative fishing effort.  The relative would be looking by depth, area, and seasonally.
## filter out beyond state waters for trips north of 28N.  All charter trips south of 28N to the SAFMC/GMFMC boundary.
## --- OK boundaries
## lat 23 : 36
## lon -71 : -98
## SAFMC/GMFMC boundary
## see https://myfwc.com/fishing/saltwater/recreational/maps/
## 83 west (federal waters) / 24'35 N, 24'33 N (state waters)
## to get SA only:
## filter out beyond state waters for trips north of 28N.  All charter trips south of 28N to the SAFMC/GMFMC boundary.
## fields to get
## Trip start and end date
## Start Port: Code, Name, County and/or State, State Code, Postal Code - no
## End Port: Code,
## Longitude and Latitude fields:  Lat and long as specific as possible
## - Fishing Area Code, Sub Area Code, Distance Code and Distance Code Name?
## depth
## south of 28N - all SA
## OK boundaries
## lat 23 : 28
## lon -71 : -83

## north of 28N - EEZ only
```

## setup (fishing_effort_location_by_permit) 

```{r setup (fishing_effort_location_by_permit)}
## setup (fishing_effort_location_by_permit) ----

## Load the 'zoo' package for date manipulations
library(zoo)

## Load the 'sf' package to create sf (simple features) objects for working with spatial data
library(sf)

## Load the 'mapview' package to view spatial objects interactively
library(mapview)

## Load the 'leaflet' package for creating interactive web maps
library(leaflet)

## Load the 'tictoc' package for benchmarking and measuring code execution time
library(tictoc)

## Load the 'stringi' package for manipulating and working with character strings
library(stringi)

## Load the 'htmltools' package for working with HTML content in R
library(htmltools)

## Define the name of the current project as "fishing_effort_location."
current_project_name <- "fishing_effort_location"

## Source another external R script using 'file.path' to construct the full file path.
get_data_path <-
  file.path(## Path to the git repository directory
    my_paths$git_r,
    ## Subdirectory for the current project
    current_project_name,
    ## Name of the R script to source
    "fishing_effort_locations_get_data.R")

## source(get_data_path)

## Define an R function 'my_to_sf' for converting a data frame to an sf object.
my_to_sf <- function(my_df) {
  my_df %>%
    ## Convert the data frame to an sf object using st_as_sf from the 'sf' package
    sf::st_as_sf(
      ## Specify the columns containing longitude and latitude as coordinates
      coords = c("longitude", "latitude"),
      ## Set the coordinate reference system (CRS) using the 'sa_shp' object
      crs = sf::st_crs(sa_shp),
      ## Keep the original LATITUDE and LONGITUDE columns in the resulting sf object
      remove = FALSE
    ) %>%
    ## Return the resulting sf object
    return()
}

## to avoid this error:
##   Loop 0 is not valid: Edge 57478 has duplicate vertex with edge 57482
sf::sf_use_s2(FALSE)

## Define an R function 'with_st_intersection' for calculating the intersection between two spatial objects.
with_st_intersection <- function(points_sf, polygons_sf) {
  ## browser()  ## Uncomment this line to enable debugging using 'browser()'

  ## Get the parameter names for the input spatial objects
  par1 <- rlang::enexpr(points_sf)
  par2 <- rlang::enexpr(polygons_sf)

  ## Start measuring the execution time
  tic(paste0("sf::st_intersection(", par1, ", ", par2, ")"))

  ## Calculate the intersection between 'points_sf' and 'polygons_sf'
  res <- sf::st_intersection(points_sf, polygons_sf)

  ## Print the execution time
  toc()

  ## Return the result of the intersection calculation
  return(res)
}

## run st_difference with benchmark
## Define an R function 'with_st_difference' for calculating the difference between two spatial objects.
with_st_difference <- function(points_sf, polygons_sf) {
  ## browser()  ## Uncomment this line to enable debugging using 'browser()'

  ## Get the parameter names for the input spatial objects
  par1 <- rlang::enexpr(points_sf)
  par2 <- rlang::enexpr(polygons_sf)

  ## Start measuring the execution time with a message
  tic(paste0("sf::st_difference(", par1, ", ", par2, ")"))

  ## Calculate the difference between 'points_sf' and 'polygons_sf'
  res <- sf::st_difference(points_sf, polygons_sf)

  ## Print the execution time
  toc()

  ## Return the result of the difference calculation
  return(res)
}
```

### functions for ten_min 

```{r functions for ten_min}
### functions for ten_min ----

## Define an R function 'get_degree' to extract degrees from geographical coordinates.
get_degree <- function(gis_coord) {
  ## Take the absolute value and round down to the nearest integer to get degrees.
  floor(abs(gis_coord))
}

## These R functions are used for converting between different formats of geographical coordinates.
## Define an R function 'get_minute' to extract minutes from geographical coordinates.
get_minute <- function(gis_coord) {
  ## Take the absolute value, find the fractional part, and convert to minutes.
  dd <- abs(gis_coord) %% 1
  minutes <- floor(dd * 60)
  return(minutes)
}

## Define an R function 'convert_to_ten_min' to convert minutes to the nearest multiple of ten.
convert_to_ten_min <- function(minute) {
  ## Divide minutes by 10, round down to the nearest multiple of 10.
  floor(minute/10) * 10
}

## Define an R function 'convert_to_decimal_degree' to convert degrees and minutes to decimal degrees.
convert_to_decimal_degree <- function(dm_num) {
  ## Extract the degrees part (first two positions) and convert it to a numeric value.
  degree <- as.numeric(substr(as.character(dm_num), 1, 2))

  ## Extract the minutes part (characters in positions 3 and 4), divide by 60, and convert to a numeric value.
  dd <- as.numeric(substr(as.character(dm_num), 3, 4)) / 60

  ## Calculate the decimal degrees by adding degrees and minutes.
  degree + dd
}

## Define an R function 'get_lat_ten_min' for rounding latitude coordinates to the nearest ten minutes and converting to decimal degrees.
get_lat_ten_min <- function(gis_lat) {
  ## Get the degrees component from latitude
  deg <- get_degree(gis_lat)

  ## Get the minutes component from latitude
  minute <- get_minute(gis_lat)

  ## Round minutes to the nearest ten minutes
  ten_min_num <- convert_to_ten_min(minute)

  ## Create a string representing degrees and ten minutes (e.g., "DDMM")
  dm_num <- paste(deg, stringi::stri_pad_left(ten_min_num, 2, 0), sep = "")

  ## Convert the string representation to decimal degrees
  convert_to_decimal_degree(dm_num)
}

## All lon should be negative, bc we know it is all in America
## Define an R function 'get_ten_min_coords' for rounding coordinates to the nearest ten minutes.
get_ten_min_coords <- function(my_df) {
  ## Create a new data frame 'ten_min_df' based on the input data frame 'my_df'
  ten_min_df <-
    my_df |>
    dplyr::mutate(
      ## Create a new column 'ten_min_lat' by calling 'get_lat_ten_min' on the 'LATITUDE' column
      ten_min_lat = get_lat_ten_min(as.numeric(my_df$LATITUDE)),

      ## Create a new column 'ten_min_lon' by rounding and negating 'LONGITUDE' to ensure it's negative
      ten_min_lon = -1 * abs(get_lat_ten_min(as.numeric(my_df$LONGITUDE)))
    )

  ## Return the resulting 'ten_min_df'
  return(ten_min_df)
}
```

### Data from db as in FHIER 

```{r Data from db as in FHIER}
### Data from db as in FHIER ----
## print_df_names(all_logbooks_db_data_2022_short_p_region)
## [1] 94471    73

## Create a new data frame by processing the data.

coord_data_2022_short_good_all_coords <-
  all_logbooks_db_data_2022_short_p_region |>

  ## Convert 'longitude' and 'latitude' columns to numeric data types
  dplyr::mutate(longitude = as.numeric(longitude),
                latitude = as.numeric(latitude)) |>

  ## Ensure that all 'longitude' values are negative by taking the absolute value and negating them
  dplyr::mutate(longitude = -abs(longitude)) |>

  ## Keep only distinct rows in the data frame
  distinct()

dim(coord_data_2022_short_good_all_coords)
## [1] 94471    73
```

### From FHIER 

```{r From FHIER}
### From FHIER ----
## View(coord_data_2022_short)
#
## coord_data_2022_short_good_all_coords_fhier <-
##   ## Convert 'longitude' and 'latitude' columns to numeric data types
##   dplyr::mutate(longitude = as.numeric(longitude),
##                 latitude = as.numeric(latitude)) |>
#
##   ## Ensure that all 'longitude' values are negative by taking the absolute value and negating them
##   dplyr::mutate(longitude = -abs(longitude)) |>
#
##   ## Keep only distinct rows in the data frame
##   distinct()
#
## dim(coord_data_2022_short_good_all_coords)
## [1] 97970    17
```

## Keep only full sets of coordinates 

```{r Keep only full sets of coordinates}
## Keep only full sets of coordinates ----
## Create a new data frame 'coord_data_2022_short_good' by filtering and keeping only distinct rows.
coord_data_2022_short_good <-
  coord_data_2022_short_good_all_coords |>

  ## Use the filter function to keep rows where either 'longitude' or 'latitude' is not NA
  dplyr::filter(!is.na(longitude) | !is.na(latitude)) |>

  ## Keep only distinct rows in the data frame
  distinct()

dim(coord_data_2022_short_good_all_coords)
## [1] 97970    17 FHIER
## [1] 97943    16 db

dim(coord_data_2022_short_good)
## [1] 97547    17
## [1] 93450    73 db
```

#### convert to sf 

```{r convert to sf}
#### convert to sf ----
coord_data_2022_short_good_sf <-
  my_to_sf(coord_data_2022_short_good)
```

## show all boundaries 

```{r show all boundaries}
## show all boundaries ----
```

## subset by Big box 

```{r subset by Big box}
## subset by Big box ----
## Michelle: I think we need to allow trips that occur anywhere in the GOM, with the eastern lat border being like a big line down the Atlantic Ocean at Bermuda. Does that make sense? Southern Border could be at Cuba. The Northern Border needs to extend up through Maine - since we require reporting no matter where they fish. Basically just a big box, regardless of Council jurisdiction.
## Jessica: I like the big box without council jurisdiction and then I am going to assume we will just plot those trips for vessels with GOM permits?  This should show the Council how many GOM vessels also fish in other regions as well as where they are fishing in the Gulf.

## Define a bounding box represented as a vector named 'big_bounding_box'.
big_bounding_box <- c(
   xmin = -97.79954,  ## Minimum longitude (western boundary)
   ymin = 21.521757,  ## Minimum latitude (southern boundary, Cuba)
   xmax = -64.790337,  ## Maximum longitude (eastern boundary, Bermuda)
   ymax = 49  ## Maximum latitude (northern boundary, Canada)
)

tic("coord_data_2022_short_good_sf_crop_big")
## Create a new spatial object 'coord_data_2022_short_good_sf_crop_big'
## by cropping 'coord_data_2022_short_good_sf' to the 'big_bounding_box'.

coord_data_2022_short_good_sf_crop_big <-
  sf::st_crop(coord_data_2022_short_good_sf,  ## Spatial object to be cropped
              big_bounding_box)  ## Bounding box used for cropping
toc()
## coord_data_2022_short_good_sf_crop_big: 0.89 sec elapsed

dim(coord_data_2022_short_good_sf_crop_big)
## [1] 95720    18
## [1] 91735    74
```

## convert back to df 

```{r convert back to df}
## convert back to df ----
## The sf::st_drop_geometry function is applied to the spatial object to remove its geometry, resulting in a data frame that contains only the non-geometric attributes or columns.

coord_data_2022_short_good_sf_crop_big_df <-
  coord_data_2022_short_good_sf_crop_big |>
  sf::st_drop_geometry()

dim(coord_data_2022_short_good_sf_crop_big_df)
## [1] 95720     17
```

## use metrics only vessels not in SRHS 

```{r use metrics only vessels not in SRHS}
## use metrics only vessels not in SRHS ----
## source(r"(~\R_code_github\get_data\get_data_from_fhier\metric_tracking_no_srhs.R)")
## fhier_reports_metrics_tracking_not_srhs_ids
```

### remove ids not in fhier_reports_metrics_tracking_not_srhs_ids 

```{r remove ids not in fhier_reports_metrics_tracking_not_srhs_ids}
### remove ids not in fhier_reports_metrics_tracking_not_srhs_ids ----
## Create a new data frame 'coord_data_2022_short_good_sf_crop_big_df_in_metricks'
## by filtering 'coord_data_2022_short_good_sf_crop_big_df' based on a condition.

coord_data_2022_short_good_sf_crop_big_df_in_metricks <-
  coord_data_2022_short_good_sf_crop_big_df |>

  ## Use the 'filter' function to retain rows meeting a specific condition.
  dplyr::filter(
    ## Keep rows where the 'vessel_official_nbr' values are present in the fhier_reports_metrics_tracking_not_srhs_ids
    vessel_official_nbr %in% fhier_reports_metrics_tracking_not_srhs_ids$vessel_official_number
  )

dim(coord_data_2022_short_good_sf_crop_big_df_in_metricks)
## [1] 93581    17
## [1] 90838    73 from mv
```

# Heatmap preparations

```{r}
library(ggplot2) ## a visualization package
library(ggmap) ## extends 'ggplot2' for creating maps and working with spatial data.
```

##### Current file: prepare_gom_heatmap_func.R 

```{r Current file: prepare_gom_heatmap_func.R}
##### Current file: prepare_gom_heatmap_func.R ----

## Load the 'tigris' package to access geographic data.
library(tigris)

## Set the 'tigris_use_cache' option to TRUE. This will enable caching of
## data retrieved from the TIGER/Line Shapefiles service, which can help
## improve data retrieval performance for future sessions.
tigris_use_cache = TRUE
```

## plot text sizes 

```{r plot text sizes}
## plot text sizes ----
text_sizes <- list(
  geom_text_size = 7,
  plot_title_text_size = 10,
  axis_title_text_size = 9,
  axis_text_x_size = 13,
  axis_text_y_size = 13,
  plot_caption_text_size = 13,
  legend_title_text_size = 10,
  legend_text_text_size = 9,
  #### common axes for Months ----
  y_left_fontsize = 10
)
```

## read in sa shp 

```{r read in sa shp}
## read in sa shp ----
## F2 in RStudio will show the function definition, when the cursor is on the name.
## Read a shapefile (geospatial data) from the specified file path and store it in the 'sa_shp' object.
sa_shp <-
  read_shapefile(r"(shapefiles_sa_eez_off_states\SA_EEZ_off_states.shp)")

## The South Atlantic Council is responsible for the conservation and management of fishery resources in federal waters ranging from 3 to 200 miles off the coasts of North Carolina, South Carolina, Georgia, and east Florida to Key West.

states_sa <- data.frame(
  state_name = c(
    "Florida", ## can exclude, if go by county
    "Georgia",
    "North Carolina",
    "South Carolina"
  )
)

## Create a data frame 'state_tbl' containing state abbreviations and state names; 2x50.
## - 'state.abb' provides state abbreviations.
## - 'tolower(state.name)' converts state names to lowercase.
## - The resulting data frame has two columns: 'state_abb' and 'state_name'.
state_tbl <- data.frame(state.abb, tolower(state.name))

## Rename the columns in the 'state_tbl' data frame.
## The first column is named 'state_abb', and the second column is named 'state_name'.
names(state_tbl) = c("state_abb", "state_name")

#from the DF, only grab the SA states defined above
sa_state_abb <-
  ## a table from above
  state_tbl %>%
  ## get only these in our list
  filter(state_name %in% tolower(states_sa$state_name)) %>%
  ## get abbreviations
  select(state_abb)
```

### r get Shapefile all waters 

```{r r get Shapefile all waters}
### r get Shapefile all waters ----
path_to_federal_state_w <-
  file.path(
    my_paths$inputs,
    r"(shapefiles\federal_and_state_waters\FederalAndStateWaters.shp)"
  )

file.exists(path_to_federal_state_w)
## T

tic("federal_state_w_sf")
federal_state_w_sf <-
  sf::read_sf(path_to_federal_state_w)
toc()

east_coat_states <- c(
  gom = c("Florida",
          "Texas",
          "Louisiana"),
  sa = c(
    "Alabama",
    "Connecticut",
    "Delaware",
    "Florida",
    "Georgia",
    "Maine",
    "Maryland",
    "Massachusetts",
    "Mississippi",
    "New Hampshire",
    "New Jersey",
    "New York",
    "North Carolina",
    "Pennsylvania",
    "Rhode Island",
    "South Carolina",
    "Virginia",
    "Washington DC"
  )
)

## Create a new data frame 'federal_state_w_sf_east' by filtering the existing data frame 'federal_state_w_sf'.
## Rows are retained if the 'Jurisdicti' column matches any of the values in 'east_coat_states'.
federal_state_w_sf_east <-
  federal_state_w_sf |>
  filter(Jurisdicti %in% east_coat_states)

## Create a new data frame 'us_s_shp' using the 'tigris' package to obtain U.S. state shapes.
## The 'cb = TRUE' parameter specifies that you want the U.S. state boundaries.
us_s_shp <-
  tigris::states(cb = TRUE, progress_bar = FALSE)

## Rows are retained if the 'NAME' column (state name) matches any of the values in 'states_sa'.
sa_s_shp <-
  us_s_shp |>
  filter(NAME %in% states_sa$state_name)

## Create a new data frame 'us_s_shp' using the 'tigris' package to obtain U.S. state shapes.
## The 'cb = TRUE' parameter specifies that you want the U.S. state boundaries.
us_s_shp <-
  tigris::states(cb = TRUE, progress_bar = FALSE)

## Rows are retained if the 'NAME' column (state name) matches any of the values in 'states_sa'.
sa_s_shp <-
  us_s_shp |>
  filter(NAME %in% states_sa$state_name)
```

## read in GOM shp 

```{r read in GOM shp}
## read in GOM shp ----
## Create a file path using 'file.path' by combining elements from 'my_paths' and specifying a shapefile path.
GOM_400fm_path <-
  file.path(my_paths$inputs,
                      r"(shapefiles\GOM_heatmap_from Kyle\GOM_400fm\GOM_400fm.shp)")

# file.exists(GOM_400fm_path)
# T

## Read a shapefile from the specified file path using 'sf::read_sf'.
## Then, group the resulting data by 'StatZone' and summarize it.
GOMsf <-
  sf::read_sf(GOM_400fm_path) %>%
  group_by(StatZone) %>%
  summarise()
```

## create 5x5 minute grid 

```{r create 5x5 minute grid}
## create 5x5 minute grid ----
## Define a function 'min_grid' that creates a grid of cells within the bounding box of a given spatial data frame.
## - 'my_sf' is the input spatial data frame (default is 'GOMsf').
## - 'minute_num' specifies the grid cell size in minutes.

min_grid <- function(my_sf = GOMsf, minute_num = 1) {
  ## Create a grid of cells using 'sf::st_make_grid' within the bounding box of 'my_sf'.
  grid <-
    sf::st_make_grid(x = sf::st_bbox(my_sf),
                     cellsize = 1 / 60 * minute_num) %>%

    ## Convert the grid to a spatial data frame using 'sf::st_as_sf'.
    sf::st_as_sf() %>%

    ## Add a 'cell_id' column to the grid using 'mutate'.
    mutate(cell_id = 1:nrow(.))

  ## Return the created grid.
  return(grid)
}

grid_gom5 <- min_grid(GOMsf, 5)
grid_sa5 <- min_grid(sa_shp, 5)

## Set the aggregate attribute to "constant" for multiple spatial objects.
sf::st_agr(GOMsf) =
  sf::st_agr(sa_shp) =
  sf::st_agr(grid_gom5) =
  sf::st_agr(grid_sa5) =
  "constant"
```

#### remove internal boundaries from the GOM shape file 

```{r remove internal boundaries from the GOM shape file}
#### remove internal boundaries from the GOM shape file ----

tic("st_union(GOMsf)")
st_union_GOMsf <- sf::st_union(GOMsf)
toc()
## st_union(GOMsf): 21.59 sec elapsed
```

### by n min grid 

```{r by n min grid}
### by n min grid ----
## Define a function 'df_join_grid' that joins a data frame with a grid using specified coordinates and CRS.

df_join_grid <- function(my_df, grid, my_crs) {
  ## Convert 'my_df' to a spatial data frame with specified coordinates and CRS using 'sf::st_as_sf'.
  my_df_grid <-
    my_df |>
    sf::st_as_sf(
      coords = c("longitude", "latitude"),
      crs = my_crs) |>

  ## Join the resulting spatial data frame with the 'grid' using the nearest feature join.
  sf::st_join(grid, join = sf::st_nearest_feature)

  ## Return the joined data frame.
  return(my_df_grid)
}

## Define a function 'crop_by_shape' that crops a spatial object using another spatial object.
## - 'my_sf' is the input spatial object to be cropped.
## - 'my_shp' is the spatial object used for cropping (default is 'GOMsf').

crop_by_shape <- function(my_sf, my_shp = GOMsf) {
  ## Join 'my_sf' with 'my_shp' to crop it, leaving only the intersecting geometries.
  my_sf |>
    sf::st_join(my_shp, left = FALSE) %>%

  ## extract the longitude and latitude coordinates from the joined spatial object.
  dplyr::mutate(longitude = sf::st_coordinates(.)[, 1],
         latitude = sf::st_coordinates(.)[, 2]) %>%

  ## Return the cropped and transformed spatial object.
  return()
}
```

### count trip ids and vessels by grid cell function 

```{r count trip ids and vessels by grid cell function}
### count trip ids and vessels by grid cell function ----
## Define a function 'add_vsl_and_trip_cnts' that adds vessel and trip counts to a data frame.
## - 'my_df' is the input data frame.
## - 'vessel_id_name' is the name of the column containing vessel IDs (default is "vessel_official_nbr").

add_vsl_and_trip_cnts <- function(my_df, vessel_id_name = "vessel_official_nbr") {
  ## Group the data frame by 'cell_id'.
  my_df |>
    group_by(cell_id) |>

  ## Add columns 'vsl_cnt' and 'trip_id_cnt' with counts of distinct vessel and trip IDs.
    ## sym() take strings as input and turn them into symbols.
    ## The !! (bang-bang or unquote) operator is used to unquote the symbol, allowing it to be used in dplyr verbs like mutate, select, or other functions that accept column names.
    ## So, the code !!rlang::sym(vessel_id_name) effectively evaluates to the column name specified by the vessel_id_name variable in the context of a dplyr verb, allowing you to work with the column dynamically based on the variable's value.

    dplyr::mutate(
      vsl_cnt =
        dplyr::n_distinct(!!rlang::sym(vessel_id_name)),
      trip_id_cnt =
        dplyr::n_distinct(trip_id)
    ) |>

  ## Ungroup the data frame to remove grouping and return the result.
  dplyr::ungroup() %>%

  ## Return the modified data frame.
  return()
}
```

### make a plot function 

```{r make a plot function}
### make a plot function ----
## Define a function 'make_map_trips' to create a ggplot2 heatmap of trip data.
## - 'map_trip_base_data' is the data containing trip information to be mapped.
## - 'shape_data' is the shape data used as a backdrop for mapping.
## - 'total_trips_title' is the title for the total trips legend.
## - 'trip_cnt_name' is the name of the column with trip counts.
## - 'caption_text' is the caption for the plot.
## - 'unit_num' specifies the unit size for the legend.
## - 'print_stat_zone' is an optional argument to include StatZone labels.
make_map_trips <-
  function(map_trip_base_data,
           shape_data,
           total_trips_title,
           trip_cnt_name,
           caption_text = "Heat map of SEFHIER trips (5 min. resolution).",
           unit_num = 1,
           print_stat_zone = NULL,
           legend_text_text_size = text_sizes[["legend_text_text_size"]]
           ) {
    ## Calculate the maximum number of trips for legend scaling.
    max_num <- max(map_trip_base_data[[trip_cnt_name]])

    ## Create a ggplot2 plot 'map_trips'.
    map_trips <-
      ggplot() +
      ## Add a filled heatmap using 'geom_sf'.
      geom_sf(data = map_trip_base_data,
              aes(geometry = x,
                  fill = !!sym(trip_cnt_name)),
              colour = NA) +
      ## Add the shape data with no fill.
      geom_sf(data = shape_data, fill = NA)

    ## Check for an optional argument 'print_stat_zone'.
    if (!missing(print_stat_zone)) {
      map_trips <-
        map_trips +
        ## Add StatZone labels using 'geom_sf_text'.
        geom_sf_text(data = shape_data,
                     aes(geometry = geometry,
                         label = StatZone),
                     size = 3.5)
    }

    map_trips <-
        map_trips +
      ## Set plot labels and theme settings.
      labs(
        x = "",
        y = "",
        fill = "",
        caption = caption_text
      ) +
      ## theme_bw() +
      scale_fill_viridis(
        name = total_trips_title,
        labels = scales::comma,
        trans = "log1p",
        limits = c(1, max_num)
      ) +
      theme(
        legend.position = "top",
        legend.justification = "left",
        legend.key.width = unit(unit_num, "npc"),
        legend.title = element_text(size =
                                      text_sizes[["legend_title_text_size"]]),
        legend.text = element_text(size =
                                     legend_text_text_size), ## for charter heatmap use 7
        plot.caption = element_text(hjust = 0,
                                    size = text_sizes[["plot_caption_text_size"]]),
    axis.text.x =
      element_text(size = text_sizes[["axis_text_x_size"]]),
axis.text.y =
      element_text(size = text_sizes[["axis_text_y_size"]])
      ) +
      ## Add a legend guide for fill color.
      guides(fill = guide_colourbar(title.position = "top"))

    ## Return the created 'map_trips' plot.
    return(map_trips)
  }
```

##### Current file: fishing_effort_location_heatmap.R 

```{r Current file: fishing_effort_location_heatmap.R}
##### Current file: fishing_effort_location_heatmap.R ----

## Use the 'source' function to execute R code from a file located at the given path.
```

## setup for fishing_effort_location_heatmap 

```{r setup for fishing_effort_location_heatmap}
## setup for fishing_effort_location_heatmap ----
library(viridis) ## additional color palettes
```

## Heatmap 

```{r Heatmap}
## Heatmap ----
```

### heatmap data 

```{r heatmap data}
### heatmap data ----

## Split the data frame into multiple sub-data frames based on the 'permit_region' column.

coord_data_2022_short_good_sf_crop_big_df_in_metricks_list <-
  split(
    ## Data frame to be split
    coord_data_2022_short_good_sf_crop_big_df_in_metricks,

    ## Split based on the 'permit_region' column
    as.factor(
      coord_data_2022_short_good_sf_crop_big_df_in_metricks$permit_region
    )
  )

## Use the 'map' function to apply the 'dim' function to each element in the list.
purrr::map(
  coord_data_2022_short_good_sf_crop_big_df_in_metricks_list,
  dim
)

## gom
## Create a new data frame 'for_heatmap_lat_lon_trips_vessels_gom_only' by applying a series of data manipulation operations.

for_heatmap_lat_lon_trips_vessels_gom_only <-
  coord_data_2022_short_good_sf_crop_big_df_in_metricks_list$gom_and_dual |>

  ## Select specific columns.
  select(trip_id, vessel_official_nbr, latitude, longitude) |>

  ## Remove duplicate rows using 'distinct'.
  distinct()

dim(for_heatmap_lat_lon_trips_vessels_gom_only)
## Rows: 41,455
## [1] 46763     4 mv

## sa
for_heatmap_lat_lon_trips_vessels_sa_only <-
  coord_data_2022_short_good_sf_crop_big_df_in_metricks_list$sa_only |>
  ## Select specific columns.
  select(trip_id, vessel_official_nbr, latitude, longitude) |>
  ## Remove duplicate rows using 'distinct'.
  distinct()

dim(for_heatmap_lat_lon_trips_vessels_sa_only)
## [1] 68122     4
## [1] 44060     4
```

#### remove vessels not in Jeannette's SA list 

```{r remove vessels not in Jeannette_s SA list}
#### remove vessels not in Jeannette's SA list ----

## Build the path to the R script 'vessel_permit_corrected_list.R' by
## combining the base path 'my_paths$git_r' and the script name.
script_path <-
  file.path(my_paths$git_r,
            "vessel_permit_list/vessel_permit_corrected_list.R")

## Source (run) the R script using the constructed script path.
## source(script_path)

## Rows are filtered to exclude vessels whose 'VESSEL_OFFICIAL_NBR' is in the
## 'vessels_to_remove_from_ours' vector.
for_heatmap_lat_lon_trips_vessels_sa_only_rm <-
  for_heatmap_lat_lon_trips_vessels_sa_only |>
  filter(!vessel_official_nbr %in% vessels_to_remove_from_ours)

dim(for_heatmap_lat_lon_trips_vessels_sa_only_rm)
## [1] 67983     4
```

### add the grid 

```{r add the grid}
### add the grid ----
## assuming data is dataframe with variables LATITUDE, LONGITUDE, and trips

tic("effort_vsl_gom")
## Create a new object 'my_crs' by extracting the coordinate reference system (CRS)
## from a spatial object 'GOMsf'.
my_crs <- sf::st_crs(GOMsf)
## Create a new data frame 'effort_vsl_gom' by joining the data frames
## 'for_heatmap_lat_lon_trips_vessels_gom_only' and 'grid_gom5' based on a common
## spatial reference system defined by 'my_crs'.
effort_vsl_gom <-
  df_join_grid(for_heatmap_lat_lon_trips_vessels_gom_only,
               grid_gom5,
               my_crs)
toc()
## effort_vsl: 0.62 sec elapsed

tic("effort_vsl_sa")
## Create a new object 'my_crs_sa' by extracting the coordinate reference system (CRS)
## from a spatial object 'sa_shp'.
my_crs_sa <- sf::st_crs(sa_shp)

## Create a new data frame 'effort_vsl_sa' by joining the data frames
## 'for_heatmap_lat_lon_trips_vessels_sa_only_rm' and 'grid_sa5' based on a
## spatial reference system defined by 'my_crs_sa'.
effort_vsl_sa <-
  df_join_grid(for_heatmap_lat_lon_trips_vessels_sa_only_rm,
               grid_sa5,
               my_crs = my_crs_sa)
toc()
## effort_vsl_sa: 1.22 sec elapsed
```

### crop by the shape 

```{r crop by the shape}
### crop by the shape ----
tic("effort_vsl_cropped_gom")
effort_vsl_cropped_gom <- crop_by_shape(effort_vsl_gom)
toc()
## effort_cropped2: 0.44 sec elapsed

dim(effort_vsl_cropped_gom)
## [1] 35822     7
## [1] 40604     7 mv

tic("effort_vsl_cropped_sa")
effort_vsl_cropped_sa <- crop_by_shape(effort_vsl_sa, sa_shp)
toc()
## effort_vsl_cropped_sa: 0.54 sec elapsed

dim(effort_vsl_cropped_sa)
## [1] 21461     8
## [1] 20147     8 mv
```

### count trip ids and vessels by grid cell 

```{r count trip ids and vessels by grid cell}
### count trip ids and vessels by grid cell ----

## Create a list 'effort_vsl_cropped_cnt_l' by applying 'add_vsl_and_trip_cnts' function to data frames.

effort_vsl_cropped_cnt_l <-
  list(effort_vsl_cropped_gom, effort_vsl_cropped_sa) |>

  ## Use the 'map' function to apply a function to each element in the list.
  purrr::map(function(effort_vsl_cropped) {

    ## Apply the 'add_vsl_and_trip_cnts' function to each 'effort_vsl_cropped' data frame.
    add_vsl_and_trip_cnts(effort_vsl_cropped)
  })

map(effort_vsl_cropped_cnt_l, dim)
## [[1]]
## [1] 35822     9
#
## [[2]]
## [1] 21461    10
## mv data:
## [[1]]
## [1] 40604     9
#
## [[2]]
## [1] 20147    10
```

#### no rule3 

```{r no rule3}
#### no rule3 ----
## Create a list 'effort_cropped_short_cnt2_short_l' by applying a set of operations to data frames.

effort_cropped_short_cnt2_short_l <-
  effort_vsl_cropped_cnt_l |>

  ## Use the 'map' function to apply a function to each element in the list.
  purrr::map(function(effort_vsl_cropped_cnt) {

    ## Use the 'select' function to remove specific columns,
    ## 'latitude', 'longitude', 'trip_id', and 'VESSEL_OFFICIAL_NBR', from each data frame.
    effort_vsl_cropped_cnt |>
      select(-c(latitude, longitude, trip_id, vessel_official_nbr))
  })

## map(effort_cropped_short_cnt2_short_l, dim)
```

#### no rule 3 

```{r no rule 3}
#### no rule 3 ----
heat.plt_gom <-
  ## Extract the first element from the list.
  ## Use the pipe operator to pass it to the next operation.
  effort_cropped_short_cnt2_short_l[[1]] |>
  ## Perform an inner join with the data frame 'grid_gom5'
  ## using a common column specified by 'join_by(cell_id)'.
  ## Store the result in the variable 'heat.plt_gom'.
  ## Have to use data.frame, to avoid:
  ## Error: y should not have class sf; for spatial joins, use st_join
  inner_join(data.frame(grid_gom5))

## the same for SA
heat.plt_sa <-
  effort_cropped_short_cnt2_short_l[[2]] |>
  ## have to use data.frame, to avoid
  ## Error: y should not have class sf; for spatial joins, use st_join
  inner_join(data.frame(grid_sa5))
## Joining with `by = join_by(cell_id)`
```

### make a plot 

```{r make a plot}
### make a plot ----

max_num3_gom <- max(heat.plt_gom$trip_id_cnt)
## 1209
## 1317 mv

max_num3_sa <- max(heat.plt_sa$trip_id_cnt)
## 590
## 561 mv

#### GOM & dual 2022 ====
map_trips_no_rule_3_gom <-
  make_map_trips(heat.plt_gom,
           st_union_GOMsf,
           "total trips",
           trip_cnt_name = "trip_id_cnt",
           unit_num = 1.2)

map_trips_no_rule_3_gom

#### SA 2022 ====
map_trips_no_rule_3_sa <-
  make_map_trips(heat.plt_sa,
           sa_shp,
           "total trips",
           trip_cnt_name = "trip_id_cnt",
           unit_num = 0.9,
           legend_text_text_size = 7.5)

map_trips_no_rule_3_sa +
## Add the spatial features from 'sa_s_shp' to the plot using 'geom_sf'.
geom_sf(data = sa_s_shp) +

## Annotate the plot with text labels from 'sa_s_shp' using 'geom_sf_text'.
geom_sf_text(data = sa_s_shp,
               label = sa_s_shp$NAME,  ## Use the 'NAME' column as labels.
               size = 3)  ## Set the size of the text labels to 3.
```

## To get end port numbers by state 

```{r To get end port numbers by state}
## To get end port numbers by state ----
permit_end_port_path <-
  file.path(
    my_paths$git_r,
    r"(fishing_effort_location\fishing_effort_location_by_permit_and_end_port.R)"
  )

## source(permit_end_port_path)
```

