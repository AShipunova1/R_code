---
title: "Fishing effort locations heatmap"
---


```{r no cache setup, results='hide', message=FALSE, warning=FALSE, cache=FALSE, include=FALSE}

# A general-purpose tool for dynamic report generation in R
library(knitr)

# Format R code automatically
library(styler)

```

# Prepare data
##### Current file: useful_functions_module.r 

```{r Current file: useful_functions_module.r}
#### Current file: useful_functions_module.r ----

# nolint: commented_code_linter
# useful functions

```
```{r start functions }
# How to use:
# my_paths <- set_work_dir()

#install.packages("tidyverse")
# The tidyverse is a collection of R packages that work together seamlessly for data manipulation, visualization, and analysis. It includes popular packages like dplyr, ggplot2, tidyr, and more, all designed to follow a consistent and "tidy" data processing philosophy.
# ✔ dplyr     1.1.3     ✔ readr     2.1.4
# ✔ forcats   1.0.0     ✔ stringr   1.5.0
# ✔ ggplot2   3.4.4     ✔ tibble    3.2.1
# ✔ lubridate 1.9.3     ✔ tidyr     1.3.0
# ✔ purrr     1.0.2
library(tidyverse)

# Load the 'magrittr' library, which provides piping data and functions.
library(magrittr)

# Load the 'readxl' library, used for reading Excel (.xlsx) files.
library(readxl)

# Load the 'rbenchmark' library, which is used for benchmarking code performance.
library(rbenchmark)

# Load the 'ROracle' library, which provides an interface for working with Oracle databases in R.
library(ROracle)

# Load the 'tictoc' package for benchmarking and measuring code execution time
library(tictoc)

# Load the 'mapview' package to view spatial objects interactively
library(mapview)

# Load the 'sf' package to create sf (simple features) objects for working with spatial data
library(sf)

# Load the 'zoo' package for date manipulations
library(zoo)

# Load the 'leaflet' package for creating interactive web maps
library(leaflet)

# Load the 'stringi' package for manipulating and working with character strings
library(stringi)

# Load the 'htmltools' package for working with HTML content in R
library(htmltools)

library(viridis) # additional color palettes

library(ggmap) # extends 'ggplot2' for creating maps and working with spatial data.

# Load the 'tigris' package to access geographic data.
library(tigris)

# Set the 'tigris_use_cache' option to TRUE. This will enable caching of
# data retrieved from the TIGER/Line Shapefiles service, which can help
# improve data retrieval performance for future sessions.
tigris_use_cache = TRUE

# Do not show warnings about groups
options(dplyr.summarise.inform = FALSE)
# Turn off the scientific notation
options(scipen = 999)

# Use my function in case we want to change the case in all functions
my_headers_case_function <- tolower

# current user name
get_username <- function(){
    return(as.character(Sys.info()["user"]))
}
```

## set working directories in useful functions 

```{r set working directories in useful functions}
# set working directories in useful functions ----

# Define a function named 'get_current_file_directory',
# to obtain the directory where the script is located.
get_current_file_directory <- function() {

  # Use 'rstudioapi::getSourceEditorContext()' to access information about the currently open script
  # Extract the 'path' from the source editor context and obtain its directory using 'dirname'
  dirname(rstudioapi::getSourceEditorContext()$path)
}

# change main_r_dir, in_dir, out_dir, git_r_dir to your local environment
  # then you can use it in the code like my_paths$input etc.
set_work_dir <- function() {

  # Set the working directory to the user's home directory (~)
  setwd("~/")
  base_dir <- getwd()

  # Initialize 'add_dir' as an empty string (for others)
  add_dir <- ""

  # Check if the username is "anna.shipunova" (Anna's computer)
  if (get_username() == "anna.shipunova") {
    # Set 'add_dir' to a specific directory path for Anna
    add_dir <- "R_files_local/test_dir"
  }

  # Construct the path to the main R directory
  main_r_dir <- file.path(add_dir, "SEFHIER/R code")

  # Define directory names for 'Inputs' and 'Outputs'
  in_dir <- "Inputs"
  out_dir <- "Outputs"

  # Construct full paths to 'Inputs' and 'Outputs' directories using 'file.path'
  # file.path is a function used to create platform-independent file paths by joining its arguments using the appropriate path separator (e.g., "\" on Windows, "/" on Unix-like systems).
  #
  # base_dir is the base directory obtained from the user's home directory.
  #
  # main_r_dir is the path to the main R directory, which may vary depending on whether the user is Anna or not.
  #
  # in_dir is the name of the 'Inputs' directory.
  #
  # So, this line effectively combines these components to create the full path to the 'Inputs' directory, ensuring that the path is correctly formatted for the user's operating system.

  full_path_to_in_dir <- file.path(base_dir, main_r_dir, in_dir)
  full_path_to_out_dir <- file.path(base_dir, main_r_dir, out_dir)

  # Change the working directory to the main R directory
  setwd(file.path(base_dir, main_r_dir))

  # Create a list of directory paths for 'inputs' and 'outputs'
  my_paths <- list("inputs" = full_path_to_in_dir,
                   "outputs" = full_path_to_out_dir)
  return(my_paths)
}

# Define a function named 'set_work_dir_local'
# This function sets the working directory to the user's home directory, defines paths to 'my_inputs,' 'my_outputs,' and 'R_code_github' directories, and returns these directory paths as a list. The use of file.path ensures that the path construction is platform-independent.

set_work_dir_local <- function() {

  # Set the working directory to the user's home directory (~)
  setwd("~/")
  base_dir <- getwd()

  # Define 'main_r_dir' as "R_files_local"
  main_r_dir <- "R_files_local"

  # Define 'in_dir' as "my_inputs"
  in_dir <- "my_inputs"

  # Construct the full path to 'my_inputs' directory
  full_path_to_in_dir <- file.path(base_dir, main_r_dir, in_dir)

  # Define 'out_dir' as "my_outputs"
  out_dir <- "my_outputs"

  # Construct the full path to 'my_outputs' directory
  full_path_to_out_dir <- file.path(base_dir, main_r_dir, out_dir)

  # Define 'git_r_dir' as "R_code_github"
  git_r_dir <- "R_code_github"

  # Construct the full path to 'R_code_github' directory
  full_path_to_r_git_dir <- file.path(base_dir, git_r_dir)

  # Change the working directory to 'R_files_local'
  setwd(file.path(base_dir, main_r_dir))

  # Create a list of directory paths for 'inputs,' 'outputs,' and 'git_r'
  my_paths <- list("inputs" = full_path_to_in_dir,
                   "outputs" = full_path_to_out_dir,
                   "git_r" = full_path_to_r_git_dir)

  # Return the list of directory paths
  return(my_paths)
}

# ===
# Change the behavior of the set_work_dir function based on the username. If the username matches "anna.shipunova," it reassigns set_work_dir to the set_work_dir_local function, effectively using a different directory structure for Anna compared to other users.

# Check if the current username is "anna.shipunova"
if (get_username() == "anna.shipunova") {
  # If the condition is true, assign the 'set_work_dir_local' function to 'set_work_dir'
  set_work_dir <- set_work_dir_local
}

# Define a function named 'load_csv_names' that takes two parameters: 'my_paths' and 'csv_names_list'
load_csv_names <- function(my_paths, csv_names_list) {

  # Extract the 'inputs' directory path from 'my_paths' and store it in 'my_inputs'
  my_inputs <- my_paths$inputs

  # Use 'lapply' to prepend the 'my_inputs' directory path to each file name in 'csv_names_list'
  # This creates a list of full file paths for the CSV files
  myfiles <- lapply(csv_names_list, function(x) file.path(my_inputs, x))

  # Use 'lapply' again to read all CSV files listed in 'myfiles'
  # The 'read_csv' function from the 'readr' package is used, specifying default column types as 'c' ('character')
  contents <- lapply(myfiles, read_csv, col_types = cols(.default = 'c'))

  # Return the contents of the CSV files as a list
  return(contents)
}

# Define a function named 'load_csv_names_in_one_df' that takes two parameters: 'path_to_files' and 'csv_names_list'
load_csv_names_in_one_df <- function(path_to_files, csv_names_list) {

    # Initialize 'myfiles' with 'csv_names_list'
    myfiles <- csv_names_list

    # Check if 'path_to_files' (input directory path) is provided
    if (length(path_to_files) > 0) {

        # If provided, use 'lapply' to prepend 'path_to_files' to each file name in 'csv_names_list'
        myfiles <- lapply(csv_names_list, function(x) file.path(path_to_files, x))
    }

    # Read all CSV files listed in 'myfiles' into a single data frame using 'map_df'
    csv_content <- purrr::map_df(myfiles, function(file_name) {

        # Use 'read_csv' from the 'readr' package to read each CSV file
        readr::read_csv(
            file_name,
            col_types = cols(.default = 'c'),  # Set default column type to 'character'
            trim_ws = TRUE,  # Trim whitespace from values
            na = c("", "NA", "NaN"),  # Treat empty strings, "NA," and "NaN" as NA values
            name_repair = "universal"  # Repair column names
        )
    })

    # Return the concatenated data frame containing all CSV file contents
    return(csv_content)
}

#
# Explanation:
#
# 1. The function `load_csv_names_in_one_df` takes two parameters: `path_to_files`, which is an optional input directory path, and `csv_names_list`, a list of CSV file names.
#
# 2. Initially, the `myfiles` variable is assigned the `csv_names_list`.
#
# 3. If `path_to_files` is provided (its length is greater than 0), the function uses `lapply` to prepend `path_to_files` to each file name in `csv_names_list`. This ensures that the full file paths are correctly constructed.
#
# 4. The `map_df` function is used to read all CSV files listed in `myfiles` and concatenate them into a single data frame (`csv_content`).
#
# 5. Within the `map_df` function, each CSV file is read using `read_csv` from the `readr` package. Various options are set, including the default column type as 'character', trimming whitespace, specifying NA values, and repairing column names.
#
# 6. Finally, the function returns the concatenated data frame containing the contents of all CSV files, making it easier to work with them as a single data structure.

# ===
# The function load_xls_names returns the concatenated data frame containing data from all Excel files. This allows you to work with the combined data more easily.
load_xls_names <- function(my_paths, xls_names_list, sheet_n = 1) {

  # Extract the 'inputs' directory path from 'my_paths' and store it in 'my_inputs'
  my_inputs <- my_paths$inputs

  # Use 'lapply' to prepend 'my_inputs' directory path to each Excel file name in 'xls_names_list'
  myfiles <- lapply(xls_names_list, function(x) file.path(my_inputs, x))

  # Read Excel files listed in 'myfiles' into one data frame using 'map_df'
  contents <- map_df(myfiles, ~read_excel(
    .x,                           # File path
    sheet = sheet_n,              # Sheet number to read (default is 1)
    .name_repair = fix_names,     # Repair column names
    guess_max = 21474836,         # Maximum number of rows to guess data types
    col_types = "text"           # Specify all columns as 'text' type
  ))

  # Return the concatenated data frame containing data from all Excel files
  return(contents)
}

# The clean_headers function is designed to clean and fix the column names of a given dataframe (my_df).
clean_headers <- function(my_df) {
    # Use the 'fix_names' function to clean and fix the column names of the dataframe.
    colnames(my_df) %<>%
        fix_names()

    # Return the dataframe with cleaned and fixed column names.
    return(my_df)
}

# ===
# The fix_names function is used to clean and standardize column names to make them suitable for use in data analysis or further processing.
# to use in a function,
# e.g. read_csv(name_repair = fix_names)
fix_names <- function(x) {
  # Use the pipe operator %>%
  x %>%

    # Remove dots from column names
    str_replace_all("\\.", "") %>%

    # Replace all characters that are not letters or numbers with underscores
    str_replace_all("[^A-z0-9]", "_") %>%

    # Ensure that letters are only in the beginning of the column name
    str_replace_all("^(_*)(.+)", "\\2\\1") %>%

    # Convert column names to lowercase using 'my_headers_case_function'
    my_headers_case_function()
}
```

### functions to clean FHIER compliance and correspondense reports 

```{r functions to clean FHIER compliance and correspondense reports}
## functions to clean FHIER compliance and correspondense reports ----

# split week column ("52: 12/26/2022 - 01/01/2023") into 3 columns with proper classes, week_num (week order number), week_start and week_end
# Define a function named 'clean_weeks' that takes a data frame 'my_df' as input.
# It returns the modified 'my_df' with the cleaned and transformed 'week' columns, including 'week_num', 'week_start', and 'week_end'.

clean_weeks <- function(my_df) {
  my_df %>%

    # Separate the 'week' column using ":" as the delimiter and create new columns 'week_num' and 'week_rest'
    separate_wider_delim(week, ":", names = c("week_num", "week_rest")) %>%

    # Further separate 'week_rest' using " - " as the delimiter and create 'week_start' and 'week_end' columns
    separate_wider_delim(week_rest, " - ", names = c("week_start", "week_end")) ->
    temp_df

  # Convert 'week_num' to integer and update 'my_df' with the result
  my_df$week_num <- as.integer(trimws(temp_df$week_num))

  # Convert 'week_start' to Date format using the specified date format "%m/%d/%Y"
  my_df$week_start <- as.Date(trimws(temp_df$week_start), "%m/%d/%Y")

  # Convert 'week_end' to Date format using the specified date format "%m/%d/%Y"
  my_df$week_end <- as.Date(trimws(temp_df$week_end), "%m/%d/%Y")

  # Return the modified 'my_df' with cleaned and transformed 'week' columns
  return(my_df)
}

# ===
# trim vesselofficialnumber, there are 273 white spaces in Feb 2023
# Define a function named 'trim_all_vessel_ids_simple' with two parameters:
# 'csvs_clean_ws' is a list of data frames to be processed,
# 'col_name_to_trim' is an optional column name to trim, default is NA.
# It returns the list of data frames (csvs_clean) where each data frame has a trimmed 'vessel_official_number' column.
trim_all_vessel_ids_simple <- function(csvs_clean_ws, col_name_to_trim = NA) {

  # Use lapply to iterate through each data frame in 'csvs_clean_ws'
  csvs_clean <- lapply(csvs_clean_ws, function(x) {

    # Check if 'col_name_to_trim' is NA
    if (is.na(col_name_to_trim)) {

      # If it's NA, find the column name matching a pattern and store it in 'col_name_to_trim'
      col_name_to_trim <- grep("vessel.*official.*number",
                               tolower(names(x)),
                               value = TRUE)
    }

    # Convert 'col_name_to_trim' to a symbol using 'sym' from tidyverse
    col_name_to_trim_s <- rlang::sym(col_name_to_trim)

    # Trim leading and trailing white spaces in the dplyr::selected column
    # Hard code vessel_official_number as vessel id
    x %>%
      dplyr::mutate(vessel_official_number = trimws(!!col_name_to_trim_s)) %>%
      # Alternative way of doing the same, not tested
      # dplyr::mutate({{col_name_to_trim_s}} := trimws(!!col_name_to_trim_s)) %>%
      return()
  })

  # Return the list of data frames with trimmed vessel IDs
  return(csvs_clean)
}

# ===
# cleaning, regularly done for csvs downloaded from PHIER
# The clean_all_csvs function is defined to clean a list of CSVs (csvs) and has an optional parameter vessel_id_field_name, which specifies the column to trim.
# It returns the list of cleaned CSVs, where each CSV has had its headers unified and the vessel ID column (if specified) trimmed for consistency.

clean_all_csvs <- function(csvs, vessel_id_field_name = NA) {
  # Clean headers of all CSVs using the 'clean_headers' function
  csvs_clean0 <- lapply(csvs, clean_headers)

  # Trim 'vesselofficialnumber' column (if specified) in all cleaned CSVs
  csvs_clean1 <-
    trim_all_vessel_ids_simple(csvs_clean0, vessel_id_field_name)

  # Return the list of cleaned CSVs
  return(csvs_clean1)
}

# ===
# The join_same_kind_csvs function is defined to concatenate multiple data frames in the 'csvs_list_2_plus' parameter vertically.
join_same_kind_csvs <- function(csvs_list_2_plus) {

  # Concatenate the data frames in 'csvs_list_2_plus' vertically using 'bind_rows'. This function binds the rows of the data frames together, assuming that they have the same column structure.
  result_df <- dplyr::bind_rows(csvs_list_2_plus)

  # Return the combined data frame
  return(result_df)
}

# ===
# Combine correspondence and compliance information into one dataframe by "vesselofficialnumber" only. Not by time!
# The join_all_csvs function is defined to perform a full join operation on two data frames: 'corresp_arr' and 'compl_arr'. It handles cases where these parameters might be lists of data frames or individual data frames.
# It returns the resulting data frame ('result_df') containing the merged data from 'compl' and 'corresp' data frames.

join_all_csvs <- function(corresp_arr, compl_arr) {

  # Initialize 'corresp' with 'corresp_arr' or join data frames in 'corresp_arr' if it's not already a data frame
  corresp <- corresp_arr
  if (!is.data.frame(corresp_arr)) {
    corresp <- join_same_kind_csvs(corresp_arr)
  }

  # Initialize 'compl' with 'compl_arr' or join data frames in 'compl_arr' if it's not already a data frame
  compl <- compl_arr
  if (!is.data.frame(compl_arr)) {
    compl <- join_same_kind_csvs(compl_arr)
  }

  # Perform a full join of 'compl' and 'corresp' data frames on the 'vesselofficialnumber' column, retaining all rows
  result_df <- compl %>%
    full_join(corresp,
              by = c("vesselofficialnumber"),
              multiple = "all")

  # Return the resulting data frame
  return(result_df)
}

# ===
# Change a column class to POSIXct in the "my_df" for the field "field_name" using the "date_format"
# The change_to_dates function is defined to convert a specific column ('field_name') in the input data frame ('my_df') to POSIXct date format using the specified 'date_format'.
#
# Inside the function, it uses the dplyr::mutate function from the dplyr package to modify 'my_df'. The {{field_name}} syntax is used to refer to the column specified by 'field_name'.
# It returns the 'result_df', which is the input data frame with the specified column converted to dates according to the specified 'date_format'.

# ===
change_to_dates <- function(my_df, field_name, date_format) {
  # Convert the specified column ('field_name') in 'my_df' to POSIXct date format using 'as.POSIXct'
  # Within the dplyr::mutate function, it uses pull to extract the column specified by 'field_name' and then applies as.POSIXct to convert the values in that column to POSIXct date format using the provided 'date_format'.
  result_df <- my_df %>%
    dplyr::mutate({
      {
        field_name
      }
    } := as.POSIXct(dplyr::pull(my_df[field_name]), format = date_format))

  # Return the data frame with the specified column converted to dates
  return(result_df)
}

# ===
# The aux_fun_for_dates function is defined as a utility function to convert a given vector 'x' to POSIXct date format using the specified 'date_format'.
aux_fun_for_dates <- function(x, date_format) {

  # Convert 'x' to POSIXct date format using 'as.POSIXct'
  out <- as.POSIXct(x, format = date_format)

  # Return the result as a POSIXct date object
  return(out)
}

# ===
  # # Previously
  # across(a:b, mean, na.rm = TRUE)
  #
  # # Now
  # across(a:b, \(x) mean(x, na.rm = TRUE))
# change_fields_arr_to_dates <- function(my_df, field_names_arr, date_format) {
#   my_df %>%
#     dplyr::mutate(across(all_of(field_names_arr), aux_fun_for_dates, date_format)) %>%
#
#     # dplyr::mutate({{field_name}} := as.POSIXct(pull(my_df[field_name]),
#                                         # format = date_format)) %>%
#     return()
# }

# The change_fields_arr_to_dates function is defined to convert multiple columns specified in 'field_names_arr' in the input data frame ('my_df') to POSIXct date format using the provided 'date_format'.
# Inside the function, it uses the dplyr::mutate function along with across from the dplyr package to target and modify the specified columns in 'field_names_arr'. The all_of(field_names_arr) ensures that all the columns listed in 'field_names_arr' are dplyr::selected.

# Within the across function, it applies the as.POSIXct function to each column ('x') in 'field_names_arr' using the provided 'date_format'. This step converts the values in these columns to POSIXct date format.
# It returns the 'result_df', which is the input data frame with the specified columns converted to dates according to the specified 'date_format'.

change_fields_arr_to_dates <-
  function(my_df, field_names_arr, date_format) {
    # Use 'mutate' and 'across' to convert all specified columns in 'field_names_arr' to POSIXct date format
    result_df <- my_df %>%
      dplyr::mutate(dplyr::across(
        all_of(field_names_arr),
        ~ as.POSIXct(.x, format = date_format)  # Apply 'as.POSIXct' to each column with the provided 'date_format'
      ))

    # Return the data frame with the specified columns converted to dates
    return(result_df)
  }

# ===
# The add_count_contacts function is defined to add two new columns ('was_contacted' and 'contact_freq') to the input data frame ('all_data_df_clean') based on the presence of contact dates.

# It returns the 'result_df', which is the input data frame with the added columns indicating whether a vessel was contacted ('was_contacted') and the frequency of contacts ('contact_freq').

# Use for contacts in the setup function before combining with compliant dataframes
add_count_contacts <- function(all_data_df_clean) {
  # Find the column name for 'contactdate' and 'vesselnumber' in 'all_data_df_clean'
  contactdate_field_name <-
    find_col_name(all_data_df_clean, "contact", "date")[1]
  vessel_id_field_name <-
    find_col_name(all_data_df_clean, "vessel", "number")[1]

  # Apply a series of transformations to 'all_data_df_clean'
  result_df <- all_data_df_clean %>%

    # Add a new column 'was_contacted' with "yes" if 'contactdate' is not NA, or "no" if it is NA
    # TODO: as.factor
    dplyr::mutate(was_contacted =
                    dplyr::if_else(is.na(contactdate_field_name), "no", "yes")) %>%

    # Group the data by 'vesselofficialnumber' and 'was_contacted', and count the occurrences, saving it in 'contact_freq' column
    dplyr::add_count(!!dplyr::sym(vessel_id_field_name), was_contacted, name = "contact_freq")

  # Return the modified data frame with the added 'was_contacted' and 'contact_freq' columns
  return(result_df)
}

# ===
# Get frequencies for each column in the list
# usage:
# group_by_arr <- c("vesselofficialnumber", "contacttype")
# count_by_column_arr(my_df, group_by_arr)
# Define a function 'count_by_column_arr' to count the frequency of combinations of columns.
# This function takes two arguments: my_df, which is the input data frame, and group_by_arr, which is a character vector containing the names of columns to group by.

count_by_column_arr <- function(my_df, group_by_arr) {
  my_df %>%
    arrange(group_by_arr[1]) %>%          # Arrange the data by the first column in 'group_by_arr'.
    group_by_at(group_by_arr) %>%         # Group the data by the columns specified in 'group_by_arr'.
    summarise(my_freq = n()) %>%           # Calculate the frequency of each combination.
    return()                              # It returns the summary result.
}

# ===
# Define a function 'count_uniq_by_column' to count the number of unique values in each column of a data frame.

# Within the function, the sapply function is used to apply another function to each column of the input data frame. Specifically, it counts the number of unique values in each column using the length(unique(x)) expression, where x represents each column of the data frame.
# The result of sapply is a vector containing the counts of unique values for each column.

# It returns the resulting data frame, which provides a summary of the counts of unique values for each column in the input data frame. This information can be valuable for assessing the diversity of values within each column.

count_uniq_by_column <- function(my_df) {
  sapply(my_df, function(x) length(unique(x))) %>%  # Apply a function to each column to count unique values.
    as.data.frame()  # Convert the result to a data frame.
}

# ===
# The data_overview function is designed to provide an overview of a given data frame, including summary statistics and counts of unique values in each column.

data_overview <- function(my_df) {
  # Use 'summary' function to generate summary statistics and print the results.
  summary(my_df) %>% print()

  # Print a header indicating the next section of the output.
  cat("\nCount unique values in each column:")

  # Call the 'count_uniq_by_column' function to count unique values in each column of the data frame.
  count_uniq_by_column(my_df)
}

# ===

# from https://stackoverflow.com/questions/53781563/combine-rows-based-on-multiple-columns-and-keep-all-unique-values
# concat_unique <- function(x){paste(unique(x),  collapse=', ')}

# Define a function 'concat_unique' to concatenate unique non-NA values from a vector x into a single character string.
concat_unique <- function(x) {
  # Use 'unique' to extract unique values, '!is.na(x)' to remove NA values, and 'collapse = ", "' to concatenate with a comma and space.
  # Finally, paste0 is used to concatenate the unique non-NA values with a comma and space separator (", ").
  paste0(unique(x[!is.na(x)]), collapse = ", ")
}

# ===
# Define a function 'print_df_names' to print the names of columns in a data frame.
# This function retrieves column names, limits the number to 'names_num' (default = 100),
# and returns them as a comma-separated string.
print_df_names <- function(my_df, names_num = 100) {
  # Use 'names' to get column names,
  # 'head' to limit the number of names to 'names_num',
  # 'paste0' to concatenate them with a comma separator, and return the result.
  names(my_df) %>%
    head(names_num) %>%
    paste0(collapse = ", ") %>%
    return()
}

# ===
# Define a function to combine rows based on multiple columns while keeping all unique values.
# This function groups the data frame by specified columns,
# applies 'concat_unique' to combine values in each column,
# and returns the result.
combine_rows_based_on_multiple_columns_and_keep_all_unique_values <-
  function(my_df, group_by_arr) {
    # Group the data frame by specified columns.
    my_df %>%
      dplyr::group_by_at(group_by_arr) %>%
      # Summarize all columns by applying 'concat_unique' to combine unique values.
      dplyr::summarise_all(concat_unique) %>%
      return()
  }

# ===
# Define a function to concatenate unique values in a sorted manner.
# This function takes a vector 'x', removes NA values, sorts the unique values,
# and then concatenates them with a comma separator.
concat_unique_sorted <- function(x) {
  # Remove NA values from the input vector 'x' and store the result.
  non_na_values <- x[!is.na(x)]

  # Sort the unique values obtained from the previous step.
  sorted_unique <- unique(sort(non_na_values))

  # Concatenate the sorted unique values with a comma separator.
  result <- paste0(sorted_unique, collapse = ", ")

  # Return the concatenated result.
  return(result)
}

# ===
# Define a function to combine rows based on multiple columns and keep all
# unique values sorted within each group.
# This function takes a data frame 'my_df' and a vector of column names
# 'group_by_arr' as input.
combine_rows_based_on_multiple_columns_and_keep_all_unique_sorted_values <- function(my_df, group_by_arr) {
  # Group the data frame 'my_df' by the columns specified in 'group_by_arr'.
  # This step ensures that we create groups based on unique combinations of
  # values in the specified columns.
  grouped_df <- my_df %>%
    dplyr::group_by_at(group_by_arr)

  # Apply the 'concat_unique_sorted' function to all columns in each group.
  # This function concatenates all unique values within each group and sorts
  # them in ascending order.
  summarized_df <- grouped_df %>%
    dplyr::summarise_all(concat_unique_sorted)

  # Return the resulting data frame 'summarized_df'.
  return(summarized_df)
}

## usage:
# my_paths <- set_work_dir()
#
## get csv data into variables
# temp_var <- get_compl_and_corresp_data(my_paths)
# compl_clean <- temp_var[[1]]
# corresp_clean <- temp_var[[2]]

csv_names_list_22_23 = c("Correspondence.csv",
                         "FHIER_Compliance_22.csv",
                         "FHIER_Compliance_23.csv")

# To add my additional folder names to each filename.

# Define a function to prepare file names by categorizing them into two
# subdirectories based on their prefixes.
# This function takes a vector of 'filenames' as input.
prepare_csv_names <- function(filenames) {
  # Define subdirectory names for correspondence and compliance files.
  add_path_corresp <- "Correspondence"
  add_path_compl <- "FHIER Compliance"

  # Use 'sapply' to process each filename in the 'filenames' vector.
  my_list <- sapply(filenames, function(x) {
    # Use 'case_when' to categorize filenames based on their prefixes.
    # If a filename starts with "correspond," it is placed in the
    # 'Correspondence' subdirectory. If it starts with "fhier_compliance,"
    # it is placed in the 'FHIER Compliance' subdirectory. Otherwise, it is
    # placed in the 'FHIER Compliance' subdirectory as a default.
    case_when(
      startsWith(my_headers_case_function(x), "correspond") ~
        file.path(add_path_corresp,  x),
      startsWith(my_headers_case_function(x), "fhier_compliance") ~
        file.path(add_path_compl,  x),
      .default = file.path(add_path_compl,  x)
    )
  })

  # Convert the resulting list into a character vector and return it.
  return(paste(my_list) %>% as.list())
}

# ===
# Define a function to read CSV files with EOFs (End of File) from a specified directory.
# This function takes 'my_paths' (directory paths) and 'csv_names_list' (list of CSV file names) as input.
read_csv_w_eofs <- function(my_paths, csv_names_list) {
  # Get the input directory path from 'my_paths'.
  my_inputs <- my_paths$inputs

  # Create a vector 'myfiles' that contains the full paths to each CSV file.
  # Add the input directory path in front of each file name.
  myfiles <- sapply(csv_names_list, function(x) file.path(my_inputs, add_csv_path, x))

  # Read CSV files using 'fread' from the 'data.table' package with 'header = TRUE' (considering the first row as column names).
  contents <- sapply(myfiles, fread, header = TRUE)

  # Convert the first CSV file into a data frame.
  # TODO: Consider changing this function to handle multiple files.
  # For now, it returns the first CSV file as a data frame.
  contents[, 1] %>%
    as.data.frame() %>%
    return()
}

# ===
# To use as a filter in FHIER
# Define a function to concatenate and output character data to a text file.
# This function takes 'my_characters' (a character vector) as input.
cat_filter_for_fhier <- function(my_characters) {
  # Concatenate the elements of 'my_characters' using a comma and space as the separator.
  # Output the concatenated string to a text file named "cat_out.txt" in the outputs directory.
  cat(my_characters,
      sep = ', ',
      file = file.path(my_paths$outputs, "cat_out.txt"))
}

# ===
#
# benchmarking to insert inside a function
# browser()
# time_for_appl <<- benchmark(replications=rep(10, 3),
                            # lapply(myfiles, read.csv, skipNul = TRUE, header = TRUE),
                            # sapply(myfiles, read.csv, skipNul = TRUE, header = TRUE, simplify = TRUE)
                            # ,
                            # columns = c('test', 'elapsed', 'relative')
# )

# write.csv(time_for_appl, "time_for_appl.csv")

# or
# browser()
# sappl_exp <- function(){
#   sapply(my_df, function(x) length(unique(x))) %>% as.data.frame()
# }
#
# map_exp <- function(){
#   my_fun <- function(x) length(unique(x))
#   map_df(my_df, my_fun)
# }
#
# time_for_appl <<- benchmark(replications=rep(10^7, 3),
#                             exp1,
#                             exp2,
#                             columns = c('test', 'elapsed', 'relative')
# )
#
# map_df(my_df, function(x) length(unique(x)))
# to compare:
# time_for_appl %>% group_by(test) %>% summarise(sum(elapsed))

```
```{r}
# Define a function named 'connect_to_secpr'.
# It returns the established database connection (con), which can be used to interact with the "SECPR" database in R.
# usage:
# con <- connect_to_secpr()
connect_to_secpr <- function() {
    # Retrieve the username associated with the "SECPR" database from the keyring.
    my_username <- keyring::key_list("SECPR")[1, 2]

    # Use 'dbConnect' to establish a database connection with the specified credentials.
    con <- dbConnect(
        dbDriver("Oracle"),  # Use the Oracle database driver.
        username = my_username,  # Use the retrieved username.
        password = keyring::key_get("SECPR", my_username),  # Retrieve the password from the keyring.
        dbname = "SECPR"  # Specify the name of the database as "SECPR."
    )

    # Return the established database connection.
    return(con)
}

# ===
# usage: complianceerrors_field_name <- find_col_name(compl_clean_sa, ".*xcompliance", "errors.*")[1]
# TODO what if two names?
# Define a function to find column names in a dataframe based on partial matches.
# This function takes 'mydf' (a dataframe), 'start_part' (the start of the column name),
# and 'end_part' (the end of the column name) as inputs.
find_col_name <- function(mydf, start_part, end_part) {
  # Create a regular expression pattern to search for column names that start with 'start_part'
  # and end with 'end_part'.
  to_search <- paste0(start_part, ".*", end_part)

  # Use 'grep' to search for column names in lowercase that match the pattern.
  # 'value = TRUE' returns the matching column names as a character vector.
  matching_names <- grep(to_search, tolower(names(mydf)), value = TRUE)

  # Return the matching column name(s) as a character vector.
  return(matching_names)
}

# https://stackoverflow.com/questions/23986140/how-to-call-exists-without-quotation-marks
# usage: vexists(con_psql, bogus_variable_name)
# Define a function to check the existence of one or more variables in the current environment.
# This function takes a variable number of arguments using '...' notation.
vexists <- function(...) {
  # Use 'substitute' to capture the variable names from the arguments and convert them to character vectors.
  vars <- as.character(substitute(...()))

  # Use 'sapply' to iterate over the variable names and check if each variable exists in the current environment.
  exists_check <- sapply(vars, exists)

  # Return a logical vector indicating the existence of each variable.
  return(exists_check)
}

# ===
# make a separate legend for grid.arrange
legend_for_grid_arrange <- function(legend_plot) {
  # legend_plot <-
  #   ggplot(data = legend_data, aes(x1, y1, colour = ll)) +
  #   geom_text(dat = legend_data,
  #             aes(label = ll),
  #             hjust = 0) +
  #   scale_color_manual(
  #     name = 'Lines',
  #     breaks = c('Mean', 'Num of weeks'),
  #     values = my_colors
  #   )
  #
  # legend_plot

  # Obtain the legend from a 'legend_plot' using the 'get_legend' function from the 'cowplot' package.
  my_legend <-
    cowplot::get_legend(legend_plot)

  return(my_legend)
}

# ===
# Define a function to append the contents of a single file to an existing flat file.
write_to_1_flat_file <- function(flat_file_name, file_name_to_write) {
  # Redirect the output to the specified 'flat_file_name' and append content.
  sink(flat_file_name, append = TRUE)

  # Read the contents of the current file.
  current_file_text <- readLines(file_name_to_write)

  # Print a header indicating the current file being processed.
  cat("\n\n##### Current file:", basename(file_name_to_write), "----\n\n")

  # Print the contents of the current file, separating lines with newline characters.
  cat(current_file_text, sep = "\n")

  # # Restore the default output behavior.
  sink()
}

# Function to separate permit groups into three categories based on a specified field
separate_permits_into_3_groups <-
  function(my_df, permit_group_field_name = "permitgroup") {
    my_df %>%
      # Use 'mutate' to create a new column 'permit_sa_gom' with categories based on permit group
      dplyr::mutate(permit_sa_gom =
               case_when(
                 # Check if 'permit_group_field_name' doesn't contain 'RCG', 'HRCG', 'CHG', or 'HCHG'; assign "sa_only" if true
                 !grepl("RCG|HRCG|CHG|HCHG", !!sym(permit_group_field_name)) ~ "sa_only",
                 # Check if 'permit_group_field_name' doesn't contain 'CDW', 'CHS', or 'SC'; assign "gom_only" if true
                 !grepl("CDW|CHS|SC", !!sym(permit_group_field_name)) ~ "gom_only",
                 # For all other cases, assign "dual"
                 .default = "dual"
               )) %>%
      # Return the modified data frame
      return()
  }


# ===

# read_rds_or_run <-
#   function(my_file_path,
#            my_data_list_of_dfs,
#            my_function) {
#     # browser()
#
#     if (file.exists(my_file_path)) {
#       # read a binary file saved previously
#       my_df <-
#         readr::read_rds(my_file_path)
#     } else {
#       tic("run the function")
#       my_df <-
#         my_function(my_data_list_of_dfs)
#       toc()
#
#       # write all as binary
#       readr::write_rds(my_df,
#                        my_file_path)
#     }
#
#     return(my_df)
#   }

# ===
# The read_rds_or_run function is designed to read data from an RDS file if it exists or run a specified function to generate the data if the file doesn't exist.
      # read a binary file saved previously
      # write all as binary
read_rds_or_run <- function(my_file_path,
                            my_data = as.data.frame(""),
                            my_function,
                            force_from_db = NULL) {

    # Check if the file specified by 'my_file_path' exists and 'force_from_db' is not set.
    if (file.exists(my_file_path) &
        is.null(force_from_db)) {
        # If the file exists and 'force_from_db' is not set, read the data from the RDS file.
        my_result <- readr::read_rds(my_file_path)
    } else {
        # If the file doesn't exist or 'force_from_db' is set, perform the following steps:
        # 1. Generate a message indicating the date and the purpose of the run.
        msg_text <- paste(today(), "run for", basename(my_file_path))
        tic(msg_text)  # Start timing the operation.

        # 2. Run the specified function 'my_function' on the provided 'my_data' to generate the result.
        my_result <- my_function(my_data)

        toc()  # Stop timing the operation.

        # 3. Save the result as an RDS binary file to 'my_file_path' for future use.
        readr::write_rds(my_result,
                         my_file_path)
    }

    # Return the generated or read data.
    return(my_result)
}


# Usage:
# dplyr::select(-all_of(names(empty_cols)))
# empty_cols <-
#   function(my_df) {
#     my_df |>
#       map_df(function(x) {
#         browser()
#         if (length(unique(x)) == 1) {
#           return(unique(x))
#         }
#       }) %>%
#     return()
#   }

# ===
# Function to remove empty columns from a data frame
remove_empty_cols <- function(my_df) {
  my_df |>
    # dplyr::select columns that do not meet the condition of being entirely NA or entirely NULL using 'select_if' function
    dplyr::select_if(function(x)
      # Check if all values in 'x' are not all NA or not all NULL
      !(all(is.na(x)) | all(is.null(x)))) %>%
    # Return the modified data frame
    return()
}

# ===
# Function to create a directory if it doesn't exist
create_dir_if_not <- function(curr_dir_name) {
  # Check if the directory does not exist
  if (!dir.exists(curr_dir_name)) {
    dir.create(curr_dir_name)  # Create the directory if it doesn't exist
  }
}

# ===
```

## sf functions 

```{r sf functions}
# sf functions ----
# ===
# convert to sf shortcut
# Function to convert a data frame to an sf object with specified coordinates and CRS
my_to_sf <- function(my_df, my_crs = sf::st_crs(sa_shp)) {
  my_df %>%
    sf::st_as_sf(
      # Specify the field names to use as coordinates
      coords = c("longitude", "latitude"),
      # Use the provided CRS (Coordinate Reference System), default to sa_shp's CRS
      crs = my_crs,
      # Keep the LATITUDE and LONGITUDE columns in the resulting sf object
      remove = FALSE
    ) %>%
    return()
}

# ===

# to avoid this error:
#   Loop 0 is not valid: Edge 57478 has duplicate vertex with edge 57482
# Disable the use of the S2 library for spherical geometry operations in the sf package
sf::sf_use_s2(FALSE)

# to use on download from db
# Define a function named 'vessels_permits_id_clean' to clean a dataframe.
vessels_permits_id_clean <- function(my_df) {
    # Create a new dataframe 'vessels_permits' by renaming two specific columns.
    vessels_permits <- my_df |>
        rename("PERMIT_VESSEL_ID" = "QCSJ_C000000000300000") |>
        rename("VESSEL_VESSEL_ID" = "QCSJ_C000000000300001")

    # Return the cleaned dataframe.
    return(vessels_permits)
}

# ===
# to use with toc(log = TRUE, quiet = TRUE)
# Define a function named print_toc_log that takes a 'variables' parameter
 # It's useful for monitoring and debugging code execution time when using the tic and toc functions to measure time intervals.
print_toc_log <- function(variables) {
  # Retrieve the log from the tic package with formatting enabled and store it in 'log.txt'
  log.txt <- tic.log(format = TRUE)

  # Write the lines of 'log.txt' to the console
  writeLines(unlist(log.txt))

  # Clear the log, removing its contents
  tic.clearlog()
}
```

## set working path 

```{r set working path}
# set working path ----

my_paths <- set_work_dir()
# Change to your input and output
# my_paths$inputs <- r"(Downloads\Oct 2023\input)"
# my_paths$outputs <- r"(\Downloads\Oct 2023\output)"
```

##### Current file: get_srhs_vessels.R 

```{r Current file: get_srhs_vessels.R}
#### Current file: get_srhs_vessels.R ----
```

## get SRHS vessels to exclude 

```{r get SRHS vessels to exclude}
# get SRHS vessels to exclude ----
# The file is provided by Kenneth Brennan

srhs_vessels_2022 <-
  r"(~\Official documents\srhs_boats\2022_SRHS_Vessels_08_18_2023.xlsx)"

srhs_vessels_2022_info <-
  read_excel(
  srhs_vessels_2022,
  # add the sheet name if needed and uncomment the next line
  # sheet = sheet_n,
  # use my fix_names function for col names
  .name_repair = fix_names,
  # if omitted, the algorithm uses only the few first lines and sometimes guesses it wrong
  guess_max = 21474836,
  # read all columns as text
  col_types = "text"
)
```

##### Current file: get_metrics_tracking.R 

```{r Current file: get_metrics_tracking.R}
#### Current file: get_metrics_tracking.R ----
```

### fhier_reports_metrics_tracking 

```{r fhier_reports_metrics_tracking}
## fhier_reports_metrics_tracking ----

# Download from FHIER / Reports / Metrics Tracking
# Put dates in, e.g. 01/01/2022 - 12/31/2022
# Click search
# Under "Detail Report - via Valid and Renewable Permits Filter (SERO_NEW Source)	" section below click "Actions", then "Download"

fhier_reports_metrics_tracking_file_names <-
  c("Detail_Report_12312021_12312022__08_23_2023.csv",
    "Detail_Report_12312022_12312023__08_23_2023.csv")

common_dir <-
  r"(~\R_files_local\my_inputs\from_Fhier\Detail Report - via Valid and Renewable Permits Filter (SERO_NEW Source))"

# save all file names to a list
# Create a vector named 'fhier_reports_metrics_tracking_file_path' using the purrr::map function.
# This vector will store file paths based on the 'fhier_reports_metrics_tracking_file_names' vector.
fhier_reports_metrics_tracking_file_path <-
  purrr::map(
    # Iterate over each element in the 'fhier_reports_metrics_tracking_file_names' vector.
    fhier_reports_metrics_tracking_file_names,
    # For each file name ('x'), create a file path by combining it with 'common_dir'.
    ~ file.path(common_dir, .x)
  )

# test
# Use the purrr::map function to check if files exist at the specified paths.
# The result will be a logical vector indicating file existence for each path.
purrr::map(fhier_reports_metrics_tracking_file_path, file.exists)
# T

# read each csv in a list of dfs
# Use the purrr::map function to read multiple CSV files into a list of data frames.
fhier_reports_metrics_tracking_list <- purrr::map(
  fhier_reports_metrics_tracking_file_path,
  # A vector of file paths to CSV files.
  ~ readr::read_csv(
    # The current file path being processed in the iteration.
    .x,
    # Specify column types; here, all columns are read as characters.
    col_types = cols(.default = 'c'),
    name_repair = fix_names  # Automatically repair column names to be syntactically valid.
  )
)
```

## check how many in diff years 

```{r check how many in diff years}
# check how many in diff years ----
# Use the 'dplyr::setdiff' function to find the set difference between two vectors.
# (1 minus 2)
dplyr::setdiff(
  fhier_reports_metrics_tracking_list[[1]]$vessel_official_number,
  fhier_reports_metrics_tracking_list[[2]]$vessel_official_number
) |>
  length()  # Calculate the length of the resulting set difference.
# [1] 669

# (2 minus 1)
dplyr::setdiff(
  fhier_reports_metrics_tracking_list[[2]]$vessel_official_number,
  fhier_reports_metrics_tracking_list[[1]]$vessel_official_number
) |>
  length()
# [1] 493

# in both years
# Use the 'dplyr::intersect' function to find the intersection of two vectors.
# In this case, we're finding the common unique values between the two vectors.
dplyr::intersect(
  fhier_reports_metrics_tracking_list[[1]]$vessel_official_number,
  fhier_reports_metrics_tracking_list[[2]]$vessel_official_number
) |>
  length()  # Calculate the length of the resulting intersection.
# 2965
```

##### Current file: metric_tracking_no_srhs.R 

```{r Current file: metric_tracking_no_srhs.R}
#### Current file: metric_tracking_no_srhs.R ----

get_data_from_fhier_dir <- "get_data/get_data_from_fhier"

get_metrics_tracking_path <-
  file.path(my_paths$git_r,
            get_data_from_fhier_dir,
            "get_metrics_tracking.R")
# source(get_metrics_tracking_path)

get_srhs_vessels_path <-
  file.path(my_paths$git_r,
            get_data_from_fhier_dir,
            "get_srhs_vessels.R")
# source(get_srhs_vessels_path)
```

### exclude srhs vessels from metric traking 

```{r exclude srhs vessels from metric traking}
## exclude srhs vessels from metric traking ----
fhier_reports_metrics_tracking_not_srhs_ids <-
  # create a data frame
  purrr::map_df(
    fhier_reports_metrics_tracking_list,
    # for each df from the list
    ~ .x |>
      # exclude SRHS vessels
      dplyr::filter(!vessel_official_number %in% srhs_vessels_2022_info$uscg__)
  ) |>
  # keep only the vessel_official_numbers, remove all other columns
  dplyr::select(vessel_official_number) |>
  # remove duplicates
  dplyr::distinct()

# dim(fhier_reports_metrics_tracking_not_srhs_ids)
# [1] 2981    1

# the same, but result kept in a list
# Create a list named 'fhier_reports_metrics_tracking_not_srhs_ids_list'
fhier_reports_metrics_tracking_not_srhs_ids_list <-
  purrr::map(
    fhier_reports_metrics_tracking_list,
    # Iterate over each data frame in this list
    ~ .x |>
      # Exclude SRHS vessels:
      # Filter rows where 'vessel_official_number' is not in 'uscg__' column of 'srhs_vessels_2022_info'
      filter(!vessel_official_number %in% srhs_vessels_2022_info$uscg__) |>
      # dplyr::select only the 'vessel_official_number' column
      dplyr::select(vessel_official_number) |>
      # Remove duplicate values from the dplyr::selected column
      dplyr::distinct()
  )


# check
# Use 'map' to apply the 'dim' function to each data frame in 'fhier_reports_metrics_tracking_list'
purrr::map(fhier_reports_metrics_tracking_list, dim)
# [[1]]
# [1] 3634   13
#
# [[2]]
# [1] 3460   13

purrr::map(fhier_reports_metrics_tracking_not_srhs_ids_list, dim)
# [[1]]
# [1] 3571    1
#
# [[2]]
# [1] 3399    1
```

##### Current file: get_db_data.R 

```{r Current file: get_db_data.R}
#### Current file: get_db_data.R ----
```

## setup (in get data) 

```{r setup (in get data)}
# setup (in get data) ----
get_data_from_fhier_dir <- "get_data/get_data_from_fhier"

# The source function is a built-in R function that loads and executes R code from an external file. It does not return any value; it simply executes the code in the specified file.

# source("~/R_code_github/useful_functions_module.r")

# Define the current project name as "get_db_data."
current_project_name <- "get_db_data"

# Construct the input path by combining the 'inputs' directory from 'my_paths' with the current project name.
# The file.path function in R is used to construct file paths in a platform-independent way. It automatically takes care of the appropriate path separator (e.g., "/" on Unix-like systems or "\" on Windows).
input_path <- file.path(my_paths$inputs, current_project_name)

# err msg if no connection, but keep running
try(con <- connect_to_secpr())
```

## get data from db 

```{r get data from db}
# get data from db ----
# RDS (R Data Serialization) files are a common format for saving R objects in RStudio, and they allow you to preserve the state of an object between R sessions.
```

### logbooks as in FHIER 

```{r logbooks as in FHIER}
## logbooks as in FHIER ----
# mv_safis_trip_download

# Create a file path by combining 'input_path' with the filename "mv_safis_trip_download.rds."
file_name_mv_safis_trip_download <-
  file.path(input_path, "mv_safis_trip_download.rds")

mv_safis_trip_download_query <-
  "select * from
srh.mv_safis_trip_download@secapxdv_dblk.sfsc.noaa.gov
"

# Define a function 'mv_safis_trip_download_fun' to retrieve data from the database using a specified query.
mv_safis_trip_download_fun <-
  function(mv_safis_trip_download_query) {
  # Use 'dbGetQuery' to execute the query on the database connection 'con' and return the result.
  result <- dbGetQuery(con, mv_safis_trip_download_query)

  # Return the result of the database query.
  return(result)
}

get_mv_safis_trip_download <-
  function() {
    # Use 'read_rds_or_run' to either read permit information from an RDS file or execute a query to obtain it.
    read_rds_or_run(file_name_mv_safis_trip_download,
                    mv_safis_trip_download_query,
                    mv_safis_trip_download_fun,
                    force_from_db)
  }

# to use alone:
# mv_safis_trip_download_data <- get_mv_safis_trip_download()
# 2023-10-19 run for mv_safis_trip_download.rds: 746.39 sec elapsed
```

### SEFHIER declarations as in FHIER 

```{r SEFHIER declarations as in FHIER}
## SEFHIER declarations as in FHIER ----
# MV_TMS_TRIP_NOTIFICATIONS

# Create a file path by combining 'input_path' with the filename "mv_tms_trip_notifications.rds."
file_name_mv_tms_trip_notifications <- file.path(input_path, "mv_tms_trip_notifications.rds")

mv_tms_trip_notifications_query <-
  "select * from
srh.mv_tms_trip_notifications@secapxdv_dblk.sfsc.noaa.gov
"

# Define a function 'mv_tms_trip_notifications_fun' to retrieve data from the database using a specified query.
mv_tms_trip_notifications_fun <- function(mv_tms_trip_notifications_query) {
  # Use 'dbGetQuery' to execute the query on the database connection 'con' and return the result.
  result <- dbGetQuery(con, mv_tms_trip_notifications_query)

  # Return the result of the database query.
  return(result)
}

get_mv_tms_trip_notifications <-
  function() {
    # Use 'read_rds_or_run' to either read permit information from an RDS file or execute a query to obtain it.
    read_rds_or_run(file_name_mv_tms_trip_notifications,
                    mv_tms_trip_notifications_query,
                    mv_tms_trip_notifications_fun,
                    force_from_db)
  }

# to use alone:
# mv_tms_trip_notifications_data <-
#   get_mv_tms_trip_notifications()
# 2023-10-19 run for mv_tms_trip_notifications.rds: 11.04 sec elapsed
```

### permit 

```{r permit}
## permit ----
# Create a file path by combining 'input_path' with the filename "permit_info.rds."
file_name_permits <- file.path(input_path, "permit_info.rds")

mv_sero_fh_permits_his_query <-
  "select * from
srh.mv_sero_fh_permits_his@secapxdv_dblk.sfsc.noaa.gov
"

# Define a function 'permit_info_fun' to retrieve data from the database using a specified query.
permit_info_fun <- function(mv_sero_fh_permits_his_query) {
  # Use 'dbGetQuery' to execute the query on the database connection 'con' and return the result.
  result <- dbGetQuery(con, mv_sero_fh_permits_his_query)

  # Return the result of the database query.
  return(result)
}

get_permit_info <-
  function() {
    # Use 'read_rds_or_run' to either read permit information from an RDS file or execute a query to obtain it.
    read_rds_or_run(file_name_permits,
                    mv_sero_fh_permits_his_query,
                    permit_info_fun,
                    force_from_db)
  }
# 2023-09-20 run the function: 40.74 sec elapsed
```

#### permit + vessel from db 

```{r permit + vessel from db}
### permit + vessel from db ----
# Doesn't work
# permit_vessel_query_exp21_query <-
# "select * from srh.mv_sero_fh_permits_his@secapxdv_dblk.sfsc.noaa.gov p
# join safis.vessels@secapxdv_dblk.sfsc.noaa.gov v
# ON (v.sero_official_number = p.vessel_id)
# where expiration_date > TO_DATE('01-JAN-21')
# "
# and top in ('CDW', 'CHS', 'SC')

# permit_vessel_query_exp21 <- dbGetQuery(con,
                          # permit_vessel_query_exp21_query)

# View(permit_vessel_query_exp21)

# add sa gom field

# names(permit_vessel_query_exp21) <-
  # make.unique(names(permit_vessel_query_exp21), sep = "_")

# print_df_names(permit_vessel_query_exp21)

# permit_vessel_query_exp21 %>%
  # filter(!(VESSEL_ID == SERO_OFFICIAL_NUMBER)) %>%
  # dim()
# 0

# Logbooks
```

## get trips info 

```{r get trips info}
# get trips info ----
trips_file_name <-
    file.path(input_path, "trips.rds")

trips_query <-
  "SELECT
  *
FROM
  safis.trips@secapxdv_dblk.sfsc.noaa.gov
WHERE
  ( trip_start_date BETWEEN TO_DATE('01-JAN-22', 'dd-mon-yy') AND CURRENT_DATE
  )
ORDER BY
  trip_end_date DESC
"

trips_fun <- function(trips_query) {
  return(dbGetQuery(con,
             trips_query))
}

get_trips_info <-
  function() {
      read_rds_or_run(trips_file_name,
                      trips_query,
                      trips_fun,
                      force_from_db)
  }
# 2023-09-20 run the function: 33.02 sec elapsed

# grep("long", names(trips_info), ignore.case = T, value = T)
# 0
```

## latitude/longitude 

```{r latitude_longitude}
# latitude/longitude ----
# dplyr::select * from safis.EFFORTS@secapxdv_dblk.sfsc.noaa.gov;

trip_coord_query <- "
  dplyr::select
  trip_id,
  area_code,
  sub_area_code,
  distance_code,
  fishing_hours,
  latitude,
  longitude,
  local_area_code,
  in_state,
  avg_depth_in_fathoms,
  e.de e_de,
  e.ue e_ue,
  e.dc e_dc,
  e.uc e_uc,
  anything_caught_flag,
  depth,
  minimum_bottom_depth,
  maximum_bottom_depth,
  fishing_gear_depth,
  ten_minute_square_list,
  trip_type,
  supplier_trip_id,
  days_at_sea,
  t.de t_de,
  t.ue t_ue,
  t.dc t_dc,
  t.uc t_uc,
  vessel_id,
  cf_permit_id,
  trip_start_date,
  port,
  state,
  trip_end_date,
  trip_end_time,
  trip_start_time,
  submit_method,
  activity_type,
  end_port,
  start_port,
  sero_vessel_permit,
  sea_time
FROM
       safis.efforts@secapxdv_dblk.sfsc.noaa.gov e
  JOIN safis.trips@secapxdv_dblk.sfsc.noaa.gov t
  USING ( trip_id )
WHERE
  ( trip_start_date BETWEEN TO_DATE('01-JAN-22', 'dd-mon-yy') AND CURRENT_DATE
  )
"

trip_coord_file_name <-
    file.path(input_path, "trip_coord.rds")

trip_coord_fun <- function(trip_coord_query) {
  return(dbGetQuery(con,
             trip_coord_query))
}

get_trip_coord_info <-
  function() {
      read_rds_or_run(trip_coord_file_name,
                      trip_coord_query,
                      trip_coord_fun,
                      force_from_db)
  }

# 2023-09-20 run the function: 30.94 sec elapsed

# DNF reports
```

## get trip neg 

```{r get trip neg}
# get trip neg ----

trip_neg_2022_file_path <-
  file.path(input_path, "trip_neg_2022.rds")

trip_neg_2022_query <-
  "SELECT *
  FROM
    safis.trips_neg@secapxdv_dblk.sfsc.noaa.gov
WHERE
  ( trip_date BETWEEN TO_DATE('01-JAN-22', 'dd-mon-yy') AND TO_DATE('01-JAN-23'
  , 'dd-mon-yy') )"

# 1495929

trip_neg_2022_fun <-
  function(trip_neg_2022_query) {
    return(dbGetQuery(con, trip_neg_2022_query))
  }

# trip_neg_query_2022: 201.21 sec elapsed
# trip_neg_query_2022: 60.06 sec elapsed
# trip_neg_query_2022: 89.38 sec elapsed

get_trip_neg_2022 <-
  function() {
    read_rds_or_run(trip_neg_2022_file_path,
                    trip_neg_2022_query,
                    trip_neg_2022_fun,
                    force_from_db)
  }
# run the function: 98.23 sec elapsed

# Declarations
```

## trips_notifications 

```{r trips_notifications}
# trips_notifications ----
trips_notifications_2022_query <-
  "SELECT
 *
FROM
  safis.trip_notifications@secapxdv_dblk.sfsc.noaa.gov
WHERE
  ( trip_start_date BETWEEN TO_DATE('01-JAN-22', 'dd-mon-yy') AND TO_DATE('01-JAN-23'
  , 'dd-mon-yy') )
  OR ( trip_end_date BETWEEN TO_DATE('01-JAN-22', 'dd-mon-yy') AND TO_DATE('01-JAN-23'
  , 'dd-mon-yy') )
"

trips_notifications_2022_file_path <-
  file.path(input_path, "trips_notifications_2022.rds")

trips_notifications_2022_fun <-
  function(trips_notifications_2022_query) {
    return(dbGetQuery(con, trips_notifications_2022_query))
  }
# trips_notifications_query: 52.08 sec elapsed
# 97279
# trips_notifications_query: 7.65 sec elapsed

get_trips_notifications_2022 <-
  function() {
    read_rds_or_run(
      trips_notifications_2022_file_path,
      trips_notifications_2022_query,
      trips_notifications_2022_fun,
      force_from_db
    )
  }
# 2023-07-15 run the function: 13.41 sec elapsed
```

## get_vessels with permits 2021+ 

```{r get_vessels with permits 2021+}
# get_vessels with permits 2021+ ----

dates_filter <- " (end_date >= TO_DATE('01-JAN-21', 'dd-mon-yy')
    OR expiration_date >= TO_DATE('01-JAN-21', 'dd-mon-yy') )
  AND effective_date <= CURRENT_DATE
"
# Use that "dates_filter" in all parts of the union below.

# The 3 part union is needed because while the permit table has only one vessel id, the vessel table has 3 different columns for that (sero_official_number, coast_guard_nbr and state_reg_nbr) and we want to join tables by all 3 in turn.
# stringr::str_glue is a function that allows you to create strings with placeholders for variable values. It works by using curly braces {} to enclose variable names within a string.
vessels_permits_query <-
  stringr::str_glue("SELECT
  *
FROM
       srh.mv_sero_fh_permits_his@secapxdv_dblk.sfsc.noaa.gov p
  JOIN safis.vessels@secapxdv_dblk.sfsc.noaa.gov
  ON ( p.vessel_id = sero_official_number )
WHERE {dates_filter}
UNION ALL
SELECT
  *
FROM
       srh.mv_sero_fh_permits_his@secapxdv_dblk.sfsc.noaa.gov p
  JOIN safis.vessels@secapxdv_dblk.sfsc.noaa.gov
  ON ( p.vessel_id = coast_guard_nbr )
WHERE
  {dates_filter}
UNION ALL
SELECT
  *
FROM
       srh.mv_sero_fh_permits_his@secapxdv_dblk.sfsc.noaa.gov p
  JOIN safis.vessels@secapxdv_dblk.sfsc.noaa.gov
  ON ( p.vessel_id = state_reg_nbr )
WHERE
{dates_filter}
")

vessels_permits_file_path <- file.path(input_path, "vessels_permits.rds")

vessels_permits_fun <-
  function(vessels_permits_query) {
    return(dbGetQuery(con,
                      vessels_permits_query))
  }

get_vessels_permits <-
  function() {
    read_rds_or_run(vessels_permits_file_path,
                    vessels_permits_query,
                    vessels_permits_fun,
                    force_from_db) |>
      vessels_permits_id_clean()
  }
# 2023-09-20 run the function: 14.08 sec elapsed
```

## dates_2022 

```{r dates_2022}
# dates_2022 ----
dates_2022_query <-
  "SELECT
  dd.year,
  dd.month_of_year,
  dd.week_of_year,
  dd.complete_date
FROM
  srh.dim_dates@secapxdv_dblk.sfsc.noaa.gov dd
WHERE
  dd.complete_date BETWEEN '01-DEC-2021' AND '31-JAN-2023'
"

dates_2022_file_path <- file.path(input_path, "dates_2022.rds")

dates_2022_fun <-
  function(dates_2022_query) {
    return(dbGetQuery(con,
                      dates_2022_query))
  }

get_dates_2022 <- function() {
  read_rds_or_run(dates_2022_file_path,
                  dates_2022_query,
                  dates_2022_fun,
                  force_from_db)
}
```

## get override data 

```{r get override data}
# get override data ----
compl_err_query <-
    "SELECT
  *
FROM
       srh.srfh_vessel_comp_err@secapxdv_dblk.sfsc.noaa.gov
  INNER JOIN srh.srfh_vessel_comp@secapxdv_dblk.sfsc.noaa.gov
  USING ( srh_vessel_comp_id )
WHERE
  comp_year > '2020'
"
# common fields
#   SRH_VESSEL_COMP_ID
# CREATED_DT
# CREATED_USER_ID
# LU_DT
# LU_USER_ID

# Define a function 'get_compl_err_data_from_db' to retrieve compliance error data from the database.
get_compl_err_data_from_db <- function(compl_err_query) {

  # Use 'ROracle::dbGetQuery' to execute the 'compl_err_query' on the database connection 'con'
  # and store the result in 'compl_err_db_data_0'.
  compl_err_db_data_0 =
    ROracle::dbGetQuery(con, compl_err_query)

  compl_err_db_data_1 <-
    compl_err_db_data_0 %>%
    # remove duplicated columns
    dplyr::select(-c(CREATED_DT,
                     CREATED_USER_ID,
                     LU_DT,
                     LU_USER_ID))

  return(compl_err_db_data_1)
}

file_name_overr <-
  file.path(input_path, "compl_err_db_data_raw.rds")

get_compl_err_db_data <- function() {
  compl_err_db_data_raw <-
    read_rds_or_run(file_name_overr,
                    compl_err_query,
                    get_compl_err_data_from_db,
                    force_from_db)
  # 2023-09-20 run the function: 14.99 sec elapsed

  # Clean the column names of the 'compl_err_db_data_raw' data frame using the 'clean_headers' function defined above.
  compl_err_db_data <- clean_headers(compl_err_db_data_raw)

  return(compl_err_db_data)
}
```

## --- main 

```{r --- main}
# --- main ----
# Define a function 'run_all_get_db_data' to fetch data from the database and store it in a result list.

run_all_get_db_data <-
  function() {
    # Initialize an empty list to store the results.
    result_l = list()

    # 1) Call the 'get_permit_info' function to retrieve permit information from the database.
    mv_sero_fh_permits_his <- get_permit_info()

    # 2) Store the retrieved data in the result list under the name "mv_sero_fh_permits_his."
    result_l[["mv_sero_fh_permits_his"]] <- mv_sero_fh_permits_his
    # dim(mv_sero_fh_permits_his)
    # [1] 183204     22

    # Repeat the steps 1 and 2 for all other types of data using the predefined functions.

    mv_safis_trip_download_data <- get_mv_safis_trip_download()
    result_l[["mv_safis_trip_download"]] <-
      mv_safis_trip_download_data
    # dim(mv_safis_trip_download_data)
    # [1] 735666    149

    mv_tms_trip_notifications_data <-
      get_mv_tms_trip_notifications()
    result_l[["mv_tms_trip_notifications"]] <-
      mv_tms_trip_notifications_data
    # dim(mv_tms_trip_notifications_data)
    # [1] 118730     41

    trips_info <- get_trips_info()
    result_l[["trips_info"]] <- trips_info
    # dim(trips_info)
    # [1] 98528    72 2022
    # [1] 142037   72 2021+

    trip_coord_info <- get_trip_coord_info()
    result_l[["trip_coord_info"]] <- trip_coord_info
    # dim(trip_coord_info)
    # [1] 141350     41

    trip_neg_2022 <- get_trip_neg_2022()
    result_l[["trip_neg_2022"]] <- trip_neg_2022
    # dim(trip_neg_2022)
    # Rows: 1,495,929
    # [1] 746087     12
    # [1] 747173     12

    trips_notifications_2022 <- get_trips_notifications_2022()
    result_l[["trips_notifications_2022"]] <-
      trips_notifications_2022
    # dim(trips_notifications_2022)
    # Rows: 129,701
    # [1] 70056    33

    vessels_permits <- get_vessels_permits()
    result_l[["vessels_permits"]] <- vessels_permits
    # dim(vessels_permits)
    # [1] 78438    51

    dates_2022 <- get_dates_2022()
    result_l[["dates_2022"]] <- dates_2022
    # dim(dates_2022)
    # 427 4

    compl_err_db_data <- get_compl_err_db_data()
    result_l[["compl_err_db_data"]] <- compl_err_db_data
    # dim(compl_err_db_data)
    # [1] 99832    38

    return(result_l)
  }

force_from_db <- NULL # read data from files if exist
# force_from_db <- "YES"

# How to use:
# Add to your code, uncomment and run:
# tic("run_all_get_db_data()")
# all_get_db_data_result_l <- run_all_get_db_data()
# toc()

# Then use like this, for example:
# View(all_get_db_data_result_l)
# mv_safis_trip_download <- all_get_db_data_result_l$mv_safis_trip_download

# Benchmark:
# reading RDS
# run_all_get_db_data(): 1.69 sec elapsed
# reading from db
# run_all_get_db_data(): 259.81 sec elapsed ~ 4 min
# run_all_get_db_data(): 606.66 sec elapsed ~ 10 min with MVs reading

# str(all_get_db_data_result_l[["compl_err_db_data"]])
# 'data.frame':	99832 obs. of  38 variables:
```

#### check 

```{r check}
### check ----
# for each df print its name and dim()
# names(all_get_db_data_result_l) |>
#   map(\(df_name) {
#     c(df_name, dim(all_get_db_data_result_l[[df_name]]))
#   })

# force_from_db <- "NULL"
# dates_2022 <- get_dates_2022()
```

## close the db connection 

```{r close the db connection}
# close the db connection ----
# try(ROracle::dbDisconnect(con))
```

##### Current file: vessel_permit_corrected_list.R 

```{r Current file: vessel_permit_corrected_list.R}
#### Current file: vessel_permit_corrected_list.R ----

# vessel_permit_corrected_list.R
# source("~/R_code_github/useful_functions_module.r")
```

## read Jeannette's file 

```{r read Jeannette_s file}
# read Jeannette's file ----

# the file is from Jeannette Oct 17 2023
v_list_file_name <-
  file.path(
    my_paths$inputs,
    r"(vessels_permits\SA.Permitted.Vessels.Among_revised.Lists.xlsx)"
  )

# Create a sequence of sheet numbers from 1 to 4. These sheet numbers will be used to specify which sheets to read from the Excel file.
sheets <- seq(1:4)

# Use 'purrr::map' to read data from multiple sheets of an Excel file.
# For each sheet, read the data and store it in a list.
all_sheets_l <-
  purrr::map(sheets,
             function(sheet_num) {
               readxl::read_excel(
                 # The name of the Excel file.
                 v_list_file_name,
                 # Specify the sheet number to read.
                 sheet = sheet_num,
                 # Maximum number of rows to guess data types
                 guess_max = 21474836,
                 # Read all columns as text to preserve data integrity.
                 col_types = "text",
                 # Use my function for name repair for column names.
                 .name_repair = fix_names,
                 # Do not use the first row as column names
                 col_names = FALSE
               )
             })
```

## Check what's inside 

```{r Check what_s inside}
# Check what's inside ----
# map(all_sheets_l, dim)
# [[1]]
# [1] 2215    1
#
# [[2]]
# [1] 55  2
#
# [[3]]
# [1] 126   1
#
# [[4]]
# [1] 130   2

# map(all_sheets_l, glimpse)
```

## Clean up dfs 

```{r Clean up dfs}
# Clean up dfs ----
```

### add sheet names to the df list 

```{r add sheet names to the df list}
## add sheet names to the df list ----
# Extract the sheet names from an Excel file specified by 'v_list_file_name'.
# - 'excel_sheets' function is used to get the sheet names.
# - 'str_replace_all' replaces any periods with underscores in sheet names.
# - 'tolower' converts all sheet names to lowercase.
all_sheets_l_names <-
  readxl::excel_sheets(v_list_file_name) |>
  str_replace_all("\\.", "_") |>
  tolower()

# Rename the elements of the 'all_sheets_l' object to match the modified sheet names.
# - 'names' function is used to access and modify the names of the object.
# - 'all_sheets_l_names' is a character vector containing the modified sheet names.
# - Use the sheet names for all dataframes in the 'all_sheets_l'  list.
#   The names of 'all_sheets_l' are updated to match the corresponding sheet names.
names(all_sheets_l) <- all_sheets_l_names[1:length(all_sheets_l)]
```

### Remove old column names 

```{r Remove old column names}
## Remove old column names ----
# like "Vessel id"
# Define a function called 'remove_first_row_if_vessel'.
# This function takes a data frame 'my_df' as its input parameter.
remove_first_row_if_vessel <- function(my_df) {

  # Check if the first cell (1,1) of 'my_df' contains the word "vessel"
  # (case-insensitive search).
  if (grepl("vessel", my_df[1, 1], ignore.case = TRUE)) {
    # If the word "vessel" is found, remove the first row (-1) from 'my_df'.
    my_df <- my_df[-1,]
  }
  # Return the modified 'my_df'. If no changes were made, the original
  # 'my_df' is returned unchanged.
  return(my_df)
}

# Use the 'map' function to apply the 'remove_first_row_if_vessel' function
# to each data frame in the 'all_sheets_l' list.
# The result is a modified 'all_sheets_l' list with the undesired header rows removed from each data frame.

all_sheets_l <-
  purrr::map(all_sheets_l, remove_first_row_if_vessel)

# View(all_sheets_l)
```

### add names to each column 

```{r add names to each column}
## add names to each column ----
```

#### 1) 

```{r 1)}
### 1) ----
# Set the names of the column in the first data frame in the list to "vessel_official_number".
names(all_sheets_l[[1]]) <- c("vessel_official_number")
```

#### 2) 

```{r 2)}
### 2) ----
# Set the names of the column in the second data frame in the list.
names(all_sheets_l[[2]]) <- c("vessel_official_number",
                              "comments")
```

#### 3) 

```{r 3)}
### 3) ----
# Set the names of the column in the third data frame in the list.
names(all_sheets_l[[3]]) <- c("vessel_official_number")
```

#### 4) 

```{r 4)}
### 4) ----
# Set the temporary names of the column in the forth data frame in the list.
names(all_sheets_l[[4]]) <- c("num",
                              "vessel_official_number-comments")
```

##### Split comments 

```{r Split comments}
#### Split comments ----
# Create a new data frame 'sheet_4_temp_1' by splitting the 'vessel_official_number-comments'
# column of the fourth data frame in 'all_sheets_l' using the 'separate_wider_delim'
# function.
sheet_4_temp_1 <- all_sheets_l[[4]] |>
  separate_wider_delim(
    "vessel_official_number-comments",  # Column to split
    delim = " - ",                     # Delimiter used for splitting
    names = c("vessel_official_number", "comments"),  # Names of the new columns
    too_few = "align_end"               # How to handle if fewer columns are created
  )


# Create a new data frame 'sheet_4_temp_2' by performing a series of operations on the
# data frame 'sheet_4_temp_1':

# 1. Use 'group_split' to split 'sheet_4_temp_1' into multiple groups.
#    - 'grp' parameter creates groups based on cumulative sums of NA values
#      in each row. A new group starts when a row contains all NA values.
#    - '.keep = TRUE' means that the grouping column is retained in each group.

# 2. Use 'purrr::map_at' to apply a function to specific columns in each group.
#    - '.at = -1' specifies the position of the columns to be operated on
#      (all columns except the last one).
#    - 'tail' function is applied to each specified column within each group.
#    - '-1' indicates that the last element (row) of each column should be extracted.

# View(sheet_4_temp_1)
sheet_4_temp_2 <-
  sheet_4_temp_1 %>%
  group_split(grp = cumsum(rowSums(is.na(.)) == ncol(.)), .keep = TRUE) %>%
  purrr::map_at(.at = -1, tail, -1)

# Create a new data frame 'sheet_4_temp_3' by applying mutations to the third group
# in 'sheet_4_temp_2' using the 'dplyr::mutate' function.

sheet_4_temp_2[[3]] <- sheet_4_temp_2[[3]] |>
  dplyr::mutate(vessel_official_number =
                  dplyr::case_when(
                    # If 'comments' contains a space, it is Janette's comment, set 'vessel_official_number' to NA.
                    grepl(" ", comments) ~ NA,
                    # For all other cases, remove '.0' from 'comments' and assign it to 'vessel_official_number'.
                    .default = stringr::str_replace(comments, "\\.0", "")
                  ))

# Create a new data frame 'sheet_4' by binding rows from a list of data frames 'sheet_4_temp_2'.
# The 'dplyr::bind_rows' function combines the data frames in the list into a single data frame.
sheet_4 <- dplyr::bind_rows(sheet_4_temp_2)

# put sheet_4 back to the common list
all_sheets_l[[4]] <- sheet_4
# View(all_sheets_l[[4]])
```

## vessels_22_sa 

```{r vessels_22_sa}
# vessels_22_sa ----
# Create a new data frame 'vessels_22_sa' by combining data from two sheets.
vessels_22_sa <-

  # Extract data from the fourth sheet of 'all_sheets_l' (sheet number 4).
  all_sheets_l[[4]] |>

  # Filter rows where the 'grp' column contains values 0 or 2.
  dplyr::filter(grp %in% c(0, 2)) |>

  # dplyr::select only the 'vessel_official_number' column.
  dplyr::select(vessel_official_number) |>

  # Combine the filtered data with data from the first sheet (sheet number 1) of 'all_sheets_l'.
  rbind(all_sheets_l[[1]]) |>
  # na.omit returns the object with incomplete cases removed.
  stats::na.omit()

# vessels_22_sa |>
  # dim()
# [1] 2321    1
```

## remove wrong ids from FHIER results 

```{r remove wrong ids from FHIER results}
# remove wrong ids from FHIER results ----
vessels_to_remove_from_ours <-
  all_sheets_l$in_ours_not_jeannettes$vessel_official_number

# length(vessels_to_remove_from_ours)
# 55

cat("all_sheets_l",
      "vessels_22_sa",
      "vessels_to_remove_from_ours",
    sep = '\n')
```

##### Current file: all_logbooks_db_data_2022_short_p_region_prep.R 

```{r Current file: all_logbooks_db_data_2022_short_p_region_prep.R}
#### Current file: all_logbooks_db_data_2022_short_p_region_prep.R ----

# Prepare all_logbooks_db_data_2022_short_p_region
# 1) download all db data
# 2) use "all logbooks = mv_safis_trip_download
# 3) Filter 2022 only
# 4) Remove unused columns
# 5) Mark sa_only vs. gom and dual for 2022 using vessel list from Jeanette’s comparison

rm_columns <- c("ACTIVITY_TYPE",
"ANYTHING_CAUGHT_FLAG",
"APP_VERSION",
"APPROVAL_DATE",
"APPROVED_BY",
"AVG_DEPTH_IN_FATHOMS",
"CAPT_NAME_FIRST",
"CAPT_NAME_LAST",
"CATCH_DC",
"CATCH_DE",
"CATCH_SEQ",
"CATCH_SOURCE_NAME",
"CATCH_SOURCE",
"CATCH_SPECIES_ITIS",
"CATCH_UE",
"COMMON_NAME",
"CONFIRMATION_SIGNATURE",
"DC",
"DE",
"DEA_PERMIT_ID",
"DEPTH",
"DISPOSITION_CODE",
"DISPOSITION_NAME",
"EFFORT_SEQ",
"EFFORT_TARGET_COMMON_NAMES",
"EFFORT_TARGET_SPECIES_LIST",
"EVENT_ID",
"FISHING_GEAR_DEPTH",
"FISHING_HOURS",
"FORM_VERSION",
"FUEL_DIESEL_GALLON_PRICE",
"FUEL_DIESEL_GALLONS",
"FUEL_GALLON_PRICE",
"FUEL_GALLONS",
"FUEL_GAS_GALLON_PRICE",
"FUEL_GAS_GALLONS",
"GEAR_CATEGORY_CODE",
"GEAR_CATEGORY_NAME",
"GEAR_CODE",
"GEAR_DESC",
"GEAR_NAME",
"GEAR_QUANTITY",
"GEAR_SIZE",
"GEAR_TYPE_CODE",
"GEAR_TYPE_NAME",
"GEARS_FISHING",
"GRADE_CODE",
"GRADE_NAME",
"HOOKS_PER_LINE",
"HOURS_DAYS_FLAG",
"IN_STATE",
"LANDING_SEQ",
"LMA_CODE",
"MARKET_CATEGORY_CODE",
"MARKET_CATEGORY_NAME",
"MAXIMUM_BOTTOM_DEPTH",
"MESH_RING_LENGTH",
"MESH_RING_WIDTH",
"MINIMUM_BOTTOM_DEPTH",
"NBR_OF_CREW",
"NBR_PAYING_PASSENGERS",
"NUM_ANGLERS",
"REPORTED_QUANTITY",
"REPORTING_SOURCE",
"RIG_CODE",
"SPECIES_ITIS",
"SPLIT_TRIP",
"STRETCH_SIZE",
"SUB_TRIP_TYPE",
"SUBMIT_METHOD",
"SUBMITTED_BY_CORPORATE_NAME",
"SUBMITTED_BY_FIRST_NAME",
"SUBMITTED_BY_LAST_NAME",
"SUBMITTED_BY_MIDDEL_NAME",
"SUBMITTED_BY_PARTICIPANT",
"SUPPLIER_EFFCAT_ID",
"SUPPLIER_TRIP_ID",
"TICKET_TYPE",
"TRANSMISSION_DATE",
"TRIP_END_TIME",
"TRIP_FEE",
"TRIP_NBR",
"TRIP_START_TIME",
"TRIP_TYPE",
"UC",
"UE",
"UNIT_MEASURE")

# source(file.path(my_paths$git_r, r"(get_data\get_db_data\get_db_data.R)"))

tic("run_all_get_db_data()")
all_get_db_data_result_l <- run_all_get_db_data()
toc()

# dim(all_get_db_data_result_l$mv_safis_trip_download)
# [1] 735666    149
```

## get 2022 only 

```{r get 2022 only}
# get 2022 only ----
# Create a new variable 'all_logbooks_db_data_2022' using the pipe operator.
# This variable will store the filtered data for the year 2022.
all_logbooks_db_data_2022 <-
  # Take the data from 'all_get_db_data_result_l$mv_safis_trip_download'
  all_get_db_data_result_l$mv_safis_trip_download |>

  # Use the dplyr::filter function to filter rows based on a condition
  dplyr::filter(
    # Check if the 'TRIP_START_DATE' is between "2022-01-01" and "2022-12-31"
    dplyr::between(
      TRIP_START_DATE,                 # Column to check
      as.Date("2022-01-01"),          # Start date for the range
      as.Date("2022-12-31")           # End date for the range
    )
  )

# dim(all_logbooks_db_data_2022)
# [1] 326670    149
```

## Remove unused columns 

```{r Remove unused columns}
# Remove unused columns ----

# Create a new variable 'all_logbooks_db_data_2022_short' by further processing the 'all_logbooks_db_data_2022' data.
all_logbooks_db_data_2022_short <-
  # Take the data from 'all_logbooks_db_data_2022'
  all_logbooks_db_data_2022 |>

  # Use dplyr::select to remove columns specified in 'rm_columns'
  dplyr::select(-any_of(rm_columns)) |>

  # Use dplyr::distinct to retain only distinct rows
  dplyr::distinct()

# dim(all_logbooks_db_data_2022_short)
# [1] 94471    72
```

## Mark sa_only vs. gom and dual for 2022 

```{r Mark sa_only vs. gom and dual for 2022}
# Mark sa_only vs. gom and dual for 2022 ----
# Get vessel list from Jeanette’s comparison
script_path <-
  file.path(my_paths$git_r,
            "vessel_permit_list/vessel_permit_corrected_list.R")

# Source (run) the R script using the constructed script path.
# source(script_path)

# Rows are filtered to keep only vessels whose 'VESSEL_OFFICIAL_NBR' is in the
# 'vessels_22_sa' vector.
all_logbooks_db_data_2022_short_p_region <-
  all_logbooks_db_data_2022_short |>
  # Use the dplyr::mutate function to add a new column 'permit_region' to the dataset
  dplyr::mutate(
    permit_region =
      # Use the case_when function to conditionally assign values to 'permit_region'
      # If vessel number is in 'vessels_22_sa', set to "sa_only"
      dplyr::case_when(VESSEL_OFFICIAL_NBR %in% vessels_22_sa$vessel_official_number ~ "sa_only",
                # For all other cases, set to "gom_and_dual"
                .default = "gom_and_dual")
  )

# dim(all_logbooks_db_data_2022_short_p_region)
# [1] 94471    73

names(all_logbooks_db_data_2022_short_p_region) <-
  names(all_logbooks_db_data_2022_short_p_region) |>
  my_headers_case_function()

# Output the objects, concatenating the representations. cat performs much less conversion than print.
cat("all_logbooks_db_data_2022_short_p_region",
    sep = '\n')
```

##### Current file: fishing_effort_locations_get_data.R 

```{r Current file: fishing_effort_locations_get_data.R}
#### Current file: fishing_effort_locations_get_data.R ----
```

## get area data 

```{r get area data}
# get area data ----
```

## From DB 

```{r From DB}
# From DB ----

# file.exists(file.path(my_paths$git_r,
#                       r"(get_data\all_logbooks_db_data_2022_short_p_region_prep.R)"))

# source(file.path(my_paths$git_r, r"(get_data\all_logbooks_db_data_2022_short_p_region_prep.R)"))
```

##### Current file: fishing_effort_location_by_permit.R 

```{r Current file: fishing_effort_location_by_permit.R}
#### Current file: fishing_effort_location_by_permit.R ----
```

## setup (fishing_effort_location_by_permit) 

```{r setup (fishing_effort_location_by_permit)}
# setup (fishing_effort_location_by_permit) ----

# Source an external R script that contains useful functions.
# source("~/R_code_github/useful_functions_module.r")

# Define the name of the current project as "fishing_effort_location."
current_project_name <- "fishing_effort_location"

# Source another external R script using 'file.path' to construct the full file path.
get_data_path <-
  file.path(# Path to the git repository directory
    my_paths$git_r,
    # Subdirectory for the current project
    current_project_name,
    # Name of the R script to source
    "fishing_effort_locations_get_data.R")

# source(get_data_path)
```

### Data from db as in FHIER 

```{r Data from db as in FHIER}
## Data from db as in FHIER ----
# print_df_names(all_logbooks_db_data_2022_short_p_region)
# [1] 94471    73

# Create a new data frame by processing the data.

coord_data_2022_short_good_all_coords <-
  all_logbooks_db_data_2022_short_p_region |>

  # Convert 'longitude' and 'latitude' columns to numeric data types
  dplyr::mutate(longitude = as.numeric(longitude),
                latitude = as.numeric(latitude)) |>

  # Ensure that all 'longitude' values are negative by taking the absolute value and negating them
  dplyr::mutate(longitude = -abs(longitude)) |>

  # Keep only distinct rows in the data frame
  dplyr::distinct()

# dim(coord_data_2022_short_good_all_coords)
# [1] 94471    73
```

## Keep only full sets of coordinates 

```{r Keep only full sets of coordinates}
# Keep only full sets of coordinates ----
# Create a new data frame 'coord_data_2022_short_good' by filtering and keeping only distinct rows.
coord_data_2022_short_good <-
  coord_data_2022_short_good_all_coords |>

  # Use the filter function to keep rows where either 'longitude' or 'latitude' is not NA
  dplyr::filter(!is.na(longitude) | !is.na(latitude)) |>

  # Keep only distinct rows in the data frame
  dplyr::distinct()

# dim(coord_data_2022_short_good_all_coords)
# [1] 97970    17 FHIER
# [1] 97943    16 db

# dim(coord_data_2022_short_good)
# [1] 97547    17
# [1] 93450    73 db
```

## read in sa shp 

```{r read in sa shp}
# read in sa shp ----
# F2 in RStudio will show the function definition, when the cursor is on the name.
sa_shp_file_path <-
    file.path(
      my_paths$inputs,
      r"(shapefiles\shapefiles_sa_eez_off_states\SA_EEZ_off_states.shp)"
    )

# file.exists(sa_shp_file_path)

# Read a shapefile (geospatial data) from the specified file path and store it in the 'sa_shp' object.
sa_shp <-
  sf::read_sf(sa_shp_file_path)
```

#### convert to sf 

```{r convert to sf}
### convert to sf ----
coord_data_2022_short_good_sf <-
  my_to_sf(coord_data_2022_short_good)
```

## Subset by Big box 

```{r Subset by Big box}
# Subset by Big box ----
# Michelle: I think we need to allow trips that occur anywhere in the GOM, with the eastern lat border being like a big line down the Atlantic Ocean at Bermuda. Does that make sense? Southern Border could be at Cuba. The Northern Border needs to extend up through Maine - since we require reporting no matter where they fish. Basically just a big box, regardless of Council jurisdiction.
# Jessica: I like the big box without council jurisdiction and then I am going to assume we will just plot those trips for vessels with GOM permits?  This should show the Council how many GOM vessels also fish in other regions as well as where they are fishing in the Gulf.

# Define a bounding box represented as a vector named 'big_bounding_box'.
big_bounding_box <- c(
   xmin = -97.79954,  # Minimum longitude (western boundary)
   ymin = 21.521757,  # Minimum latitude (southern boundary, Cuba)
   xmax = -64.790337,  # Maximum longitude (eastern boundary, Bermuda)
   ymax = 49  # Maximum latitude (northern boundary, Canada)
)

tic("coord_data_2022_short_good_sf_crop_big")
# Create a new spatial object 'coord_data_2022_short_good_sf_crop_big'
# by cropping 'coord_data_2022_short_good_sf' to the 'big_bounding_box'.

coord_data_2022_short_good_sf_crop_big <-
  sf::st_crop(coord_data_2022_short_good_sf,  # Spatial object to be cropped
              big_bounding_box)  # Bounding box used for cropping
toc()
# coord_data_2022_short_good_sf_crop_big: 0.89 sec elapsed

# dim(coord_data_2022_short_good_sf_crop_big)
# [1] 95720    18
# [1] 91735    74
```

## convert back to df 

```{r convert back to df}
# convert back to df ----
# The sf::st_drop_geometry function is applied to the spatial object to remove its geometry, resulting in a data frame that contains only the non-geometric attributes or columns.

coord_data_2022_short_good_sf_crop_big_df <-
  coord_data_2022_short_good_sf_crop_big |>
  sf::st_drop_geometry()

# dim(coord_data_2022_short_good_sf_crop_big_df)
# [1] 95720     17
```

## use metrics only vessels not in SRHS 

```{r use metrics only vessels not in SRHS}
# use metrics only vessels not in SRHS ----
# source(r"(~\R_code_github\get_data\get_data_from_fhier\metric_tracking_no_srhs.R)")
# fhier_reports_metrics_tracking_not_srhs_ids
```

### remove ids not in fhier_reports_metrics_tracking_not_srhs_ids 

```{r remove ids not in fhier_reports_metrics_tracking_not_srhs_ids}
## remove ids not in fhier_reports_metrics_tracking_not_srhs_ids ----
# Create a new data frame 'coord_data_2022_short_good_sf_crop_big_df_in_metricks'
# by filtering 'coord_data_2022_short_good_sf_crop_big_df' based on a condition.

coord_data_2022_short_good_sf_crop_big_df_in_metricks <-
  coord_data_2022_short_good_sf_crop_big_df |>

  # Use the 'filter' function to retain rows meeting a specific condition.
  dplyr::filter(
    # Keep rows where the 'vessel_official_nbr' values are present in the fhier_reports_metrics_tracking_not_srhs_ids
    vessel_official_nbr %in% fhier_reports_metrics_tracking_not_srhs_ids$vessel_official_number
  )

# dim(coord_data_2022_short_good_sf_crop_big_df_in_metricks)
# [1] 93581    17
# [1] 90838    73 from mv
```

# Heatmap preparations
##### Current file: prepare_heatmap_func.R 

```{r Current file: prepare_heatmap_func.R}
#### Current file: prepare_heatmap_func.R ----
```

## plot text sizes 

```{r plot text sizes}
# plot text sizes ----
text_sizes <- list(
  geom_text_size = 7,
  plot_title_text_size = 10,
  axis_title_text_size = 9,
  axis_text_x_size = 13,
  axis_text_y_size = 13,
  plot_caption_text_size = 13,
  legend_title_text_size = 10,
  legend_text_text_size = 9,
  #### common axes for Months ----
  y_left_fontsize = 10
)
```

The South Atlantic Council is responsible for the conservation and management of fishery resources in federal waters ranging from 3 to 200 miles off the coasts of North Carolina, South Carolina, Georgia, and east Florida to Key West.

```{r}
states_sa <- data.frame(
  state_name = c(
    "Florida", # can exclude, if go by county
    "Georgia",
    "North Carolina",
    "South Carolina"
  )
)

# Create a data frame 'state_tbl' containing state abbreviations and state names; 2x50.
# - 'state.abb' provides state abbreviations.
# - 'tolower(state.name)' converts state names to lowercase.
# - The resulting data frame has two columns: 'state_abb' and 'state_name'.
state_tbl <- data.frame(state.abb, tolower(state.name))

# Rename the columns in the 'state_tbl' data frame.
# The first column is named 'state_abb', and the second column is named 'state_name'.
names(state_tbl) = c("state_abb", "state_name")

#from the DF, only grab the SA states defined above
sa_state_abb <-
  # a table from above
  state_tbl %>%
  # get only these in our list
  filter(state_name %in% tolower(states_sa$state_name)) %>%
  # get abbreviations
  dplyr::select(state_abb)

# Create a new data frame 'us_s_shp' using the 'tigris' package to obtain U.S. state shapes.
# The 'cb = TRUE' parameter specifies that you want the U.S. state boundaries.
us_s_shp <-
  tigris::states(cb = TRUE, progress_bar = FALSE)

# Rows are retained if the 'NAME' column (state name) matches any of the values in 'states_sa'.
sa_s_shp <-
  us_s_shp |>
  filter(NAME %in% states_sa$state_name)

# Create a new data frame 'us_s_shp' using the 'tigris' package to obtain U.S. state shapes.
# The 'cb = TRUE' parameter specifies that you want the U.S. state boundaries.
us_s_shp <-
  tigris::states(cb = TRUE, progress_bar = FALSE)

# Rows are retained if the 'NAME' column (state name) matches any of the values in 'states_sa'.
sa_s_shp <-
  us_s_shp |>
  filter(NAME %in% states_sa$state_name)
```

## read in GOM shp 

```{r read in GOM shp}
# read in GOM shp ----
# Create a file path using 'file.path' by combining elements from 'my_paths' and specifying a shapefile path.
GOM_400fm_path <-
  file.path(my_paths$inputs, r"(shapefiles\GOM_400fm\GOM_400fm.shp)")
# file.exists(GOM_400fm_path)
# T

# Read a shapefile from the specified file path using 'sf::read_sf'.
# Then, group the resulting data by 'StatZone' and summarize it.
GOMsf <-
  sf::read_sf(GOM_400fm_path) %>%
  group_by(StatZone) %>%
  summarise()
```

## create 5x5 minute grid 

```{r create 5x5 minute grid}
# create 5x5 minute grid ----
# Define a function 'min_grid' that creates a grid of cells within the bounding box of a given spatial data frame.
# - 'my_sf' is the input spatial data frame (default is 'GOMsf').
# - 'minute_num' specifies the grid cell size in minutes.

min_grid <- function(my_sf = GOMsf, minute_num = 1) {
  # Create a grid of cells using 'sf::st_make_grid' within the bounding box of 'my_sf'.
  grid <-
    sf::st_make_grid(x = sf::st_bbox(my_sf),
                     cellsize = 1 / 60 * minute_num) %>%

    # Convert the grid to a spatial data frame using 'sf::st_as_sf'.
    sf::st_as_sf() %>%

    # Add a 'cell_id' column to the grid using 'mutate'.
    dplyr::mutate(cell_id = 1:nrow(.))

  # Return the created grid.
  return(grid)
}

grid_gom5 <- min_grid(GOMsf, 5)
grid_sa5 <- min_grid(sa_shp, 5)

grid_sa5 |>
  head() |>
  knitr::kable(caption = "grid_sa5")

# Set the aggregate attribute to "constant" for multiple spatial objects.
sf::st_agr(GOMsf) =
  sf::st_agr(sa_shp) =
  sf::st_agr(grid_gom5) =
  sf::st_agr(grid_sa5) =
  "constant"
```

#### remove internal boundaries from the GOM shape file 

```{r remove internal boundaries from the GOM shape file}
### remove internal boundaries from the GOM shape file ----
# to speed up the lengthy process try to read a saved file, if exists
my_file_path_local <- file.path(my_paths$outputs,
                           "fishing_effort_location",
                           "st_union_GOMsf.rds")
my_file_path_out <- file.path(my_paths$outputs,
                           "st_union_GOMsf.rds")

# If the file exists, read the data from the RDS file.
if (file.exists(my_file_path_local)) {
  current_path <- my_file_path_local
  st_union_GOMsf <- readr::read_rds(current_path)
} else if (file.exists(my_file_path_out)) {
  current_path <- my_file_path_out
  st_union_GOMsf <- readr::read_rds(current_path)
} else {
  tic("st_union(GOMsf)")
  st_union_GOMsf <- sf::st_union(GOMsf)
  toc()

  readr::write_rds(st_union_GOMsf,
                   my_file_path_out)
}

st_union_GOMsf |>
  head() |>
  knitr::kable(caption = "st_union_GOMsf")
```

### Trips by n min grid 

```{r Trips by n min grid}
## Trips by n min grid ----
# Define a function 'df_join_grid' that joins a data frame with a grid using specified coordinates and CRS.

df_join_grid <- function(my_df, grid, my_crs) {
  # Convert 'my_df' to a spatial data frame with specified coordinates and CRS using 'sf::st_as_sf'.
  my_df_grid <-
    my_df |>
    sf::st_as_sf(
      coords = c("longitude", "latitude"),
      crs = my_crs) |>

  # Join the resulting spatial data frame with the 'grid' using the nearest feature join.
  sf::st_join(grid, join = sf::st_nearest_feature)

  # Return the joined data frame.
  return(my_df_grid)
}

# Define a function 'crop_by_shape' that crops a spatial object using another spatial object.
# - 'my_sf' is the input spatial object to be cropped.
# - 'my_shp' is the spatial object used for cropping (default is 'GOMsf').

crop_by_shape <- function(my_sf, my_shp = GOMsf) {
  # Join 'my_sf' with 'my_shp' to crop it, leaving only the intersecting geometries.
  my_sf |>
    sf::st_join(my_shp, left = FALSE) %>%

  # extract the longitude and latitude coordinates from the joined spatial object.
  dplyr::mutate(longitude = sf::st_coordinates(.)[, 1],
         latitude = sf::st_coordinates(.)[, 2]) %>%

  # Return the cropped and transformed spatial object.
  return()
}
```

### count trip ids and vessels by grid cell function 

```{r count trip ids and vessels by grid cell function}
## count trip ids and vessels by grid cell function ----
# Define a function 'add_vsl_and_trip_cnts' that adds vessel and trip counts to a data frame.
# - 'my_df' is the input data frame.
# - 'vessel_id_name' is the name of the column containing vessel IDs (default is "vessel_official_nbr").

add_vsl_and_trip_cnts <- function(my_df, vessel_id_name = "vessel_official_nbr") {
  # Group the data frame by 'cell_id'.
  my_df |>
    group_by(cell_id) |>

  # Add columns 'vsl_cnt' and 'trip_id_cnt' with counts of distinct vessel and trip IDs.
    # sym() take strings as input and turn them into symbols.
    # The !! (bang-bang or unquote) operator is used to unquote the symbol, allowing it to be used in dplyr verbs like dplyr::mutate, dplyr::select, or other functions that accept column names.
    # So, the code !!rlang::sym(vessel_id_name) effectively evaluates to the column name specified by the vessel_id_name variable in the context of a dplyr verb, allowing you to work with the column dynamically based on the variable's value.

    dplyr::mutate(
      vsl_cnt =
        dplyr::n_distinct(!!rlang::sym(vessel_id_name)),
      trip_id_cnt =
        dplyr::n_distinct(trip_id)
    ) |>

  # Ungroup the data frame to remove grouping and return the result.
  dplyr::ungroup() %>%

  # Return the modified data frame.
  return()
}
```

### make a plot function 

```{r make a plot function}
## make a plot function ----
# Define a function 'make_map_trips' to create a ggplot2 heatmap of trip data.
# - 'map_trip_base_data' is the data containing trip information to be mapped.
# - 'shape_data' is the shape data used as a backdrop for mapping.
# - 'total_trips_title' is the title for the total trips legend.
# - 'trip_cnt_name' is the name of the column with trip counts.
# - 'caption_text' is the caption for the plot.
# - 'unit_num' specifies the unit size for the legend.
# - 'print_stat_zone' is an optional argument to include StatZone labels.
make_map_trips <-
  function(map_trip_base_data,
           shape_data,
           total_trips_title,
           trip_cnt_name,
           caption_text = "Heat map of SEFHIER trips (5 min. resolution).",
           unit_num = 1, # the width of the scale key
           print_stat_zone = NULL,
           legend_text_text_size = text_sizes[["legend_text_text_size"]]
           ) {
    # Calculate the maximum number of trips for legend scaling.
    max_num <- max(map_trip_base_data[[trip_cnt_name]])

    # Create a ggplot2 plot 'map_trips'.
    map_trips <-
      ggplot() +
      # Add a filled heatmap using 'geom_sf'.
      geom_sf(data = map_trip_base_data,
              aes(geometry = x,
                  fill = !!sym(trip_cnt_name)),
              colour = NA) +
      # Add the shape data with no fill.
      geom_sf(data = shape_data, fill = NA)

    # Check for an optional argument 'print_stat_zone'.
    if (!missing(print_stat_zone)) {
      map_trips <-
        map_trips +
        # Add StatZone labels using 'geom_sf_text'.
        geom_sf_text(data = shape_data,
                     aes(geometry = geometry,
                         label = StatZone),
                     size = 3.5)
    }

    map_trips <-
        map_trips +
      # Set plot labels and theme settings.
      labs(
        x = "",
        y = "",
        fill = "",
        # caption = caption_text
        title = caption_text
      ) +
      # theme_bw() +
      scale_fill_viridis(
        name = total_trips_title,
        labels = scales::comma,
        trans = "log1p",
        limits = c(1, max_num)
      ) +
      theme(
        # legend.position = "top",
        # legend.justification = "left",
        # legend.key.width = unit(unit_num, "npc"),
        legend.key.height = grid::unit(unit_num, "line"),
        legend.title = element_text(size =
                                      text_sizes[["legend_title_text_size"]]),
        legend.text = element_text(size =
                                     legend_text_text_size), # for charter heatmap use 7
        plot.caption = element_text(hjust = 0,
                                    size = text_sizes[["plot_caption_text_size"]]),
    axis.text.x =
      element_text(size = text_sizes[["axis_text_x_size"]]),
axis.text.y =
      element_text(size = text_sizes[["axis_text_y_size"]])
      ) +
      # Add a legend guide for fill color.
      guides(fill = guide_colourbar(title.position = "top"))

    # Return the created 'map_trips' plot.
    return(map_trips)
  }
```

##### Current file: fishing_effort_location_heatmap.R 

```{r Current file: fishing_effort_location_heatmap.R}
#### Current file: fishing_effort_location_heatmap.R ----

# Use the 'source' function to execute R code from a file located at the given path.
```

## Heatmap 

```{r Heatmap}
# Heatmap ----
```

### heatmap data 

```{r heatmap data}
## heatmap data ----

# Split the data frame into multiple sub-data frames based on the 'permit_region' column.

coord_data_2022_short_good_sf_crop_big_df_in_metricks_list <-
  split(
    # Data frame to be split
    coord_data_2022_short_good_sf_crop_big_df_in_metricks,

    # Split based on the 'permit_region' column
    as.factor(
      coord_data_2022_short_good_sf_crop_big_df_in_metricks$permit_region
    )
  )

# Use the 'map' function to apply the 'dim' function to each element in the list.
purrr::map(
  coord_data_2022_short_good_sf_crop_big_df_in_metricks_list,
  dim
)

# gom
# Create a new data frame 'for_heatmap_lat_lon_trips_vessels_gom_only' by applying a series of data manipulation operations.

for_heatmap_lat_lon_trips_vessels_gom_only <-
  coord_data_2022_short_good_sf_crop_big_df_in_metricks_list$gom_and_dual |>

  # dplyr::select specific columns.
  dplyr::select(trip_id, vessel_official_nbr, latitude, longitude) |>

  # Remove duplicate rows using 'distinct'.
  dplyr::distinct()

# dim(for_heatmap_lat_lon_trips_vessels_gom_only)
# Rows: 41,455
# [1] 46763     4 mv

# sa
for_heatmap_lat_lon_trips_vessels_sa_only <-
  coord_data_2022_short_good_sf_crop_big_df_in_metricks_list$sa_only |>
  # dplyr::select specific columns.
  dplyr::select(trip_id, vessel_official_nbr, latitude, longitude) |>
  # Remove duplicate rows using 'distinct'.
  dplyr::distinct()

# dim(for_heatmap_lat_lon_trips_vessels_sa_only)
# [1] 68122     4
# [1] 44060     4

for_heatmap_lat_lon_trips_vessels_sa_only |>
  head() |>
  knitr::kable(caption = "for_heatmap_lat_lon_trips_vessels_sa_only")
```

#### remove vessels not in Jeannette's SA list 

```{r remove vessels not in Jeannette_s SA list}
### remove vessels not in Jeannette's SA list ----

# Build the path to the R script 'vessel_permit_corrected_list.R' by
# combining the base path 'my_paths$git_r' and the script name.
script_path <-
  file.path(my_paths$git_r,
            "vessel_permit_list/vessel_permit_corrected_list.R")

# Source (run) the R script using the constructed script path.
# source(script_path)

# Rows are filtered to exclude vessels whose 'VESSEL_OFFICIAL_NBR' is in the
# 'vessels_to_remove_from_ours' vector.
for_heatmap_lat_lon_trips_vessels_sa_only_rm <-
  for_heatmap_lat_lon_trips_vessels_sa_only |>
  filter(!vessel_official_nbr %in% vessels_to_remove_from_ours)

# dim(for_heatmap_lat_lon_trips_vessels_sa_only_rm)
# [1] 67983     4
```

### add the grid 

```{r add the grid}
## add the grid ----
# assuming data is dataframe with variables LATITUDE, LONGITUDE, and trips

tic("effort_vsl_gom")
# Create a new object 'my_crs' by extracting the coordinate reference system (CRS)
# from a spatial object 'GOMsf'.
my_crs <- sf::st_crs(GOMsf)
# Create a new data frame 'effort_vsl_gom' by joining the data frames
# 'for_heatmap_lat_lon_trips_vessels_gom_only' and 'grid_gom5' based on a common
# spatial reference system defined by 'my_crs'.
effort_vsl_gom <-
  df_join_grid(for_heatmap_lat_lon_trips_vessels_gom_only,
               grid_gom5,
               my_crs)
toc()
# effort_vsl: 0.62 sec elapsed

tic("effort_vsl_sa")
# Create a new object 'my_crs_sa' by extracting the coordinate reference system (CRS)
# from a spatial object 'sa_shp'.
my_crs_sa <- sf::st_crs(sa_shp)

# Create a new data frame 'effort_vsl_sa' by joining the data frames
# 'for_heatmap_lat_lon_trips_vessels_sa_only_rm' and 'grid_sa5' based on a
# spatial reference system defined by 'my_crs_sa'.
effort_vsl_sa <-
  df_join_grid(for_heatmap_lat_lon_trips_vessels_sa_only_rm,
               grid_sa5,
               my_crs = my_crs_sa)
toc()
# effort_vsl_sa: 1.22 sec elapsed

effort_vsl_sa |>
  head() |>
  knitr::kable(caption = "effort_vsl_sa")
```

### crop by the shape 

```{r crop by the shape}
## crop by the shape ----
tic("effort_vsl_cropped_gom")
effort_vsl_cropped_gom <- crop_by_shape(effort_vsl_gom)
toc()
# effort_cropped2: 0.44 sec elapsed

# dim(effort_vsl_cropped_gom)
# [1] 35822     7
# [1] 40604     7 mv

ggplot() +
  # Add the shape data with no fill.
  geom_sf(data = effort_vsl_sa) +
  # Add a filled heatmap using 'sa_shp'.
  geom_sf(data = sa_shp,
          colour = "red",
          fill = NA)

tic("effort_vsl_cropped_sa")
effort_vsl_cropped_sa <- crop_by_shape(effort_vsl_sa, sa_shp)
toc()
# effort_vsl_cropped_sa: 0.54 sec elapsed

ggplot() +
  # Add a filled heatmap using 'sa_shp'.
  geom_sf(data = effort_vsl_cropped_sa,
          colour = "darkred") +
  # Add a filled heatmap using 'sa_shp'.
  geom_sf(data = sa_shp,
          colour = "red",
          fill = NA)

# dim(effort_vsl_cropped_sa)
# [1] 21461     8
# [1] 20147     8 mv
```

### count trip ids and vessels by grid cell 

```{r count trip ids and vessels by grid cell}
## count trip ids and vessels by grid cell ----

# Create a list 'effort_vsl_cropped_cnt_l' by applying 'add_vsl_and_trip_cnts' function to data frames.

effort_vsl_cropped_cnt_l <-
  list(effort_vsl_cropped_gom, effort_vsl_cropped_sa) |>

  # Use the 'map' function to apply a function to each element in the list.
  purrr::map(function(effort_vsl_cropped) {

    # Apply the 'add_vsl_and_trip_cnts' function to each 'effort_vsl_cropped' data frame.
    add_vsl_and_trip_cnts(effort_vsl_cropped)
  })

# map(effort_vsl_cropped_cnt_l, dim)
# [[1]]
# [1] 35822     9
#
# [[2]]
# [1] 21461    10
# mv data:
# [[1]]
# [1] 40604     9
#
# [[2]]
# [1] 20147    10

effort_vsl_cropped_cnt_l$effort_vsl_cropped_sa |>
  head() |>
  knitr::kable(caption = "effort_vsl_cropped_cnt_l")
```

#### remove extra columns 

```{r remove extra columns}
### remove extra columns ----
# Create a list 'effort_cropped_short_cnt2_short_l' by applying a set of operations to data frames.

effort_cropped_short_cnt2_short_l <-
  effort_vsl_cropped_cnt_l |>

  # Use the 'map' function to apply a function to each element in the list.
  purrr::map(function(effort_vsl_cropped_cnt) {

    # Use the 'select' function to remove specific columns,
    # 'latitude', 'longitude', 'trip_id', and 'VESSEL_OFFICIAL_NBR', from each data frame.
    effort_vsl_cropped_cnt |>
      dplyr::select(-c(latitude, longitude, trip_id, vessel_official_nbr))
  })

# map(effort_cropped_short_cnt2_short_l, dim)

effort_cropped_short_cnt2_short_l$effort_vsl_cropped_sa |>
  head() |>
  knitr::kable(caption = "effort_vsl_cropped_cnt_l")
```

#### join data with grids; no rule 3 

```{r join data with grids; no rule 3}
### join data with grids; no rule 3 ----
heat.plt_gom <-
  # Extract the first element from the list.
  # Use the pipe operator to pass it to the next operation.
  effort_cropped_short_cnt2_short_l[[1]] |>
  # Perform an inner join with the data frame 'grid_gom5'
  # using a common column specified by 'join_by(cell_id)'.
  # Store the result in the variable 'heat.plt_gom'.
  # Have to use data.frame, to avoid:
  # Error: y should not have class sf; for spatial joins, use st_join
  inner_join(data.frame(grid_gom5))

# the same for SA
heat.plt_sa <-
  effort_cropped_short_cnt2_short_l[[2]] |>
  # have to use data.frame, to avoid
  # Error: y should not have class sf; for spatial joins, use st_join
  inner_join(data.frame(grid_sa5))
# Joining with `by = join_by(cell_id)`

heat.plt_sa |>
  head() |>
  knitr::kable(caption = "heat.plt_sa")
```

### make a plot 

```{r make a plot}
## make a plot ----

max_num3_gom <- max(heat.plt_gom$trip_id_cnt)
# 1209
# 1317 mv

max_num3_sa <- max(heat.plt_sa$trip_id_cnt)
# 590
# 561 mv

max_num3_sa
```

#### GOM & dual 2022 

```{r GOM & dual 2022}
### GOM & dual 2022 ----
map_trips_no_rule_3_gom <-
  make_map_trips(heat.plt_gom,
           st_union_GOMsf,
           "total trips",
           trip_cnt_name = "trip_id_cnt",
           # unit_num = 1.2
           unit_num = 3,
           legend_text_text_size = 6)

```
```{r show gom heatmap}
```
```{r out.width = "90%", fig.fullwidth = TRUE}
#| column: screen-inset-shaded

map_trips_no_rule_3_gom
```

#### SA 2022 

```{r SA 2022}
### SA 2022 ----
map_trips_no_rule_3_sa <-
  make_map_trips(heat.plt_sa,
           sa_shp,
           "total trips",
           trip_cnt_name = "trip_id_cnt",
           # unit_num = 0.9,
           unit_num = 5,
           # legend_text_text_size = 7.5
           legend_text_text_size = 6
           )

```
```{r show sa heatmap}
```
```{r out.width = "80%", fig.fullwidth = TRUE}
#| column: screen-inset-shaded

map_trips_no_rule_3_sa +
# Add the spatial features from 'sa_s_shp' to the plot using 'geom_sf'.
geom_sf(data = sa_s_shp) +

# Annotate the plot with text labels from 'sa_s_shp' using 'geom_sf_text'.
geom_sf_text(data = sa_s_shp,
               label = sa_s_shp$NAME,  # Use the 'NAME' column as labels.
               size = 3)  # Set the size of the text labels to 3.
```

## To get end port numbers by state 

```{r To get end port numbers by state}
# To get end port numbers by state ----
# permit_end_port_path <-
#   file.path(
#     my_paths$git_r,
#     r"(fishing_effort_location\fishing_effort_location_by_permit_and_end_port.R)"
#   )
#
# # source(permit_end_port_path)
```

##### Current file: fishing_effort_location_by_permit_and_end_port.R 

```{r Current file: fishing_effort_location_by_permit_and_end_port.R}
#### Current file: fishing_effort_location_by_permit_and_end_port.R ----

# fishing_effort_location_by_permit_and_end_port

# effort_vsl_cropped_cnt_l |> View()
# coord_data_2022_short_good_sf_crop_big_df_in_metricks_list |> View()

# Split the data frame into multiple sub-data frames based on the 'permit_region' column.

coord_data_2022_short_good_sf_crop_big_df_in_metricks_list <-
  split(
    # Data frame to be split
    coord_data_2022_short_good_sf_crop_big_df_in_metricks,

    # Split based on the 'permit_region' column
    as.factor(
      coord_data_2022_short_good_sf_crop_big_df_in_metricks$permit_region
    )
  )

# Use the 'map' function to apply the 'dim' function to each element in the list.
purrr::map(
  coord_data_2022_short_good_sf_crop_big_df_in_metricks_list,
  dim
)

# $gom_and_dual
# [1] 46772    73
#
# $sa_only
# [1] 44066    73

coord_data_2022_short_good_sf_crop_big_df_in_metricks_list$sa_only |>
  dplyr::select(activity_type_name) |>
  dplyr::distinct()
# 1    TRIP WITH EFFORT
# 2                <NA>
# 3 TRIP UNABLE TO FISH

coord_data_2022_short_good_sf_crop_big_df_in_metricks_list$sa_only |>
  dplyr::select(notif_landing_location_state) |>
  dplyr::distinct()
# <NA>
# AL
# FL
# TX
# LA

sa_end_port <-
  coord_data_2022_short_good_sf_crop_big_df_in_metricks_list$sa_only |>
  dplyr::select(
    trip_id,
    vessel_id,
    vessel_official_nbr,
    end_port_state,
    notif_landing_location_state
  ) |>
  dplyr::distinct()

dim(sa_end_port)
# [1] 44009     5

sa_end_port_cnt_vessels <-
  sa_end_port |>
  dplyr::select(vessel_official_nbr, end_port_state) |>
  dplyr::distinct() |>
  filter(end_port_state %in% sa_state_abb$state_abb) |>
  count(end_port_state)
#   END_PORT_STATE   n
# 1             FL 565
# 2             GA  25
# 3             NC 217
# 4             SC 142

# This code snippet performs operations on the `sa_end_port` data frame to count
# the occurrences of unique values in the 'notif_landing_location_state' column
# that match the values in the 'state_abb' column of the 'sa_state_abb' data frame.

# The `sa_end_port` data frame is piped into the 'select' function to keep only
# the 'vessel_official_nbr' and 'notif_landing_location_state' columns.
sa_end_port |>
  dplyr::select(vessel_official_nbr, notif_landing_location_state) |>

# The 'filter' function is used to retain only the rows where
# 'notif_landing_location_state' values are present in the 'state_abb' column of
# the 'sa_state_abb' data frame.
  filter(notif_landing_location_state %in% sa_state_abb$state_abb) |>

# The 'count' function calculates the number of occurrences of each unique
# 'notif_landing_location_state' value in the resulting data frame, returning the
# count of occurrences for each state.
  count(notif_landing_location_state)
# 1                           FL 688

# This code snippet calculates the count of unique end_port_state values in the
# sa_end_port data frame that match the values in the 'state_abb' column of the
# 'sa_state_abb' data frame.

# The `sa_end_port` data frame is piped into the 'select' function to retain only
# the 'trip_id' and 'end_port_state' columns.
sa_end_port_cnt_trips <- sa_end_port |>
  dplyr::select(trip_id, end_port_state) |>

# The 'distinct' function is used to retain unique rows based on the combination
# of 'trip_id' and 'end_port_state'.
  dplyr::distinct() |>

# The 'filter' function is applied to keep only rows where 'end_port_state' is
# present in the 'state_abb' column of the 'sa_state_abb' data frame.
  filter(end_port_state %in% sa_state_abb$state_abb) |>

# Finally, the 'count' function calculates the number of occurrences of each
# unique 'end_port_state' value in the resulting data frame, returning the count
# of trips for each state.
  count(end_port_state)

#   END_PORT_STATE     n
# FL 27332
# GA   351
# NC  7503
# SC  5128

sa_end_port_cnt_trips |>
  head() |>
  knitr::kable(caption = "sa_end_port_cnt_trips")
```

