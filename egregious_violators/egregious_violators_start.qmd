---
title: Egregious Violators
---

# Setup 

```{r Setup}
# Setup ----
```

This script identifies and processes "egregious violators" in the SEFHIER program.

SEFHIER stands for Southeast For-Hire Integrated Electronic Reporting.

It combines compliance and correspondence data, applies specific filters,
and prepares a report of vessels requiring further investigation.

The Setup section includes necessary libraries, functions, and data loading.

### The "egregious violator" definition 

```{r The egregious violator definition}
## The "egregious violator" definition ----
```


1. NO reports for all 26 weeks back from week ago today;

2. Permits have not expired and were active for the same period as (1);

3. The grace period is 7 days back from today.

4. It needs to be that we called at least 1 time and emailed at least 1 time. Or they contacted us at least once.

5. Not counting any correspondence (regardless of the type - email/call, voicemail or not) that includes "No contact made" in the text of the entry as an actual "direct" contact for any egregious vessel (May 6 2024)

### New requirement 2023-08-09 

```{r New requirement 20230809}
## New requirement 2023-08-09 ----
```

6. It should be at least 2 contact "attempts". i.e., if they are ignoring our calls and emails then they cannot continue to go on in perpetuity without reporting and never be seen as egregious. So, at least 1 call (could be a voicemail) and also at a 2nd call (could be a voicemail) or an email. So, if we called 1x and left a voicemail and then attempted an email, then we have tried enough at this point and they need to be passed to OLE.

at least 1 call (could be a voicemail) and also at a 2nd call (could be a voicemail) or an email. So, if we called 1x and left a voicemail and then attempted an email, then we have tried enough

### New requirement 2024-02-26 

```{r New requirement 20240226}
## New requirement 2024-02-26 ----
```

7. It needs to be that we called at least 1 time and emailed at least 1 time. Or they contacted us at least once.

### New requirement 2024-05-06 

```{r New requirement 20240506}
## New requirement 2024-05-06 ----
```

8. Exclude any correspondence (regardless of the type - email/call, voicemail or not) that includes "No contact made" in the text of the entry as a actual "direct" contact for any egregious vessel.

Note. Update (download) all input files every time before run.

Note: If there is no comment with the word "manually" before the code, it will work automatically.

Note: In the following code '<<<<' and '>>>>' mark the start and the end of definitions and help documents for helper functions.

```{r no cache setup, results='hide', message=FALSE, warning=FALSE, cache=FALSE, include=FALSE}



#' Quarto
#'
#' Quarto enables you to weave together content and executable code into a finished document.
#'
#' Running Code
#'
#' The **Run** button allows you to run individual or bunch of chunks as a regular R script.
#'
#' When you click the **Render** button a document will be generated that includes both content and the output of embedded code.
#'

# Setup for Quarto

# Load required libraries
# A general-purpose tool for dynamic report generation in R
library(knitr)

# Adds features to a kable output
library(kableExtra)

# Format R code automatically
library(styler)

# Load the tidyr package for data manipulation
library(tidyr)

# Load the magrittr package for piping operations
library(magrittr)

# Load the stringr package for string manipulation
library(stringr)

# Load the openxlsx package for reading and writing Excel files
library(openxlsx)

```

```{r df format setup}
#| include: false

# Customize the appearance of dataframes in HTML

# Uncomment if using tabs
# kable <- function(data) {
#   knitr::kable(data, booktabs = true, digits = 2) %>%
#     kable_styling('striped', full_width = FALSE)
# }

# Define a custom print function for data frames in knitr
knit_print.data.frame = function(x, ...) {
  res = paste(c(
    '',
    '',
    knitr::kable(x, digits = 2) |>
      kableExtra::kable_styling('striped', full_width = FALSE)
  ),
  collapse = '
')
  knitr::asis_output(res)
}

# Register the custom print function for data frames in the knitr namespace
registerS3method(
  'knit_print', 'data.frame', knit_print.data.frame,
  envir = asNamespace('knitr')
)

# Set global chunk options in knitr if needed
# knitr::opts_chunk$set(echo = TRUE)

# Set the table format for knitr to HTML if needed
# options(knitr.table.format = 'HTML')

# End of Quarto setup

```

### Set up packages and options 

```{r Set up packages and options}
## Set up packages and options ----
# <<<<

# Explanations for the following code:

 get_username  <-  function(){
    return(as.character(Sys.info()["user"]))
}

 get_username  <-  function(){
    return(as.character(Sys.info()["user"]))
}
# >>>>


# Install if needed and load all the packages.
# 
# *NB*. It is better to install/load each package separately, if any one suggest updates it is safe to choose option 1 (update all).
# 
# Load the ROracle package for database interactions with Oracle databases
library(ROracle)
# Collection of package development tools.
library(devtools)
```


Explainations for the following code:

- `if (!require("auxfunctions"))` checks if the `auxfunctions` package is installed and loaded:

  - `require("auxfunctions")` attempts to load the `auxfunctions` package.

  - The `!` operator negates the result, so the condition is true if the package is not installed or cannot be loaded.

- `devtools::install_github("AShipunova1/R_code/auxfunctions")` installs the `auxfunctions` package from the specified GitHub repository:

  - `devtools::install_github()` is a function from the `devtools` package that installs an R package directly from a GitHub repository.

  - `"AShipunova1/R_code/auxfunctions"` specifies the repository and subdirectory containing the package.

This code checks if the `auxfunctions` package is available, and if not, it installs it from the GitHub repository `AShipunova1/R_code/auxfunctions`.

One doesn't have to have a GitHub account to use it.

The installation details depend on the username.

For most users, install from the main branch if not already installed

For the package developer, install from the development branch to test new features


```{r}
# Check if the username is not "anna.shipunova"
if (!auxfunctions::get_username() == "anna.shipunova") {
    # If the auxfunctions package is not installed, install it from GitHub
  if (!require('auxfunctions')) {
    devtools::install_github("AShipunova1/R_code/auxfunctions")
  }
} else {
  # For Anna Shipunova, rebuild the package from the development branch. To force the installation change to 'force = TRUE'
  devtools::install_github("AShipunova1/R_code/auxfunctions@development", force = FALSE)
  # restart R session to pick up changes
  # .rs.restartR()
}

# Helper functions for SEFHIER data analysis.
library(auxfunctions)
# Handling time series data.
library(zoo)
# Compares dataframes and identifies differences.
library(diffdf)

# Install and attach R packages for Google Sheets and Google Drive
# 
# Refer to this guide: https://felixanalytix.medium.com/how-to-read-write-append-google-sheet-data-using-r-programming-ecf278108691#:~:text=There%20are%203%20ways%20to%20read%20this%20Google%20sheet%20into%20R.&text=Just%20to%20take%20the%20URL,URL%20but%20just%20the%20ID).
# 
library(googlesheets4) # Google Sheets via the Sheets API v4 
library(googledrive) # Interact with Google Drive 

# Set options to prevent converting long numbers to scientific notation for input/output in spreadsheets and csv files
# This ensures vessel numbers and other large integers are displayed in full

options(scipen = 999)

# Synchronize timezone settings between R and Oracle to ensure consistent date-time handling
Sys.setenv(TZ = Sys.timezone())
Sys.setenv(ORA_SDTZ = Sys.timezone())
```

### Set up paths 

```{r Set up paths}
## Set up paths ----
# <<<<

# Explanations for the following code:

 function_message_print  <-  function(text_msg) {
  cat(crayon::bgCyan$bold(text_msg),
      sep = "\n")
}

 function_message_print  <-  function(text_msg) {
  cat(crayon::bgCyan$bold(text_msg),
      sep = "\n")
}
# >>>>

# <<<<

# Explanations for the following code:

 set_work_dir  <-  function() {

  # Set the working directory to the user's home directory (~)
  setwd("~/")
  base_dir <- getwd()

  # Define 'main_r_dir' as "R_files_local"
  main_r_dir <- "R_files_local"

  # Define 'in_dir' as "my_inputs"
  in_dir <- "my_inputs"

  # Construct the full path to 'my_inputs' directory
  full_path_to_in_dir <- file.path(base_dir, main_r_dir, in_dir)

  # Define 'out_dir' as "my_outputs"
  out_dir <- "my_outputs"

  # Construct the full path to 'my_outputs' directory
  full_path_to_out_dir <- file.path(base_dir, main_r_dir, out_dir)

  # Define 'git_r_dir' as "R_code_github"
  git_r_dir <- "R_code_github"

  # Construct the full path to 'R_code_github' directory
  full_path_to_r_git_dir <- file.path(base_dir, git_r_dir)

  # Change the working directory to 'R_files_local'
  setwd(file.path(base_dir, main_r_dir))

  # Create a list of directory paths for 'inputs,' 'outputs,' and 'git_r'
  my_paths <- list("inputs" = full_path_to_in_dir,
                   "outputs" = full_path_to_out_dir,
                   "git_r" = full_path_to_r_git_dir)

  # Return the list of directory paths
  return(my_paths)
}

 set_work_dir  <-  function() {

  # Set the working directory to the user's home directory (~)
  setwd("~/")
  base_dir <- getwd()

  # Define 'main_r_dir' as "R_files_local"
  main_r_dir <- "R_files_local"

  # Define 'in_dir' as "my_inputs"
  in_dir <- "my_inputs"

  # Construct the full path to 'my_inputs' directory
  full_path_to_in_dir <- file.path(base_dir, main_r_dir, in_dir)

  # Define 'out_dir' as "my_outputs"
  out_dir <- "my_outputs"

  # Construct the full path to 'my_outputs' directory
  full_path_to_out_dir <- file.path(base_dir, main_r_dir, out_dir)

  # Define 'git_r_dir' as "R_code_github"
  git_r_dir <- "R_code_github"

  # Construct the full path to 'R_code_github' directory
  full_path_to_r_git_dir <- file.path(base_dir, git_r_dir)

  # Change the working directory to 'R_files_local'
  setwd(file.path(base_dir, main_r_dir))

  # Create a list of directory paths for 'inputs,' 'outputs,' and 'git_r'
  my_paths <- list("inputs" = full_path_to_in_dir,
                   "outputs" = full_path_to_out_dir,
                   "git_r" = full_path_to_r_git_dir)

  # Return the list of directory paths
  return(my_paths)
}
# >>>>

# <<<<

# Explanations for the following code:

 current_project_paths  <-  function() {
  
  my_paths <- auxfunctions::set_work_dir()
  
  #' get this project name
  current_project_dir_name <- this.path::this.dir()
  
  #' find its base name
  current_project_name <-
    basename(current_project_dir_name)
  
  #' use current_project_name to create input and output paths
  curr_proj_input_path <- file.path(my_paths$inputs, current_project_name)
  
  auxfunctions::create_dir_if_not(curr_proj_input_path)
  
  curr_proj_output_path <- file.path(my_paths$outputs, current_project_name)
  
  auxfunctions::create_dir_if_not(curr_proj_output_path)
  
  current_proj_paths <-
    list(
      "project_name" = current_project_name,
      "code" = current_project_dir_name,
      "input" = curr_proj_input_path,
      "output" = curr_proj_output_path
    )
  
  return(current_proj_paths)
}

 current_project_paths  <-  function() {
  
  my_paths <- auxfunctions::set_work_dir()
  
  #' get this project name
  current_project_dir_name <- this.path::this.dir()
  
  #' find its base name
  current_project_name <-
    basename(current_project_dir_name)
  
  #' use current_project_name to create input and output paths
  curr_proj_input_path <- file.path(my_paths$inputs, current_project_name)
  
  auxfunctions::create_dir_if_not(curr_proj_input_path)
  
  curr_proj_output_path <- file.path(my_paths$outputs, current_project_name)
  
  auxfunctions::create_dir_if_not(curr_proj_output_path)
  
  current_proj_paths <-
    list(
      "project_name" = current_project_name,
      "code" = current_project_dir_name,
      "input" = curr_proj_input_path,
      "output" = curr_proj_output_path
    )
  
  return(current_proj_paths)
}
# >>>>

# <<<<

# Explanations for the following code:

 create_dir_if_not  <-  function(curr_dir_name) {
  # Check if the directory does not exist
  if (!dir.exists(curr_dir_name)) {
    dir.create(curr_dir_name)  # Create the directory if it doesn't exist
  }
}

 create_dir_if_not  <-  function(curr_dir_name) {
  # Check if the directory does not exist
  if (!dir.exists(curr_dir_name)) {
    dir.create(curr_dir_name)  # Create the directory if it doesn't exist
  }
}
# >>>>
```


Different methods are used based on the user to accommodate different directory structure.

This allows the script to run correctly on multiple systems without manual path changes.

In the code in this section all user provided values have the word "manually" in the description. Everything else is created automatically.

Manually: Change the following 2 lists (*my_paths* and *current_in_out_paths*) to your environment if needed. The variable _names_ are used throughout the code, so please change only the quoted _values_ inside the lists.


```{r}
# Check if the current username is not "anna.shipunova"
if (!auxfunctions::get_username() == "anna.shipunova") {
  auxfunctions::function_message_print(
    "Please CHANGE the following 2 lists values to your environment if needed. Use full path to your directories in quotes."
  )
  
  # 1) General directories (to look up additional files, e.g. processed data). It can be left as is if you don't have it. You can provide path to individual files later.
  my_paths <- list(inputs  = "~/my_inputs",
                   outputs = "~/my_outputs",
                   git_r   = "~/R_code")
  
  # 2) Current project code, input and output directories
  current_in_out_paths <-
    list(
      project_name = "validation_survey",
      code = "~/validation_survey/code",
      input = "~/validation_survey/input",
      output = "~/validation_survey/output"
    )
  
} else {
  # If the username is "anna.shipunova", use Anna's directory structure.
  my_paths <- auxfunctions::set_work_dir()
  current_in_out_paths <- auxfunctions::current_project_paths()
}
```


The following section uses provided directory names lists to automatically create separate variables for future use and create current input/output directories if they do not exists.


Create variables to store shortcuts to project directories


```{r}
# This is usually the current directory name.
current_project_name <- current_in_out_paths$project_name

current_project_path <- current_in_out_paths$code
            
current_project_input_path <- current_in_out_paths$input

current_project_output_path <- current_in_out_paths$output
```

Create input and output directories if they don't exist

```{r}
auxfunctions::create_dir_if_not(current_project_input_path)

auxfunctions::create_dir_if_not(current_project_output_path)
```

#### Additional individual paths to data files 

```{r Additional individual paths to data files}
### Additional individual paths to data files ----
```

This section sets up paths for specific data files used in the project
##### Compliance and Correspondence 

```{r Compliance and Correspondence}
#### Compliance and Correspondence ----
# <<<<

# Explanations for the following code:

 prepare_csv_full_path  <-  function(filenames_list,
           add_path,
           input_dir_part = NA) {
    
    #' my_paths is a default
    if (is.na(input_dir_part)) {
      my_paths <- auxfunctions::set_work_dir()
      input_dir_part <- my_paths$inputs
    }
    
    #' add_path
    #' Use subdirectory names for correspondence and compliance files.
    #' add_path <- "from_Fhier/Correspondence"
    #' or
    #' add_path <- "from_Fhier/FHIER Compliance"
    
    # Use 'sapply' to add paths in front of each filename in the 'filenames_list' vector.
    my_list <- sapply(filenames_list, function(x) {
      file.path(input_dir_part, add_path, x)
    })
    
    # Convert the resulting list into a character vector and return it.
    return(paste(my_list) %>% as.list())
  }

 prepare_csv_full_path  <-  function(filenames_list,
           add_path,
           input_dir_part = NA) {
    
    #' my_paths is a default
    if (is.na(input_dir_part)) {
      my_paths <- auxfunctions::set_work_dir()
      input_dir_part <- my_paths$inputs
    }
    
    #' add_path
    #' Use subdirectory names for correspondence and compliance files.
    #' add_path <- "from_Fhier/Correspondence"
    #' or
    #' add_path <- "from_Fhier/FHIER Compliance"
    
    # Use 'sapply' to add paths in front of each filename in the 'filenames_list' vector.
    my_list <- sapply(filenames_list, function(x) {
      file.path(input_dir_part, add_path, x)
    })
    
    # Convert the resulting list into a character vector and return it.
    return(paste(my_list) %>% as.list())
  }
# >>>>
```


Download from FHIER first.

Manually: Provide full paths here, changing _values_ inside the quotes:


```{r}
correspondence_csv_path <- "Your full path to correspondence.csv"
fhier_compliance_csv_path_list <- 
  list("Your full path to fhier_compliance.csv year 1",
       "Your full path to fhier_compliance.csv year 2")
```


Depending on a user name who runs the code, the file paths are constructed here.


```{r}
# Check if the username is not "anna.shipunova"
if (!auxfunctions::get_username() == "anna.shipunova") {
    # Combine correspondence CSV path and compliance CSV paths into one list
  all_csv_full_paths_list <-
    c(correspondence_csv_path, fhier_compliance_csv_path_list)
} else {
  # For Anna Shipunova
  # Manually: Change file names to the last download
  # Using raw string literals (r"()") to handle backslashes in file paths

  all_csv_names_list = c(
    "Correspondence_2024_06_17.csv",
    r"(2024_06_17\FHIER_Compliance_2023__06_17_2024.csv)",
    r"(2024_06_17\FHIER_Compliance_2024__06_17_2024.csv)"
  )
  
  # Add a full path in front of each file name for correspondence CSV
  # prepare_csv_full_path function constructs full file paths by combining base path, additional path, and file names

  corresp_full_path <-
    auxfunctions::prepare_csv_full_path(all_csv_names_list[[1]],
                          add_path = "from_Fhier/Correspondence",
                          input_dir_part = my_paths$inputs)
  
  # Add a full path in front of each file name for compliance CSVs
  compliance_full_paths <-
    auxfunctions::prepare_csv_full_path(all_csv_names_list[2:3],
                                        add_path = "from_Fhier/FHIER Compliance",
                                        input_dir_part = my_paths$inputs)
  
  # Combine correspondence full path and compliance full paths into one list
  all_csv_full_paths_list <-
    c(corresp_full_path,
      compliance_full_paths)

  # all_csv_full_paths_list contains full paths to all required CSV files for correspondence and compliance

  # Check if all the specified files exist
  purrr::map(all_csv_full_paths_list, file.exists)
}
```

##### Processed Metric Tracking (permits from FHIER) 

```{r Processed Metric Tracking permits from FHIER}
#### Processed Metric Tracking (permits from FHIER) ----
```


processed_metrics_tracking_file_names contains paths to RDS files with SEFHIER permitted vessels data for different years
Manually: Add your full path to processed Metrics tracking for each year instead of "Your full path here".

Define paths for processed Metrics tracking CSVs.

Depending on a user name who runs the code, the file paths are constructed here.



```{r}
# Check if the username is not "anna.shipunova"
if (!auxfunctions::get_username() == "anna.shipunova") {
  processed_metrics_tracking_file_names <- 
    c("Your full path here/SEFHIER_permitted_vessels_nonSRHS_2022.rds",
      "Your full path here/SEFHIER_permitted_vessels_nonSRHS_2023.rds")
} else {
  # for Anna Shipunova
  processed_input_data_path <-
    file.path(my_paths$inputs, "processing_logbook_data", "Outputs")

  # check
  dir.exists(processed_input_data_path)
  # if not TRUE: Check your provided path and/or create manually.
  
  # Get file names for all years
  processed_metrics_tracking_file_names_all <-
  list.files(path = processed_input_data_path,
             pattern = "SEFHIER_permitted_vessels_nonSRHS_*",
             recursive = TRUE,
             full.names = TRUE)

  # Exclude links (shortcuts) to ensure we're only working with actual data files
  processed_metrics_tracking_file_names <-
  grep(
    processed_metrics_tracking_file_names_all,
    pattern = "Shortcut.lnk",
    invert = TRUE,
    value = TRUE
  )

}
```


Check if all processed metrics tracking files exist

```{r}
purrr::map(processed_metrics_tracking_file_names, file.exists)
```


if not TRUE: Check your provided path and/or create manually.

##### Physical Address List from FHIER path 

```{r Physical Address List from FHIER path}
#### Physical Address List from FHIER path ----
```

Download first from REPORTS / For-hire Primary Physical Address List

Set the path for FHIER addresses based on the user

Manually: Add your full path instead of "Your full path here".


```{r}
# Check if the username is not "anna.shipunova"
if (!auxfunctions::get_username() == "anna.shipunova") {
  fhier_addresses_path <- "Your full path here"
} else {
  # for Anna Shipunova, update file name's date
  fhier_addresses_path <-
    file.path(
      my_paths$inputs,
      r"(from_Fhier\address\For-hire Primary Physical Address List_06_17_2024.csv)"
    )
}
```


fhier_addresses_path points to a CSV file containing the primary physical addresses of for-hire vessels

Verify that the FHIER addresses file exists at the specified path, correct the path if it is doesn't exist


```{r}
file.exists(fhier_addresses_path)
```

##### Home port processed city and state path 

```{r Home port processed city and state path}
#### Home port processed city and state path ----
```

Download first from Google drive.

Set the path for processed PIMS home ports based on the user.

Manually: Add your full path instead of "Your full path here".


```{r}
# Check if the username is not "anna.shipunova"
if (!auxfunctions::get_username() == "anna.shipunova") {
  processed_pims_home_ports_path <- "Your full path here"
} else {
  # for Anna Shipunova, update file name's date
  processed_pims_home_ports_path <-
    file.path(my_paths$outputs,
              "home_ports",
              "vessels_from_pims_ports_2024-06-18.csv")
}
```

processed_pims_home_ports_path points to a CSV file containing processed data about vessel home ports, including city and state information

Verify that the processed PIMS home ports file exists at the specified path, correct the path if it is doesn't exist.

```{r}
file.exists(processed_pims_home_ports_path)
```

##### Data from the previous results of "egregious violators for investigation" path 

```{r Data from the previous results of egregious violators for investigation path}
#### Data from the previous results of "egregious violators for investigation" path ----
```


Depending on a user name who runs the code, define the path to the previous results file.

Manually: Add your full path instead of "Your full path here".


```{r}
# Check if the username is not "anna.shipunova"
if (!auxfunctions::get_username() == "anna.shipunova") {
  prev_result_path <- "Your full path here"
} else {
  # for Anna Shipunova, update file name's date
  prev_result_path <-
    file.path(current_project_input_path,
              "egregious_violators_to_investigate_2024-05-17.xlsx")
}
```

Check if the previous results file exists.
If it is doesn't exist download it first and correct the path, or load directly from Google drive, see Get data.

```{r}
file.exists(prev_result_path)
```

#### Set up Google Drive paths 

```{r Set up Google Drive paths}
### Set up Google Drive paths ----
```

Hard coded Google drive folder names, manually change here if changing in Google drive.

```{r}
egr_violators_googledrive_folder_name <- "Egregious violators"
output_egr_violators_googledrive_folder_name <- "output"
```


Get the path to the main Egregious violators folder on Google Drive

It is used to read the previous result and for saving the new result.

When asked for the authentication the first time choose the appropriate option and follow the instructions. 

If there is an option with your google email account (like your.name@noaa.gov) you can choose that option (usually #2) and it will confirm your access automatically.

`n_max = 1` means we will use the first result, assuming we have only one folder with such name on Google dive.


```{r}
egr_violators_googledrive_folder_path <-
  googledrive::drive_find(pattern =
                            egr_violators_googledrive_folder_name,
                          type = "folder",
                          n_max = 1)
```


Get the path to the output folder within the Egregious violators folder on Google Drive

Explainations for the following code:

- `output_egr_violators_googledrive_folder_path <- ...` assigns the result of the `googledrive::drive_ls` function call to the variable `output_egr_violators_googledrive_folder_path`.

- `googledrive::drive_ls(...)` lists the contents of a Google Drive folder based on the specified parameters:

  - `path = googledrive::as_id(egr_violators_googledrive_folder_path)` specifies the path to the folder using its ID:

    - `googledrive::as_id(egr_violators_googledrive_folder_path)` converts `egr_violators_googledrive_folder_path` into a format recognized as an ID by the `googledrive` package.

  - `pattern = output_egr_violators_googledrive_folder_name` specifies a pattern to match folder names against, using `output_egr_violators_googledrive_folder_name`.

  - `type = "folder"` specifies that only folders should be listed.

  - `n_max = 1` specifies that only the first matching folder should be returned.
  

```{r}
output_egr_violators_googledrive_folder_path <-
  googledrive::drive_ls(
    path = googledrive::as_id(egr_violators_googledrive_folder_path),
    pattern = output_egr_violators_googledrive_folder_name,
    type = "folder",
    n_max = 1
  )
```

### Define dates 

```{r Define dates}
## Define dates ----
```


Define start and end years for the analysis period

Manually: These values may be adjusted as needed


```{r}
# start year for the analysis
my_year1 <- "2023"
my_beginning1 <- stringr::str_glue("{my_year1}-01-01")
my_end1 <- stringr::str_glue("{my_year1}-12-31")

# last year for the analysis
my_year2 <- "2024"
my_beginning2 <- stringr::str_glue("{my_year2}-01-01")
my_end2 <- stringr::str_glue("{my_year2}-12-31")
```


Following are the definitions of dates used throughout the code.

Set the current date as the data file date

```{r}
data_file_date <- 
  lubridate::today()
```

How many weeks and days to take in to the account?
The 26-week period is used to define long-term non-compliance

```{r}
number_of_weeks_for_non_compliancy = 26
```

Calculate number of days in non compl weeks

```{r}
days_in_non_compl_weeks <- 
  number_of_weeks_for_non_compliancy * 7

# test, should be TRUE
days_in_non_compl_weeks == 182
```


The 7-day grace period allows for recent reports that may not yet be processed

```{r}
grace_period = 7 # days

# Calculate the date 26 weeks (plus grace period) before the current date
half_year_ago <-
  data_file_date - days_in_non_compl_weeks - grace_period
```


Check the week number and day of the week for the period's start

This can be used to verify the calculation against a calendar


```{r}
lubridate::week(half_year_ago)

lubridate::wday(half_year_ago, label = T)
```



```{r}
# Set the minimum date for permit expiration (30 days from today)
permit_expired_check_date <- data_file_date + 30
```


Define the start of the last week, excluding it from analysis

```{r}
last_week_start <- data_file_date - grace_period
```

## Get data 

```{r Get data}
# Get data ----
```

# Prepare data


```{r}
# This is used only with source()
get_data_path <- 
  file.path(current_project_path, "egregious_violators_get_data.R")
# get data for egregious violators
# use from egregious_violators_start.R
```

This section loads and prepares various datasets required for the egregious violators analysis

- Compliance records

- Correspondence logs

- Permit information

- Address data

The data is loaded from CSV files, databases, and previously processed results.

This section outlines the different data sources and their purposes in the analysis.
 
The following data are loaded from files or from the Oracle database.

1) compliance data
Download files from FHIER / Reports / FHIER COMPLIANCE REPORT 

For the last 6 month

FHIER_Compliance_...csv

2) correspondence data

Download files from FHIER / Home / Correspondence

Actions / Download 

For the whole period, starting 01/01/2021

"~\my_inputs\from_Fhier\Correspondence\Correspondence_2024_02_15.csv"

3) processed Metrics tracking

From a separate code
 
For the last 6 month

SEFHIER_permitted_vessels_nonSRHS_YEAR.csv

4) Physical Address List from FHIER

Downloaded from REPORTS / For-hire Primary Physical Address List

For the whole period, starting 01/01/2021

"For-hire Primary Physical Address List.csv"

5) home port cleaned city and state from PIMS

"~\R_files_local\my_outputs\home_ports\vessels_from_pims_ports.csv"

6) address information from Oracle db

"db_participants_address.rds"

7) Previous results (from google drive)

There are two options here, to downloaded the file as .xlsx or to get it directly from Google drive into R.

"~\egregious_violators\egregious_violators_to_investigate_2024-05-17.xlsx"

## FHIER 

```{r FHIER}
# FHIER ----
```

This section focuses on data from the Fisheries Information System for Harvest and Effort Reporting (FHIER)

Compliance

Correspondence

Permit info from processed metrics tracking

### Compliance and Correspondence data 

```{r Compliance and Correspondence data}
## Compliance and Correspondence data ----
# <<<<

# Explanations for the following code:

 clean_all_csvs  <-  function(csvs, vessel_id_field_name = NA) {
  # Clean headers of all CSVs using the 'clean_headers' function
  csvs_clean0 <- lapply(csvs, clean_headers)

  # Trim 'vesselofficialnumber' column (if specified) in all cleaned CSVs
  csvs_clean1 <-
    auxfunctions::trim_all_vessel_ids_simple(csvs_clean0, vessel_id_field_name)

  # Return the list of cleaned CSVs
  return(csvs_clean1)
}

 clean_all_csvs  <-  function(csvs, vessel_id_field_name = NA) {
  # Clean headers of all CSVs using the 'clean_headers' function
  csvs_clean0 <- lapply(csvs, clean_headers)

  # Trim 'vesselofficialnumber' column (if specified) in all cleaned CSVs
  csvs_clean1 <-
    auxfunctions::trim_all_vessel_ids_simple(csvs_clean0, vessel_id_field_name)

  # Return the list of cleaned CSVs
  return(csvs_clean1)
}
# >>>>

# <<<<

# Explanations for the following code:

 corresp_cleaning  <-  function(corresp_df) {
    corresp_df_contact_cnts <- auxfunctions::add_count_contacts(corresp_df)
    
    createdon_field_name <-
      auxfunctions::find_col_name(corresp_df, "created", "on")[1]
    contactdate_field_name <-
      auxfunctions::find_col_name(corresp_df, "contact", "date")[1]
    
    corresp_df_contact_cnts <-
      auxfunctions::change_to_dates(corresp_df_contact_cnts, createdon_field_name)
    corresp_df_contact_cnts <-
      auxfunctions::change_to_dates(corresp_df_contact_cnts, contactdate_field_name)
    
    return(corresp_df_contact_cnts)
  }

 corresp_cleaning  <-  function(corresp_df) {
    corresp_df_contact_cnts <- auxfunctions::add_count_contacts(corresp_df)
    
    createdon_field_name <-
      auxfunctions::find_col_name(corresp_df, "created", "on")[1]
    contactdate_field_name <-
      auxfunctions::find_col_name(corresp_df, "contact", "date")[1]
    
    corresp_df_contact_cnts <-
      auxfunctions::change_to_dates(corresp_df_contact_cnts, createdon_field_name)
    corresp_df_contact_cnts <-
      auxfunctions::change_to_dates(corresp_df_contact_cnts, contactdate_field_name)
    
    return(corresp_df_contact_cnts)
  }
# >>>>

# <<<<

# Explanations for the following code:

 compliance_cleaning  <-  function(compl_arr) {
  # Initialize 'compl' as the input 'compl_arr'.
  # if it is just one df already, do nothing
  compl <- compl_arr

  # Clean the 'week' column by splitting it into three columns with proper classes: 'week_num' (week order number), 'week_start', and 'week_end'.
  compl_clean <-
    purrr::map(compl, clean_weeks)

  # Find a column name containing 'permit', 'group', and 'expiration' (permitgroupexpiration).
  permitgroupexpirations <-
    purrr::map(compl,
        \(x) {
          grep("permit.*group.*expiration",
               tolower(names(x)),
               value = TRUE)
        })

  # Change the classes of dates in the 'permitgroupexpiration' columns from character to POSIXct.
  compl_dates <-
    compl_clean |>
    purrr::imap(\(x, idx) {
      field_name <- permitgroupexpirations[[idx]]
      x |>
        dplyr::mutate({{field_name}} := as.POSIXct(dplyr::pull(x[field_name]),
                                            format = "%m/%d/%Y"))
      # change_to_dates(x, permitgroupexpirations[[idx]], "%m/%d/%Y")
    })

  # Return the cleaned and processed compliance data.
  return(compl_dates)
}

 compliance_cleaning  <-  function(compl_arr) {
  # Initialize 'compl' as the input 'compl_arr'.
  # if it is just one df already, do nothing
  compl <- compl_arr

  # Clean the 'week' column by splitting it into three columns with proper classes: 'week_num' (week order number), 'week_start', and 'week_end'.
  compl_clean <-
    purrr::map(compl, clean_weeks)

  # Find a column name containing 'permit', 'group', and 'expiration' (permitgroupexpiration).
  permitgroupexpirations <-
    purrr::map(compl,
        \(x) {
          grep("permit.*group.*expiration",
               tolower(names(x)),
               value = TRUE)
        })

  # Change the classes of dates in the 'permitgroupexpiration' columns from character to POSIXct.
  compl_dates <-
    compl_clean |>
    purrr::imap(\(x, idx) {
      field_name <- permitgroupexpirations[[idx]]
      x |>
        dplyr::mutate({{field_name}} := as.POSIXct(dplyr::pull(x[field_name]),
                                            format = "%m/%d/%Y"))
      # change_to_dates(x, permitgroupexpirations[[idx]], "%m/%d/%Y")
    })

  # Return the cleaned and processed compliance data.
  return(compl_dates)
}
# >>>>
```

This section handles the processing of compliance reports and correspondence data from FHIER

It reads Compliance and Correspondence CSV files, cleans them by trimming vessel IDs and cleaning column names, and processes correspondence data to add a column indicating if a contact was made. The file paths and the processing logic differ based on the user running the code.

Read correspondence and compliance csvs

Load CSV files into a list, treating all columns as character type


```{r}
csv_contents <- 
  lapply(all_csv_full_paths_list, 
         readr::read_csv, 
         col_types = readr::cols(.default = 'c'))
```

Clean all CSVs: Trim vessel IDs and clean column names

Apply cleaning functions to standardize data across all CSV files

Replace all non-alphanumeric characters with underscores ('_'), unify the case


```{r}
csvs_clean1 <- 
  auxfunctions::clean_all_csvs(csv_contents)
```

Every time processing for Compliance and Correspondence downloaded from FHIER

For correspondence:

Extract the first element (correspondence data) from the cleaned CSV list.

Add a new column named "was_contacted", which indicates whether a contact was made with each vessel based on the presence of a contact date. If the contact date is missing (`NA`), it assigns "no"; otherwise, it assigns "yes".

- The `add_count` function is then used to count the number of contacts per vessel, distinguishing between vessels that were contacted and those that were not. The result is stored in a new column named "contact_freq".

Change to date format `created_on` and `contact_date` fields

Clean and process correspondence data, adding contact information and frequency


```{r}
corresp_contact_cnts_clean0 <- 
  csvs_clean1[[1]] |> 
  auxfunctions::corresp_cleaning()

# Display the structure of the cleaned correspondence data
head(corresp_contact_cnts_clean0) |> 
  glimpse()
```

For compliance:

Clean and process compliance data for all years

Use all dataframes from the csvs_clean1 list except the first (correspondence)


```{r}
compl_clean_list <-
  csvs_clean1[2:length(csvs_clean1)] |>
  auxfunctions::compliance_cleaning()
```

Assign analysis years as names to the compliance data frames for easy reference

```{r}
names(compl_clean_list) <- c(my_year1, my_year2)
```

Check the size of each cleaned compliance data frame to ensure proper data loading and processing

```{r}
purrr::map(compl_clean_list, dim)
```

Example result for dimensions check

```{r}
# $`2023`
# [1] 149731     20
# 
# $`2024`
# [1] 71350    20
```

This step merges the cleaned compliance data from multiple years into a single dataset for easier analysis

```{r}
compl_clean <-
  rbind(compl_clean_list[[my_year1]], compl_clean_list[[my_year2]])
```

Check dimensions of the combined compliance data frame
This helps verify the size of the merged dataset and ensure all data was combined correctly

```{r}
dim(compl_clean)
# [1] 221081     20
```

Check dimensions of the cleaned correspondence data frame

```{r}
dim(corresp_contact_cnts_clean0)
# [1] 34549    22
```

### Get Metric Tracking (permits from FHIER) 

```{r Get Metric Tracking permits from FHIER}
## Get Metric Tracking (permits from FHIER) ----
```

The metrics tracking data contains permit information from FHIER (For-Hire Integrated Electronic Reporting Program)

It is processed using a separate script (processing_metrics_tracking.R) stored on Google Drive
 
Read the processed metrics tracking files for all years

```{r}
processed_metrics_tracking_permits <-
  purrr::map_df(processed_metrics_tracking_file_names,
         readr::read_rds)
```

Convert column names to lowercase for consistency

This ensures uniform naming conventions across different datasets


```{r}
names(processed_metrics_tracking_permits) <-
  names(processed_metrics_tracking_permits) |>
  tolower()
```

Example of column names

```{r}
# [1] "vessel_official_number, vessel_name, effective_date, end_date, permits, sa_permits_, gom_permits_, permit_region, permit_sa_gom_dual"
```

Check dimensions of the processed metrics tracking data frame

```{r}
dim(processed_metrics_tracking_permits)
```

An example 

```{r}
# [1] 9977    9
```

### Physical Address List from FHIER 

```{r Physical Address List from FHIER}
## Physical Address List from FHIER ----
# <<<<

# Explanations for the following code:

 fix_names  <-  function(x) {
  # Use the pipe operator %>%
  x %>%

    # Remove dots from column names
    stringr::str_replace_all("\\.", "") %>%

    # Replace all characters that are not letters or numbers with underscores
    stringr::str_replace_all("[^A-z0-9]", "_") %>%

    # Ensure that letters are only in the beginning of the column name
    stringr::str_replace_all("^(_*)(.+)", "\\2\\1") %>%

    # Convert column names to lowercase using 'my_headers_case_function'
    my_headers_case_function()
}

 fix_names  <-  function(x) {
  # Use the pipe operator %>%
  x %>%

    # Remove dots from column names
    stringr::str_replace_all("\\.", "") %>%

    # Replace all characters that are not letters or numbers with underscores
    stringr::str_replace_all("[^A-z0-9]", "_") %>%

    # Ensure that letters are only in the beginning of the column name
    stringr::str_replace_all("^(_*)(.+)", "\\2\\1") %>%

    # Convert column names to lowercase using 'my_headers_case_function'
    my_headers_case_function()
}
# >>>>
```


Download first from REPORTS / For-hire Primary Physical Address List.

Load FHIER addresses from the provided path

This dataset contains physical address information for for-hire vessels.

Read all columns as characters.

Use the same column names convention.


```{r}
fhier_addresses <-
  readr::read_csv(fhier_addresses_path,
           col_types = readr::cols(.default = 'c'),
           name_repair = auxfunctions::fix_names)
```

Check dimensions of the loaded FHIER addresses data frame

```{r}
dim(fhier_addresses)
# Example result: [1] 3386    7
```

## PIMS 

```{r PIMS}
# PIMS ----
```


Load processed PIMS (Permit Information Management System) home port data

### Home port processed city and state 

```{r Home port processed city and state}
## Home port processed city and state ----
```

This dataset contains information about vessel home ports, including city and state


```{r}
processed_pims_home_ports <- 
  readr::read_csv(processed_pims_home_ports_path)

# Example
dim(processed_pims_home_ports)
# [1] 23303     3
```

## Load from Oracle db 

```{r Load from Oracle db}
# Load from Oracle db ----
```

### Get owners addresses 

```{r Get owners addresses}
## Get owners addresses ----
# <<<<

# Explanations for the following code:

 connect_to_secpr  <-  function() {
    # Retrieve the username associated with the "SECPR" database from the keyring.
    my_username <- keyring::key_list("SECPR")[1, 2]

    # Use 'dbConnect' to establish a database connection with the specified credentials.
    con <- DBI::dbConnect(
        DBI::dbDriver("Oracle"),  # Use the Oracle database driver.
        username = my_username,  # Use the retrieved username.
        password = keyring::key_get("SECPR", my_username),  # Retrieve the password from the keyring.
        dbname = "SECPR"  # Specify the name of the database as "SECPR."
    )

    # Return the established database connection.
    return(con)
}

 connect_to_secpr  <-  function() {
    # Retrieve the username associated with the "SECPR" database from the keyring.
    my_username <- keyring::key_list("SECPR")[1, 2]

    # Use 'dbConnect' to establish a database connection with the specified credentials.
    con <- DBI::dbConnect(
        DBI::dbDriver("Oracle"),  # Use the Oracle database driver.
        username = my_username,  # Use the retrieved username.
        password = keyring::key_get("SECPR", my_username),  # Retrieve the password from the keyring.
        dbname = "SECPR"  # Specify the name of the database as "SECPR."
    )

    # Return the established database connection.
    return(con)
}
# >>>>

# <<<<

# Explanations for the following code:

 read_rds_or_run  <-  function(my_file_path,
                            my_data = as.data.frame(""),
                            my_function,
                            force_from_db = NULL) {

  if (file.exists(my_file_path)) {
    modif_time <- file.info(my_file_path)$mtime
  }

    # Check if the file specified by 'my_file_path' exists and 'force_from_db' is not set.
    if (file.exists(my_file_path) &
        is.null(force_from_db)) {
        # If the file exists and 'force_from_db' is not set, read the data from the RDS file.

        function_message_print("File already exists, reading.")

        my_result <- readr::read_rds(my_file_path)

    } else {

      # If the file doesn't exist or 'force_from_db' is set, perform the following steps:

      # 0. Print this message.
      function_message_print(c(
        "File",
        my_file_path,
        "doesn't exists, pulling data from database.",
        "Must be on VPN."
      ))

      # 1. Generate a message indicating the date and the purpose of the run for "tic".
      msg_text <-
        paste(today(), "run for", basename(my_file_path))
      tictoc::tic(msg_text)  # Start timing the operation.

      # 2. Run the specified function 'my_function' on the provided 'my_data' to generate the result. I.e. download data from the Oracle database. Must be on VPN.

      my_result <- my_function(my_data)

      tictoc::toc()  # Stop timing the operation.

      # 3. Save the result as an RDS binary file to 'my_file_path' for future use.
      # try is a wrapper to run an expression that might fail and allow the user's code to handle error-recovery.

      # 4. Print this message.
      function_message_print(c("Saving new data into a file here: ",
                       my_file_path))

      try(readr::write_rds(my_result,
                           my_file_path))

      modif_time <- date()
    }

  # Print out the formatted string with the file name ('my_file_name') and the modification time ('modif_time') to keep track of when the data were downloaded.
  my_file_name <- basename(my_file_path)
  function_message_print(
    stringr::str_glue("File: {my_file_name} modified {modif_time}"))

    # Return the generated or read data.
    return(my_result)
}

 read_rds_or_run  <-  function(my_file_path,
                            my_data = as.data.frame(""),
                            my_function,
                            force_from_db = NULL) {

  if (file.exists(my_file_path)) {
    modif_time <- file.info(my_file_path)$mtime
  }

    # Check if the file specified by 'my_file_path' exists and 'force_from_db' is not set.
    if (file.exists(my_file_path) &
        is.null(force_from_db)) {
        # If the file exists and 'force_from_db' is not set, read the data from the RDS file.

        function_message_print("File already exists, reading.")

        my_result <- readr::read_rds(my_file_path)

    } else {

      # If the file doesn't exist or 'force_from_db' is set, perform the following steps:

      # 0. Print this message.
      function_message_print(c(
        "File",
        my_file_path,
        "doesn't exists, pulling data from database.",
        "Must be on VPN."
      ))

      # 1. Generate a message indicating the date and the purpose of the run for "tic".
      msg_text <-
        paste(today(), "run for", basename(my_file_path))
      tictoc::tic(msg_text)  # Start timing the operation.

      # 2. Run the specified function 'my_function' on the provided 'my_data' to generate the result. I.e. download data from the Oracle database. Must be on VPN.

      my_result <- my_function(my_data)

      tictoc::toc()  # Stop timing the operation.

      # 3. Save the result as an RDS binary file to 'my_file_path' for future use.
      # try is a wrapper to run an expression that might fail and allow the user's code to handle error-recovery.

      # 4. Print this message.
      function_message_print(c("Saving new data into a file here: ",
                       my_file_path))

      try(readr::write_rds(my_result,
                           my_file_path))

      modif_time <- date()
    }

  # Print out the formatted string with the file name ('my_file_name') and the modification time ('modif_time') to keep track of when the data were downloaded.
  my_file_name <- basename(my_file_path)
  function_message_print(
    stringr::str_glue("File: {my_file_name} modified {modif_time}"))

    # Return the generated or read data.
    return(my_result)
}
# >>>>

# <<<<

# Explanations for the following code:

 remove_empty_cols  <-  function(my_df) {
  # Define an inner function "not_all_na" that checks if any value in a vector is not NA.
  not_all_na <- function(x) any(!is.na(x))

  my_df |>
    # Select columns from "my_df" where the result of the "not_all_na" function is true,
    # i.e., select columns that have at least one non-NA value.
    dplyr::select(tidyselect::where(not_all_na)) %>%
    # Return the modified data frame, which contains only the selected columns.
    return()
}

 remove_empty_cols  <-  function(my_df) {
  # Define an inner function "not_all_na" that checks if any value in a vector is not NA.
  not_all_na <- function(x) any(!is.na(x))

  my_df |>
    # Select columns from "my_df" where the result of the "not_all_na" function is true,
    # i.e., select columns that have at least one non-NA value.
    dplyr::select(tidyselect::where(not_all_na)) %>%
    # Return the modified data frame, which contains only the selected columns.
    return()
}
# >>>>

# <<<<

# Explanations for the following code:

 clean_headers  <-  function(my_df) {
    # Use the 'fix_names' function to clean and fix the column names of the dataframe.
    new_names <-
        colnames(my_df) |>
        auxfunctions::fix_names()

    colnames(my_df) <- 
        new_names
    
    # Return the dataframe with cleaned and fixed column names.
    return(my_df)
}

 clean_headers  <-  function(my_df) {
    # Use the 'fix_names' function to clean and fix the column names of the dataframe.
    new_names <-
        colnames(my_df) |>
        auxfunctions::fix_names()

    colnames(my_df) <- 
        new_names
    
    # Return the dataframe with cleaned and fixed column names.
    return(my_df)
}
# >>>>
```

Create parameters for `read_rds_or_run` function to read or download "participants address"


```{r}
# Define the SQL query to fetch participant address data
db_participants_address_query <-
  "select * from
SRH.MV_SERO_VESSEL_ENTITY@secapxdv_dblk
"

# Set the file path for storing or reading the participant address data

# It uses the predefined path to the input directory and a file name to read or write to.
db_participants_address_file_path <-
  file.path(current_project_input_path,
            "db_participants_address.rds")
```


Attempt to establish a connection to Oracle database

Print an error message if no connection, but keep running the code.

```{r}
try(con <- auxfunctions::connect_to_secpr())
```

Define a parameter for a function to fetch participant address data from the database

```{r}
db_participants_address_fun <-
  function(db_participants_address) {
    # browser() # Commented out browser function for debugging
    return(dbGetQuery(con,
                      db_participants_address))
  }
```


Fetch or load participant address data, clean it, and prepare for analysis, using the parameters.

Read the file with db_participants_address if exists, 

load from the Oracle database if not,

remove empty columns,

change column names the same way as everything else.


```{r}
db_participants_address <-
  auxfunctions::read_rds_or_run(
    db_participants_address_file_path,
    db_participants_address_query,
    db_participants_address_fun,
    #' If you want to update the existing file, change the NULL to "yes" 
    force_from_db = NULL
  ) |>
  auxfunctions::remove_empty_cols() |>
  auxfunctions::clean_headers()
```

## Data from the previous results of "egregious violators for investigation" 

```{r Data from the previous results of egregious violators for investigation}
# Data from the previous results of "egregious violators for investigation" ----
# <<<<

# Explanations for the following code:

 my_read_xlsx  <-  function(file_path, sheet_n, start_row = 1) {
  res_df <-
    openxlsx::read.xlsx(
      file_path,
      sheet_n,
      startRow = start_row,
      detectDates = TRUE,
      colNames = TRUE,
      sep.names = "_"
    ) |>
    clean_headers()

  return(res_df)
}

 my_read_xlsx  <-  function(file_path, sheet_n, start_row = 1) {
  res_df <-
    openxlsx::read.xlsx(
      file_path,
      sheet_n,
      startRow = start_row,
      detectDates = TRUE,
      colNames = TRUE,
      sep.names = "_"
    ) |>
    clean_headers()

  return(res_df)
}
# >>>>
```

The following section deals with data from previous "egregious violators for investigation" results

Instructions for retrieving previous results

There are 2 functions,

get_previous_result_from_local_file() assumes you have downloaded the previous results and 

get_previous_result_from_google_drive() gets data directly from Google drive

Run only one of them and save the dataframe in `prev_result` variable. 

Function to retrieve and process previous results from a downloaded file

```{r}
get_previous_result_from_local_file <- function() {
  
  # Download first as .xlsx from Google drive
  
  # Read,
  # remove empty columns,
  # change column names the same way as everything else.
  prev_result0 <-
    auxfunctions::my_read_xlsx(prev_result_path) |>
    auxfunctions::remove_empty_cols() |>
    auxfunctions::clean_headers()
  
  # An example
  dim(prev_result0)
  # [1] 151  42
  
  # clean excel number conversions, remove ".0" at the end
  prev_result <-
    prev_result0 |>
    dplyr::mutate(vessel_official_number =
                    stringr::str_replace(vessel_official_number, "\\.0$", ""))
  
  return(prev_result)
}
```

Function to retrieve and process previous results directly from Google Drive

```{r}
get_previous_result_from_google_drive <- function() {
  
  previous_result_google_ss_name <-
    # Takes the base name of the file from `prev_result_path` (removes directory path)
    basename(prev_result_path) |>
    # Removes the file extension from the base name
    tools::file_path_sans_ext()
  
  # When asked for the authentication the first time choose the appropriate option and follow the instructions. If you are writing again in the same R session you can choose the option 2 and it will confirm your access automatically.
  # We assuming that there is only one file with that name in your Google drive.
  
  my_previous_ss <- googlesheets4::gs4_find(previous_result_google_ss_name, n_max = 1)
  
  # Load it to R.
  # And clean it as usual, changing headers to lower case with underscores and removing empty columns
  previous_result <-
    googlesheets4::read_sheet(my_previous_ss) |>
    auxfunctions::remove_empty_cols() |>
    auxfunctions::clean_headers()
  
  return(previous_result)
}
```

Un-comment and run one of the functions.


```{r}
# prev_result <- get_previous_result_from_local_file()
# or
prev_result <- get_previous_result_from_google_drive()
```

## Results 

```{r Results}
# Results ----
# <<<<

# Explanations for the following code:

 pretty_print  <-  function(my_text, my_title,
                         the_end = "---") {
  # Print out to console
  title_message_print(my_title)
  cat(c(my_text, the_end),
      sep = "\n")
}

 pretty_print  <-  function(my_text, my_title,
                         the_end = "---") {
  # Print out to console
  title_message_print(my_title)
  cat(c(my_text, the_end),
      sep = "\n")
}
# >>>>
```

Create a vector of data frame names containing the results


```{r}
results <-
  c(
    "compl_clean",
    "corresp_contact_cnts_clean0",
    "processed_metrics_tracking_permits",
    "fhier_addresses",
    "processed_pims_home_ports",
    "db_participants_address",
    "prev_result"
  )

auxfunctions::pretty_print(results, "Data are in:")
```


Data are in:

compl_clean

corresp_contact_cnts_clean0

processed_metrics_tracking_permits

fhier_addresses

processed_pims_home_ports

db_participants_address

prev_result

## Preparing compliance info 

```{r Preparing compliance info}
# Preparing compliance info ----
```

### Permit Expiration 

```{r Permit Expiration}
## Permit Expiration ----
```

#### Add permit_expired column 

```{r Add permit_expired column}
### Add permit_expired column ----
```

Explainations for the following code:

1. Add a new column 'permit_expired' using 'mutate'.

2. Use 'case_when' to determine if 'permit_groupexpiration' is greater than permit_expired_check_date defined earlier.

3. If true, set 'permit_expired' to "no", otherwise set it to "yes".


```{r}
compl_clean_w_permit_exp <-
  compl_clean |>
  # if permit group expiration is after permit_expired_check_date than "not expired"
  dplyr::mutate(permit_expired =
           dplyr::case_when(
             permit_groupexpiration > permit_expired_check_date ~ "no",
             .default = "yes"
           ))

# check
# glimpse(compl_clean_w_permit_exp)
```

#### Get only not expired last 27 weeks of data minus grace period (total 26 weeks) 

```{r Get only not expired last 27 weeks of data minus grace period total 26 weeks}
### Get only not expired last 27 weeks of data minus grace period (total 26 weeks) ----
```


27 weeks are used to account for a one-week grace period, resulting in 26 weeks of usable data


```{r}
compl_clean_w_permit_exp__not_exp <-
  compl_clean_w_permit_exp |>
  # the last 27 week
  dplyr::filter(week_start > half_year_ago) |>
  # before the last week (a report's grace period)
  dplyr::filter(week_end < last_week_start) |>
  # not expired
  dplyr::filter(tolower(permit_expired) == "no")
```


Check if the dates make sense

```{r}
min(compl_clean_w_permit_exp__not_exp$permit_groupexpiration)
# E.g.
# [1] "2024-08-31 EDT"

min(compl_clean_w_permit_exp__not_exp$week_start)
# E.g.
# [1] "2024-01-08"

max(compl_clean_w_permit_exp__not_exp$week_start)
# [1] "2024-06-17"

max(compl_clean_w_permit_exp__not_exp$week_end)
# [1] "2024-06-23"
```

#### Add year_month column from week_start 

```{r Add year_month column from week_start}
### Add year_month column from week_start ----

# as.yearmon converts dates to a year-month format (e.g., "Jan 2024")
compl_clean_w_permit_exp_last_half_year <-
  compl_clean_w_permit_exp__not_exp |>
  dplyr::mutate(year_month = as.yearmon(week_start)) |>
  # keep entries for the current check period only
  dplyr::filter(year_month >= as.yearmon(half_year_ago))
```


Compare dimensions to verify the filtering has reduced the dataset as expected

```{r}
dim(compl_clean_w_permit_exp)
# [1] 221081     21

dim(compl_clean_w_permit_exp_last_half_year)
# [1] 57296    22
```

#### Have only SA and dual permits 

```{r Have only SA and dual permits}
### Have only SA and dual permits ----
```

This section filters the data to include only SA and dual permits.

Filter rows where 'permitgroup' contains "CDW", "CHS", or "SC"


```{r}
compl_clean_w_permit_exp_last_half_year__sa <-
  compl_clean_w_permit_exp_last_half_year |>
  dplyr::filter(grepl("CDW|CHS|SC", permitgroup))

# Check the dimensions of the resulting dataframe
dim(compl_clean_w_permit_exp_last_half_year__sa)
# [1] 38761    22
```

#### Keep fewer columns in compliance df 

```{r Keep fewer columns in compliance df}
### Keep fewer columns in compliance df ----
```

Define a vector of column names to be removed from the compliance dataframe

```{r}
remove_columns_from_compliance <- c(
  "name",
  "gom_permitteddeclarations__",
  "captainreports__",
  "negativereports__",
  "complianceerrors__",
  "set_permits_on_hold_",
  "override_date",
  "override_by",
  "contactedwithin_48_hours_",
  "submittedpower_down_",
  "permit_expired"
)
```


Remove specified columns and keep only unique rows


```{r}
compl_clean_w_permit_exp_last_half_year__sa__short <-
  compl_clean_w_permit_exp_last_half_year__sa |>
  dplyr::select(-tidyselect::any_of(remove_columns_from_compliance)) |> 
  dplyr::distinct()
```

Check the dimensions of the resulting dataframe

```{r}
dim(compl_clean_w_permit_exp_last_half_year__sa__short)
# [1] 38761    11
```


Work with the whole period

#### Add compliant_after_overr 

```{r Add compliant_after_overr}
### Add compliant_after_overr ----
# <<<<

# Explanations for the following code:

 add_compliant_after_override  <-  function(my_compl_df,
           overridden_col_name = "overridden",
           compliance_col_name = "is_comp") {
  # browser()
  res <-
    my_compl_df |>
    dplyr::rowwise() |>
    dplyr::mutate(
      compliant_after_override =
        dplyr::case_when(
          !!sym(compliance_col_name) %in% c(0, "NO") &
            !!sym(overridden_col_name) %in% c(0, "NO")  ~ "no",
          !!sym(compliance_col_name) %in% c(1, "YES") ~ "yes",
          !!sym(overridden_col_name) %in% c(1, "YES") ~ "yes",
          is.na(!!sym(compliance_col_name)) ~ NA,
          .default = toString(!!sym(compliance_col_name))
        )
    ) |>
    ungroup()

  return(res)
}

 add_compliant_after_override  <-  function(my_compl_df,
           overridden_col_name = "overridden",
           compliance_col_name = "is_comp") {
  # browser()
  res <-
    my_compl_df |>
    dplyr::rowwise() |>
    dplyr::mutate(
      compliant_after_override =
        dplyr::case_when(
          !!sym(compliance_col_name) %in% c(0, "NO") &
            !!sym(overridden_col_name) %in% c(0, "NO")  ~ "no",
          !!sym(compliance_col_name) %in% c(1, "YES") ~ "yes",
          !!sym(overridden_col_name) %in% c(1, "YES") ~ "yes",
          is.na(!!sym(compliance_col_name)) ~ NA,
          .default = toString(!!sym(compliance_col_name))
        )
    ) |>
    ungroup()

  return(res)
}
# >>>>
```

use tictoc package for benchmarking 

Apply the add_compliant_after_override function to add a new column indicating compliance status after overrides


```{r}
tictoc::tic("compl_overr")
compl_clean_w_permit_exp_last_half_year__sa__short__comp_after_overr <-
  compl_clean_w_permit_exp_last_half_year__sa__short |>
  auxfunctions::add_compliant_after_override(overridden_col_name = "overridden_",
                                             compliance_col_name = "compliant_")
tictoc::toc()
# compl_overr: 8.76 sec elapsed
```


check compliant/overridden combinations' counts

```{r}
compl_clean_w_permit_exp_last_half_year__sa__short__comp_after_overr |> 
  dplyr::select(compliant_, overridden_, compliant_after_override) |>
  dplyr::count(compliant_, overridden_, compliant_after_override)
# E.g.
#   compliant_ overridden_ compliant_after_override     n
#   <chr>      <chr>       <chr>                    <int>
# 1 NO         NO          no                       10768
# 2 NO         YES         yes                        199
# 3 YES        NO          yes                      27794
```


Verify that the compliant_after_override column contains only "yes" and "no" values

```{r}
compl_clean_w_permit_exp_last_half_year__sa__short__comp_after_overr$compliant_after_override |>
  unique() == c("yes", "no")

dim(compl_clean_w_permit_exp_last_half_year__sa__short__comp_after_overr)
# E.g.
# [1] 38761    12
```


Ensure that the number of unique vessels remains the same after data transformations


```{r}
dplyr::n_distinct(compl_clean_w_permit_exp_last_half_year__sa$vessel_official_number) ==
  dplyr::n_distinct(
    compl_clean_w_permit_exp_last_half_year__sa__short__comp_after_overr$vessel_official_number
  )
# T
```

#### Get only non-compliant entries for the past half year 

```{r Get only noncompliant entries for the past half year}
### Get only non-compliant entries for the past half year ----
compl_clean_w_permit_exp_last_half_year__sa_non_c <-
  compl_clean_w_permit_exp_last_half_year__sa__short__comp_after_overr |>
  # not compliant
  dplyr::filter(tolower(compliant_after_override) == "no")

# check
dim(compl_clean_w_permit_exp_last_half_year__sa_non_c)
# E.g.
# [1] 10768    12
```

#### Keep only vessels with info for all weeks in the period 

```{r Keep only vessels with info for all weeks in the period}
### Keep only vessels with info for all weeks in the period ----
```


That should eliminate entries for vessels having permits only a part of the period

Calculate the total number of distinct weeks in the dataset


```{r}
all_weeks_num <-
  compl_clean_w_permit_exp_last_half_year__sa_non_c |>
  dplyr::select(week) |>
  dplyr::distinct() |>
  nrow()
```


Explainations for the following code:

1. Group the data frame by 'vessel_official_number'.

2. Filter the groups based on the condition that the number of distinct weeks is greater than or equal to 'all_weeks_num'.

3. Remove the grouping from the data frame.

4. Exclude the 'week' column from the resulting data frame, we don't need it anymore.


```{r}
compl_clean_w_permit_exp_last_half_year__sa_non_c__all_weeks_present <-
  compl_clean_w_permit_exp_last_half_year__sa_non_c |>
  dplyr::group_by(vessel_official_number) |>
  dplyr::filter(dplyr::n_distinct(week) >= all_weeks_num) |> 
  dplyr::ungroup() |> 
  dplyr::select(-week)
```


Check how many entries were removed

```{r}
dim(compl_clean_w_permit_exp_last_half_year__sa_non_c)
# E.g.
# [1] 10768    12

dim(compl_clean_w_permit_exp_last_half_year__sa_non_c__all_weeks_present)
# E.g.
# [1] 3278   11
```

#### Check the last report date 

```{r Check the last report date}
### Check the last report date ----
```

##### Get ids only 

```{r Get ids only}
#### Get ids only ----
```

Create a dataframe with unique vessel official numbers

```{r}
compl_clean_w_permit_exp_last_half_year__sa_non_c__all_weeks_present__vesl_ids <-
  compl_clean_w_permit_exp_last_half_year__sa_non_c__all_weeks_present |>
  dplyr::select(vessel_official_number) |>
  dplyr::distinct()
```


check vessel's number

```{r}
dim(compl_clean_w_permit_exp_last_half_year__sa_non_c__all_weeks_present__vesl_ids)
# E.g.
# 149 
```

##### Check these ids in the full compliance information 

```{r Check these ids in the full compliance information}
#### Check these ids in the full compliance information ----
```

Check if there are any new submitted reports for previously non-compliant vessels


```{r}
compl_clean_w_permit_exp_last_half_year__sa |>
  dplyr::filter(
    vessel_official_number %in% compl_clean_w_permit_exp_last_half_year__sa_non_c__all_weeks_present__vesl_ids$vessel_official_number
  ) |>
  dplyr::group_by(vessel_official_number) |>
  dplyr::filter(
    tolower(compliant_) == "yes" &
      tolower(overridden_) == "yes" &
      # not the current month
      year_month < as.yearmon(data_file_date)
  ) |>
  nrow()
```

A result of 0 indicates no new compliant reports have been submitted for selected vessels

End of Compliance preparations 

Results: processed Compliance is in `compl_clean_w_permit_exp_last_half_year__sa_non_c__all_weeks_present`

## Preparing Correspondence 

```{r Preparing Correspondence}
# Preparing Correspondence ----
```

### Remove 999999 

```{r Remove 999999}
## Remove 999999 ----
```


Remove vessels with official numbers starting with "99999" as they are placeholder or test entries


```{r}
corresp_contact_cnts_clean <-
  corresp_contact_cnts_clean0 |>
  dplyr::filter(!grepl("^99999", vessel_official_number))
```


check number of vessels in correspondence 

```{r}
dplyr::n_distinct(corresp_contact_cnts_clean$vesselofficial_number)
```

E.g.
4281

check

```{r}
corresp_contact_cnts_clean |>
  dplyr::select(calltype, voicemail, contacttype) |>
  dplyr::distinct() |> head(10)
```

### Correspondence Filters 

```{r Correspondence Filters}
## Correspondence Filters ----
```


This section defines various filters for correspondence data using quosures.

Quosures are a part of tidy evaluation in R, allowing expressions to be captured without evaluation, which is useful for creating functions with flexible inputs.

Define a filter to check if any entry was an outgoing call


```{r}
we_called_filter <-
  dplyr::quo(any(tolower(contacttype) == "call" &
        tolower(calltype) == "outgoing"))
```


Define a filter to check if any entry was an outgoing email or other contact type


```{r}
we_emailed_once_filter <-
  dplyr::quo(any(
    tolower(contacttype) %in% c("email", "other") &
      tolower(calltype) == "outgoing"
  ))
```


Explainations for the following code:

**Expression inside quo()**:

   - `!grepl("No contact made", contactcomments, ignore.case = TRUE)`: This expression is a negation of the `grepl` function, which is used to search for a pattern ("No contact made") in the `contactcomments` column.

   - `grepl()` returns `TRUE` for each element of `contactcomments` that contains the pattern, and `FALSE` otherwise.

   - The `!` operator negates the result, so the filter condition will be `TRUE` for rows where "No contact made" is not found in the `contactcomments` column.

The `exclude_no_contact_made_filter` function creates a filter condition to exclude rows where "No contact made" is found in the `contactcomments` column.


```{r}
exclude_no_contact_made_filter <-
  dplyr::quo(!grepl("No contact made", 
            contactcomments, 
            ignore.case = TRUE))
```


don't need a second contact

Define a filter to check if any contact was incoming


```{r}
they_contacted_direct_filter <-
  dplyr::quo(
    any(
      tolower(calltype) == "incoming"
      )
  )
```

#### Use the filters 

```{r Use the filters}
### Use the filters ----
```

Apply filters to create a subset of correspondence data meeting specific criteria

Explainations for the following code:

- `corresp_contact_cnts_clean |> ...` starts the pipeline with the data frame `corresp_contact_cnts_clean`.

- `dplyr::filter(tolower(calltype) == "incoming" | ...)` filters rows based on the conditions specified:

  - `dplyr::filter()` is used to subset rows in the data frame.

  - `tolower(calltype) == "incoming"` converts the `calltype` column to lowercase and checks if it equals "incoming".

  - The `|` operator means logical OR, so it includes rows where the condition on either side is true.

  - `(contact_freq > 1 & (!!we_called_filter & !!we_emailed_once_filter))`:

    - `contact_freq > 1` checks if the `contact_freq` column is greater than 1.

    - `&` is the logical AND operator, so it requires both conditions to be true.

    - `!!we_called_filter` evaluates the `we_called_filter` variable as a logical expression.

    - `!!we_emailed_once_filter` evaluates the `we_emailed_once_filter` variable as a logical expression.

    - The parentheses around this subexpression ensure it is evaluated as a single logical unit.

- `dplyr::filter(!!exclude_no_contact_made_filter)` applies another filter condition:

  - `!!exclude_no_contact_made_filter` evaluates the `exclude_no_contact_made_filter` variable as a logical expression.

This code filters the `corresp_contact_cnts_clean` data frame to include rows where either the `calltype` is "incoming" or the `contact_freq` is greater than 1 and both `we_called_filter` and `we_emailed_once_filter` are true. Additionally, it filters rows based on the `exclude_no_contact_made_filter` condition.


```{r}
corresp_contact_cnts_clean_direct_cnt_2atmps <-
  corresp_contact_cnts_clean |>
  dplyr::filter(tolower(calltype) == "incoming" |
           (
             contact_freq > 1 &
               (!!we_called_filter &
                  !!we_emailed_once_filter)
           )) |> 
  dplyr::filter(!!exclude_no_contact_made_filter)
```


Check the dimensions of the original and filtered datasets


```{r}
dim(corresp_contact_cnts_clean)
# E.g. [1] 33001    22
dim(corresp_contact_cnts_clean_direct_cnt_2atmps)
# E.g. [1] 31061    22
```


Check how many vessels left after filtering 

```{r}
dplyr::n_distinct(corresp_contact_cnts_clean_direct_cnt_2atmps$vesselofficial_number)
# E.g.
# [1] 3865
```

### Fix dates 

```{r Fix dates}
## Fix dates ----
```

Prepare to clean and standardize date formats

check how the dates look

```{r}
head(corresp_contact_cnts_clean_direct_cnt_2atmps$contact_date, 1) |> str()
# chr "02/15/2024 03:15PM"
```



Explainations for the following code:

Mutate new columns 'created_on_dttm' and 'contact_date_dttm' by parsing 'created_on' and 'contact_date' using lubridate package.

The date-time formats considered are "mdY R".

1. Use the pipe operator to pass 'corresp_contact_cnts_clean_direct_cnt_2atmps' as the left-hand side of the next expression.

2. Use 'mutate' to create new columns with parsed date-time values.

3. Use 'lubridate::parse_date_time' to parse the date-time values using the specified formats.


```{r}
corresp_contact_cnts_clean_direct_cnt_2atmps_clean_dates <-
  corresp_contact_cnts_clean_direct_cnt_2atmps |>
  dplyr::mutate(
    created_on_dttm =
      lubridate::parse_date_time(created_on,
                                 c("mdY R")),
    contact_date_dttm =
      lubridate::parse_date_time(contact_date,
                                 c("mdY R"))
  )
```


check how the dates look now

```{r}
head(corresp_contact_cnts_clean_direct_cnt_2atmps_clean_dates$contact_date_dttm, 1) |> 
  str()
# E.g.
# POSIXct[1:1], format: "2024-06-17 15:24:00"
```


End of Correspondence preparations 

Processed Correspondence is in 
`corresp_contact_cnts_clean_direct_cnt_2atmps_clean_dates`

## Join correspondence with compliance 

```{r Join correspondence with compliance}
# Join correspondence with compliance ----
```


Combine correspondence and compliance data for further analysis.

An inner join is used to combine correspondence and compliance data.

This ensures we only keep vessels that appear in both datasets, effectively filtering for vessels with both compliance and correspondence records

Explainations for the following code:

Create a new dataframe 'compl_corr_to_investigation' by performing an inner join between

'correspondence' and 'compliance'.

The join is performed on the column 'vessel_official_number'.

Use 'multiple = "all"' and 'relationship = "many-to-many"' to handle multiple matches during the join.

1. Use the 'inner_join' function from the dplyr package to combine the two dataframes based on the specified columns.

2. Pass the column names and other parameters to the 'by', 'multiple', and 'relationship' arguments.


```{r}
compl_corr_to_investigation <-
  dplyr::inner_join(
    corresp_contact_cnts_clean_direct_cnt_2atmps_clean_dates,
    compl_clean_w_permit_exp_last_half_year__sa_non_c__all_weeks_present,
    by = c("vessel_official_number"),
    multiple = "all",
    relationship = "many-to-many"
  )
```


Verify the dimensions and content of the joined dataset


```{r}
dim(compl_corr_to_investigation)
# E.g.
# [1] 30844    32

dplyr::n_distinct(compl_corr_to_investigation$vesselofficial_number)
# E.g.
# 141

head(compl_corr_to_investigation) |> 
  glimpse()
```

### Store the count of unique vessels 

```{r Store the count of unique vessels}
## Store the count of unique vessels ----
```

For later verification and reporting

```{r}
num_of_vsl_to_investigate <- 
  dplyr::n_distinct(compl_corr_to_investigation$vesselofficial_number)
```


Results: Compliance & Correspondence joined together are in
`compl_corr_to_investigation`

## Output needed investigation 

```{r Output needed investigation}
# Output needed investigation ----
```

# Prepare output

"Investigation" in this context refers to vessels that meet the criteria for egregious violations and require further action.

Steps:

1. Remove unused columns.

2. Create additional columns.

3. Mark vessels already in the know list (prev_result).

4. Duals vs. sa_only

### 1. Remove extra columns 

```{r 1 Remove extra columns}
## 1. Remove extra columns ----
```

List of columns to be excluded from the final output for simplification and focus on relevant data. Commented are column names to retain.


```{r}
unused_fields <- c(
  "vesselofficial_number",
  "primary",
  # "contact_date",
  "follow_up",
  "log_group",
  "calltype",
  "voicemail",
  # "contacttype",
  "contact_reason",
  # "contactrecipientname",
  # "contactphone_number",
  # "contactemailaddress",
  "contactcomments",
  "srfhuser",
  "created_on",
  "follow_up_nbr",
  "srhs_vessel",
  # "vessel_official_number",
  "was_contacted",
  "contact_freq",
  "created_on_dttm",
  # "contact_date_dttm",
  # "name",
  # "permit_expired",
  # "permitgroup",
  # "permit_groupexpiration",
  "compliant_after_override")
```



Create a simplified version of the investigation data, removing unused fields and concatenating unique values for each vessel

Explainations for the following code:

1. Exclude columns specified in 'unused_fields' from the data frame.

2. Group the data frame by 'vessel_official_number'.

3. `summarise_all` applies the function 'concat_unique' to all columns to concatenate unique non-missing values into a single string.

4. Remove the grouping from the data frame.


```{r}
compl_corr_to_investigation_short <-
  compl_corr_to_investigation |>
  dplyr::select(-any_of(unused_fields)) |>
  dplyr::group_by(vessel_official_number) |>
  dplyr::summarise_all(auxfunctions::concat_unique) |>
  dplyr::ungroup()
```


Visual check if data make sense

```{r}
compl_corr_to_investigation_short |> 
  head() |> 
  dplyr::glimpse() |>
  knitr::kable(caption = "")
```


Check if number of vessels didn't change

```{r}
nrow(compl_corr_to_investigation_short) == num_of_vsl_to_investigate
```

### 2. Create additional columns 

```{r 2 Create additional columns}
## 2. Create additional columns ----
```

#### Add list of contact dates and contact type in parentheses  

```{r Add list of contact dates and contact type in parentheses}
### Add list of contact dates and contact type in parentheses  ----
# <<<<

# Explanations for the following code:

 find_col_name  <-  function(mydf, start_part, end_part) {
  # Create a regular expression pattern to search for column names that start with 'start_part'
  # and end with 'end_part'.
  to_search <- paste0(start_part, ".*", end_part)

  # Use 'grep' to search for column names in lowercase that match the pattern.
  # 'value = TRUE' returns the matching column names as a character vector.
  matching_names <- grep(to_search, tolower(names(mydf)), value = TRUE)

  # Return the matching column name(s) as a character vector.
  return(matching_names)
}

 find_col_name  <-  function(mydf, start_part, end_part) {
  # Create a regular expression pattern to search for column names that start with 'start_part'
  # and end with 'end_part'.
  to_search <- paste0(start_part, ".*", end_part)

  # Use 'grep' to search for column names in lowercase that match the pattern.
  # 'value = TRUE' returns the matching column names as a character vector.
  matching_names <- grep(to_search, tolower(names(mydf)), value = TRUE)

  # Return the matching column name(s) as a character vector.
  return(matching_names)
}
# >>>>
```


Define column name variables for flexibility across different data sources.

Spaces and underscores placements vary from source to source.


```{r}
contactdate_field_name <-
  auxfunctions::find_col_name(compl_corr_to_investigation_short, "contact", "date")[1]

contacttype_field_name <-
  auxfunctions::find_col_name(compl_corr_to_investigation_short, "contact", "type")[1]

contactphonenumber_field_name <-
  auxfunctions::find_col_name(compl_corr_to_investigation_short, ".*contact", "number.*")[1]
```


 Function to create a summary of contact dates and types for each vessel
 
Explainations for the following code:

Define a function 'get_date_contacttype' that takes a dataframe 'compl_corr_to_investigation' as input.

1. Add a new column 'date__contacttype' by concatenating the values from 'contactdate_field_name' and 'contacttype'.

2. Select only the 'vessel_official_number' and 'date__contacttype' columns.

3. Arrange the dataframe by 'vessel_official_number' and 'date__contacttype'.

4. Keep distinct rows based on 'vessel_official_number' and 'date__contacttype'.

5. Group the dataframe by 'vessel_official_number'.

6. Summarize the data by creating a new column 'date__contacttypes' that concatenates all 'date__contacttype' values for each vessel separated by a comma.

7. Return the resulting dataframe.


```{r}
get_date_contacttype <-
  function(my_df) {
  
    res <-
      my_df |>
      # Combine contact date and type into a single column

      dplyr::mutate(date__contacttype =
                      paste(
                        !!rlang::sym(contactdate_field_name),
                        !!rlang::sym(contacttype_field_name)
                      )) |>
      # use 2 columns only
      dplyr::select(vessel_official_number, date__contacttype) |>
      dplyr::distinct() |>
      # sort
      dplyr::arrange(vessel_official_number, date__contacttype) |>
      # for each vessel id...
      dplyr::group_by(vessel_official_number) |>
      # ...combine all date__contacttypes separated by comma in one cell
      dplyr::summarise(date__contacttypes =
                         paste(date__contacttype, collapse = ", ")) |> 
      dplyr::ungroup()
    
    return(res)
  }
```


Apply the get_date_contacttype function

```{r}
date__contacttype_per_id <-
  get_date_contacttype(compl_corr_to_investigation_short)
```


Verify that the number of vessels remains consistent

```{r}
nrow(date__contacttype_per_id) == num_of_vsl_to_investigate
```


Display a sample of the resulting data for verification

```{r}
date__contacttype_per_id |>
  head() |>
  dplyr::glimpse() |>
  knitr::kable(caption = "")
```

##### Add the new column back 

```{r Add the new column back}
#### Add the new column back ----
# Join the short compliance/correspondence data with date and contact type information
compl_corr_to_investigation__corr_date <-
  dplyr::left_join(compl_corr_to_investigation_short,
            date__contacttype_per_id) |>
  # Joining with `by = join_by(vessel_official_number)`
  # these columns are not longer needed
  dplyr::select(-dplyr::all_of(c(
    contactdate_field_name,
    contacttype_field_name
  )))
```

check, the last column should be like 

$ date__contacttypes     <chr> "03/13/2024 11:59AM, 09/21/2023 03:41PM, 08/18/2023 10:52AM,…

```{r}
compl_corr_to_investigation__corr_date |> 
  head() |> 
  dplyr::glimpse() |>
  knitr::kable(caption = "")
```

#### Add pims home port info 

```{r Add pims home port info}
### Add pims home port info ----
```


Rename columns in the processed PIMS home ports data for consistency


```{r}
processed_pims_home_ports_renamed <- 
  processed_pims_home_ports |> 
  dplyr::rename("hailing_port_city" = city_fixed,
         "hailing_port_state" = state_fixed)
```


Combine compliance/correspondence data with hailing port information


```{r}
compl_corr_to_investigation__corr_date__hailing_port <- 
  dplyr::left_join(
    compl_corr_to_investigation__corr_date,
    processed_pims_home_ports_renamed,
    dplyr::join_by(vessel_official_number)
  )
```

#### Add prepared addresses 

```{r Add prepared addresses}
### Add prepared addresses ----

# Define the path to the address preparation script
# This is used only with source()
prep_addresses_path <-
  file.path(current_project_path,
            stringr::str_glue("{current_project_name}_prep_addresses.R"))
```


Check if the file exists.

```{r}
file.exists(prep_addresses_path)
```

Check and add missing addresses

This part aims to gather and clean up address information for vessels from two main sources: FHIER and an Oracle database. It then combines this information to create a more complete set of address data for each vessel.

The main output is a dataset called `compl_corr_to_investigation__corr_date__hailing_port__fhier_addr__db_addr`, which combines vessel information with address data from both FHIER and the Oracle database.

The code starts by cleaning and simplifying the FHIER address data, keeping only the most relevant information. It then retrieves additional address data from an Oracle database to supplement the FHIER data.

It identifies vessels that are missing address information in the FHIER data.

It then retrieves additional address information for these vessels from the Oracle database.

The code cleans and processes this Oracle database information, combining multiple entries for the same vessel when necessary.

Finally, it joins all this information together, creating a more complete dataset with address information from both sources.

## Addresses from FHIER 

```{r Addresses from FHIER}
# Addresses from FHIER ----
```

### Fewer fields 

```{r Fewer fields}
## Fewer fields ----
# <<<<

# Explanations for the following code:

 clean_names_and_addresses  <-  function(my_df) {
    my_df_cleaned <-
      my_df |>
      dplyr::mutate(
        dplyr::across(dplyr::where(is.character), ~ stringr::str_squish(.x)),
        dplyr::across(dplyr::where(is.character), ~ tidyr::replace_na(.x, "")),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, ", ;", ";")
        ),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, "\\s+[,;]", ",")
        ),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, ";,+", ";")
        ),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, ";;+", ";")
        ),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, ",,+", ",")
        ),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, "[,;] *\\bUN\\b *", "")
        ),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, "\\bUN\\b", "")
        ),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, "\\s*\\bUN\\b\\s*", "")
        ),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, "^[,;] ", "")
        ),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, "^[,;]$", "")
        ),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, "[,;]$", "")
        ),
        dplyr::across(dplyr::where(is.character), 
                      ~ stringr::str_squish(.x))
      )

  return(my_df_cleaned)
}

 clean_names_and_addresses  <-  function(my_df) {
    my_df_cleaned <-
      my_df |>
      dplyr::mutate(
        dplyr::across(dplyr::where(is.character), ~ stringr::str_squish(.x)),
        dplyr::across(dplyr::where(is.character), ~ tidyr::replace_na(.x, "")),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, ", ;", ";")
        ),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, "\\s+[,;]", ",")
        ),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, ";,+", ";")
        ),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, ";;+", ";")
        ),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, ",,+", ",")
        ),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, "[,;] *\\bUN\\b *", "")
        ),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, "\\bUN\\b", "")
        ),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, "\\s*\\bUN\\b\\s*", "")
        ),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, "^[,;] ", "")
        ),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, "^[,;]$", "")
        ),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, "[,;]$", "")
        ),
        dplyr::across(dplyr::where(is.character), 
                      ~ stringr::str_squish(.x))
      )

  return(my_df_cleaned)
}
# >>>>
```

fhier_addresses are from get_data (For-hire Primary Physical Address List)

Select relevant columns from FHIER addresses data


```{r}
fhier_addr_short <-
  fhier_addresses |>
  dplyr::select(
    vessel_official_number,
    permit_holder_names,
    physical_address_1,
    physical_address_2,
    physical_city,
    physical_county,
    physical_state,
    physical_zip_code,
    phone_number,
    primary_email
  )
```

Clean and standardize FHIER address data, removing duplicates

```{r}
fhier_addr_short_clean <-
  fhier_addr_short |>
  auxfunctions::clean_names_and_addresses() |>
  dplyr::distinct()

# nrow(fhier_addr_short_clean)
```

Address combination code is commented out to preserve individual address fields

```{r}
# fhier_addr_short__comb_addr <-
#   fhier_addr_short |>
#   clean_names_and_addresses() |>
#   mutate(
#     fhier_address =
#       str_glue(
#         "
#         {physical_address_1}, {physical_address_2}, {physical_city}, {physical_county}, {physical_state}, {physical_zip_code}
#       "
#       )
#   ) |>
#   select(
#     -c(
#       physical_address_1,
#       physical_address_2,
#       physical_city,
#       physical_county,
#       physical_state,
#       physical_zip_code
#     )
#   ) |>
#   clean_names_and_addresses() |>
#   distinct()

# dim(fhier_addr_short__comb_addr)
# [1] 2390    5

# dim(fhier_addr_short_clean)
```

### Add addresses from FHIER 

```{r Add addresses from FHIER}
## Add addresses from FHIER ----
```

Join FHIER address data with the existing dataset

```{r}
compl_corr_to_investigation__corr_date__hailing_port__fhier_addr <-
  left_join(compl_corr_to_investigation__corr_date__hailing_port,
            fhier_addr_short_clean)
```

Joining with `by = join_by(vessel_official_number)`
Result: Dataset now includes FHIER address information for each vessel


```{r}
# View(compl_corr_to_investigation__corr_date__hailing_port__fhier_addr)
```

Verify completeness of contact information


```{r}
compl_corr_to_investigation__corr_date__hailing_port__fhier_addr |>
  filter(
    is.na(contactrecipientname) |
      is.na(contactphone_number) |
      is.na(contactemailaddress)
  ) |> nrow()
# 0
```

### Vessels with no addresses 

```{r Vessels with no addresses}
## Vessels with no addresses ----
# <<<<

# Explanations for the following code:

 print_df_names  <-  function(my_df, names_num = 100) {
  # Use 'names' to get column names,
  # 'head' to limit the number of names to 'names_num',
  # 'paste0' to concatenate them with a comma separator, and return the result.
  names(my_df) %>%
    head(names_num) %>%
    paste0(collapse = ", ") %>%
    return()
}

 print_df_names  <-  function(my_df, names_num = 100) {
  # Use 'names' to get column names,
  # 'head' to limit the number of names to 'names_num',
  # 'paste0' to concatenate them with a comma separator, and return the result.
  names(my_df) %>%
    head(names_num) %>%
    paste0(collapse = ", ") %>%
    return()
}
# >>>>


# print_df_names(compl_corr_to_investigation__corr_date__hailing_port__fhier_addr)
```

Explanation:

This code snippet creates a dataframe `no_addr_vsl_ids` containing unique `vessel_official_number` values based on certain conditions.

1. **Starting with the DataFrame:**

   - `compl_corr_to_investigation__corr_date__hailing_port__fhier_addr |>`: Pipes the dataframe `compl_corr_to_investigation__corr_date__hailing_port__fhier_addr` into the next function.

2. **Filtering Rows:**

   - `dplyr::filter(physical_address_1 %in% is_empty)`: Uses the `filter` function from the `dplyr` package to keep rows where the `physical_address_1` column is empty.

     - `physical_address_1 %in% is_empty`: This condition checks if the values in the `physical_address_1` column are empty.

3. **Selecting Columns:**

   - `dplyr::select(vessel_official_number)`: Selects only the `vessel_official_number` column from the filtered dataframe.

4. **Removing Duplicate Rows:**

   - `dplyr::distinct()`: Removes duplicate rows from the dataframe, ensuring that each `vessel_official_number` appears only once in the final result.

The resulting `no_addr_vsl_ids` dataframe contains unique `vessel_official_number` values where the corresponding `physical_address_1` column is empty in the `compl_corr_to_investigation__corr_date__hailing_port__fhier_addr` dataframe.

Define a vector of values considered as empty

```{r}
is_empty <- c(NA, "NA", "", "UN", "N/A")

# Create a dataframe of unique vessel official numbers with empty physical addresses
no_addr_vsl_ids <-
  compl_corr_to_investigation__corr_date__hailing_port__fhier_addr |>
  dplyr::filter(physical_address_1 %in% is_empty) |>
  dplyr::select(vessel_official_number) |>
  dplyr::distinct()

# Count the number of unique vessel official numbers with empty addresses
dplyr::n_distinct(no_addr_vsl_ids$vessel_official_number)
# 109
# 71
```

## Addresses from Oracle db 

```{r Addresses from Oracle db}
# Addresses from Oracle db ----
```

Explanation:

This code snippet processes the `db_participants_address` dataframe by filtering rows based on the `official_number` and ensuring distinct rows in the resulting dataframe.

1. **Starting with the DataFrame:**

   - `db_participants_address |>`: Starts with the `db_participants_address` dataframe and pipes it into the next function.


2. **Filtering Rows:**

   - `dplyr::filter(official_number %in% no_addr_vsl_ids$vessel_official_number)`:

     - `dplyr::filter(...)`: The `filter` function from the `dplyr` package is used to keep rows that meet certain conditions.

     - `official_number %in% no_addr_vsl_ids$vessel_official_number`: This condition keeps only the rows where the `official_number` is found in the `vessel_official_number` column of the `no_addr_vsl_ids` dataframe.

       - `%in%`: The `%in%` operator checks if elements of `official_number` are present in `no_addr_vsl_ids$vessel_official_number`.

3. **Removing Duplicate Rows:**

   - `dplyr::distinct()`: The `distinct` function from the `dplyr` package removes duplicate rows from the filtered dataframe, ensuring each row is unique.

The result is a new dataframe `db_participants_address__needed` that contains only the rows from `db_participants_address` where the `official_number` is present in the `no_addr_vsl_ids$vessel_official_number` column, and all duplicate rows are removed.


```{r}
db_participants_address__needed <-
  db_participants_address |>
  dplyr::filter(official_number %in% no_addr_vsl_ids$vessel_official_number) |>
  dplyr::distinct()

dim(db_participants_address__needed)
# [1] 139  37

dplyr::n_distinct(db_participants_address__needed$official_number)
# 71
```

### Keep fewer columns in db_participants_address__needed 

```{r Keep fewer columns in db_participants_address__needed}
## Keep fewer columns in db_participants_address__needed ----
```

Define a vector of column names to keep in the final dataframe

```{r}
col_names_to_keep <-
  c(
    "official_number",
    "entity_name",
    "primary_email",
    # "is_primary",
    "ph_area",
    "ph_number",
    "entity_name",
    "physical_city",
    "physical_county",
    "physical_state",
    "physical_zip_code",
    "mailing_address1",
    "mailing_address2",
    "mailing_city",
    "mailing_county",
    "mailing_country",
    "mailing_state",
    "mailing_zip_code"
  )
```

Explanation:

This code snippet processes the `db_participants_address__needed` dataframe by selecting specific columns, removing duplicate rows, and arranging the rows based on the `official_number` column.

1. **Creating a Regular Expression Pattern for Column Names:**

   - `my_cols_ends <- paste0(col_names_to_keep, '$', collapse = '|')`:

     - `col_names_to_keep`: This variable contains a list or vector of column name prefixes you want to keep.

     - `paste0(...)`: This function concatenates the elements of `col_names_to_keep` with a `$` at the end of each element, creating a regular expression pattern to match column names that end with any of the specified prefixes.

     - `collapse = '|'`: The `collapse` parameter ensures that the elements are joined by a `|`, which is the OR operator in regular expressions.

2. **Selecting Specific Columns:**

   - `db_participants_address__needed |>`: Starts with the `db_participants_address__needed` dataframe and pipes it into the next function.

   - `dplyr::select(tidyselect::matches(my_cols_ends))`: Uses the `select` function from `dplyr` and the `matches` function from `tidyselect` to select columns whose names match the regular expression pattern stored in `my_cols_ends`.

3. **Removing Duplicate Rows:**

   - `dplyr::distinct()`: Removes duplicate rows from the selected columns.

The result is a new dataframe `db_participants_address__needed_short1` that contains only the columns matching the specified pattern, with duplicates removed.

Create a regular expression pattern to match column names ending with the specified prefixes


```{r}
my_cols_ends <- paste0(col_names_to_keep,
                  '$',
                  collapse = '|')

# Create a new dataframe with selected columns and remove duplicates
db_participants_address__needed_short <-
  db_participants_address__needed |>
  dplyr::select(tidyselect::matches(my_cols_ends)) |>
  dplyr::distinct()
```

Verify the number of rows in the final dataset

```{r}
nrow(compl_corr_to_investigation__corr_date__hailing_port__fhier_addr)
# 199
```

Confirm unique vessel official numbers is the same

```{r}
dplyr::n_distinct(compl_corr_to_investigation__corr_date__hailing_port__fhier_addr$vessel_official_number)
# 199
#
```

one vessel per row, OK

have to combine rows

```{r}
dim(db_participants_address__needed_short)
# 106
dplyr::n_distinct(db_participants_address__needed_short$official_number)
# 71
```

### Combine area and phone numbers 

```{r Combine area and phone numbers}
## Combine area and phone numbers ----
```

Explanation:

This code creates a new dataframe `db_participants_address__needed_short__phone0` by modifying the `db_participants_address__needed_short` dataframe. It adds two new columns (`erv_phone` and `erb_phone`) that concatenate existing columns.

1. **Starting with the Original Dataframe:**

   - `db_participants_address__needed_short |>`: Begins with the `db_participants_address__needed_short` dataframe and pipes it into the next function.

2. **Adding New Columns:**

   - `dplyr::mutate(...)`: The `mutate` function from the `dplyr` package is used to add or modify columns in the dataframe.

     - `erv_phone = paste0(erv_ph_area, erv_ph_number)`: Creates a new column `erv_phone` by concatenating the `erv_ph_area` and `erv_ph_number` columns using `paste0`, which combines strings without any separator.

     - `erb_phone = paste0(erb_ph_area, erb_ph_number)`: Similarly, creates a new column `erb_phone` by concatenating the `erb_ph_area` and `erb_ph_number` columns.

The result is a new dataframe `db_participants_address__needed_short__phone0` that contains all the original columns from `db_participants_address__needed_short` plus two new columns (`erv_phone` and `erb_phone`) that contain concatenated phone numbers.


```{r}
db_participants_address__needed_short__phone0 <-
  db_participants_address__needed_short |>
  dplyr::mutate(erv_phone = paste0(erv_ph_area, erv_ph_number),
         erb_phone = paste0(erb_ph_area, erb_ph_number))
```

### Make erv and erb combinations 

```{r Make erv and erb combinations}
## Make erv and erb combinations ----
# <<<<

# Explanations for the following code:

 list_sort_uniq  <-  function(my_lists) {
  # browser()
  res <-
    my_lists |>
    stringr::str_trim() |>
    unique() |>
    sort() |>
    list() |>
    purrr::flatten()
  return(res)
}

 list_sort_uniq  <-  function(my_lists) {
  # browser()
  res <-
    my_lists |>
    stringr::str_trim() |>
    unique() |>
    sort() |>
    list() |>
    purrr::flatten()
  return(res)
}
# >>>>
```

Define a vector of column name parts to be used for creating new combined columns


```{r}
col_part_names <-
  c(
    "entity_name",
    "primary_email",
    # "ph_is_primary",
    # "ph_area",
    # "ph_number",
    "physical_city",
    "physical_county",
    "physical_state",
    "physical_zip_code",
    "mailing_address1",
    "mailing_address2",
    "mailing_city",
    "mailing_county",
    # "mailing_country",
    "mailing_state",
    "mailing_zip_code"
  )
```

Create a new dataframe that combines ERV and ERB information for each column part

Explanation:

1. **Mapping Over Column Parts:**

   - `col_part_names |> purrr::map(\(curr_col_part) { ... })`: It iterates over each element in `col_part_names` using the `map` function from the purrr package. For each column part (`curr_col_part`), it executes the code inside the curly braces `{ ... }`.

2. **Generating New Column Names:**

   - `new_col_name <- stringr::str_glue("db_{curr_col_part}")`: It creates a new column name by combining the prefix "db_" with the current column part (`curr_col_part`) using `str_glue` from the stringr package.
   
 Use !!new_col_name := to dynamically create new column names based on curr_col_part

3. **Grouping and Mutating Data:**

   - `db_participants_address__needed_short__phone0 |> dplyr::group_by(official_number) |> ...`: It groups the dataframe `db_participants_address__needed_short__phone0` by the column `official_number` using `group_by` from dplyr. Then, it proceeds with further data manipulation operations.

4. **Applying Purrr::pmap Function:**

   - `purrr::pmap(dplyr::across(dplyr::ends_with(curr_col_part)), ...)`: It applies the `pmap` function from the purrr package to iterate over columns that end with the current column part (`curr_col_part`). Within the `pmap` call, a custom function (`auxfunctions::list_sort_uniq`) is applied to each corresponding set of columns.

5. **Ungrouping and Selecting Columns:**

   - `... |> dplyr::ungroup() |> dplyr::select(-official_number)`: After the mutation step, it ungroups the dataframe and removes the `official_number` column using `ungroup()` and `select()` functions from dplyr, respectively.

6. **Binding Columns Together:**

   - `dplyr::bind_cols(db_participants_address__needed_short__phone0, .)`: Finally, it binds the original dataframe `db_participants_address__needed_short__phone0` with the transformed columns obtained from the mapping operation using `bind_cols` from dplyr.

This code dynamically generates new columns in the dataframe based on the provided column parts, applies a custom function to each set of corresponding columns, and then binds the resulting columns back to the original dataframe.


```{r}
tictoc::tic("map all pairs")
db_participants_address__needed_short__erv_erb_combined3 <-
  col_part_names |>
  purrr::map(\(curr_col_part)  {
    new_col_name <- stringr::str_glue("db_{curr_col_part}")
    # cat(new_col_name, sep = "\n")

    db_participants_address__needed_short__phone0 |>
      dplyr::group_by(official_number) |>
      dplyr::mutate(!!new_col_name :=
                      purrr::pmap(dplyr::across(dplyr::ends_with(curr_col_part)),
                                    ~ auxfunctions::list_sort_uniq(.)),
             .keep = "none" ) |>
      dplyr::ungroup() |>
      dplyr::select(-official_number)

  }) %>%
  dplyr::bind_cols(db_participants_address__needed_short__phone0, .)
tictoc::toc()
# map all pairs: 14.31 sec elapsed
```

#### Shorten db_participants_address__needed_short__erv_erb_combined3 

```{r Shorten db_participants_address__needed_short__erv_erb_combined3}
### Shorten db_participants_address__needed_short__erv_erb_combined3 ----
```

Explanation:

Create a shortened version of the dataframe, keeping only the official_number and db_ columns

1. **Dataframe Selection and Transformation:**

   - `db_participants_address__needed_short__erv_erb_combined3 |>`: Starts with the input dataframe `db_participants_address__needed_short__erv_erb_combined3` and pipes it into the subsequent functions.

2. **Selecting Specific Columns:**

   - `dplyr::select(official_number, tidyselect::all_of(tidyselect::starts_with("db_")))`: Uses `dplyr::select` to retain only the `official_number` column and any columns whose names start with "db_".

     - `tidyselect::all_of(tidyselect::starts_with("db_"))`: Uses the `tidyselect` package to identify all column names that start with "db_". The `all_of` function ensures that the selected columns exist within the dataframe.

3. **Ensuring Unique Rows:**
   - `dplyr::distinct()`: Ensures that the resulting dataframe contains only unique rows, removing any duplicate rows based on the selected columns.

The result is a new dataframe `db_participants_address__needed_short__erv_erb_combined_short` that contains only the `official_number` column and columns starting with "db_", with all duplicate rows removed.


```{r}
db_participants_address__needed_short__erv_erb_combined_short <-
  db_participants_address__needed_short__erv_erb_combined3 |>
  dplyr::select(official_number,
                tidyselect::all_of(tidyselect::starts_with("db_"))) |>
  dplyr::distinct()

dim(db_participants_address__needed_short__erv_erb_combined_short)
# 94 17

dplyr::n_distinct(db_participants_address__needed_short__erv_erb_combined_short$official_number)
# 71
```

check

```{r}
# db_participants_address__needed_short__erv_erb_combined_short |>
#   dplyr::filter(official_number == "1235397") |>
#   dplyr::glimpse()
# $ db_physical_city     <list> ["SOUTH ISLANDIA"], ["ISLANDIA"]
```

### Combine similar fields 

```{r Combine similar fields}
## Combine similar fields ----
```


Create a new dataframe with combined and unique values for each participant field

Explanations for the following code:

1. Iterate over each participant column using 'col_part_names'.

col_part_names is a list of participant column name suffixes (e.g., "physical_city", "physical_state", etc.)

   - 'map' applies the provided function to each element of the list.

2. Define the old and new column names based on the current participant column.

   - 'str_glue' is used for string interpolation to create column names.

3. Group the DataFrame by 'official_number' using 'group_by'.

4. For each group, create a new column with unique sorted values for the current participant.

   - 'list_sort_uniq' ensures unique values and sorts them.

5. Ungroup the DataFrame and remove the 'official_number' column.

   - 'ungroup' removes grouping structure.

   - 'select' is used to exclude the 'official_number' column and keep only the new column.

6. Bind the resulting columns to 'db_participants_address__needed_short__erv_erb_combined_short'.

   - 'bind_cols' combines columns horizontally.

7. Select only the 'official_number' and columns ending with '_u'.

The '_u' suffix in new column names indicates that these columns contain unique, combined values

8. Keep only distinct rows in the final DataFrame using 'distinct'.

9. The resulting DataFrame is stored in 'db_participants_address__needed_short__erv_erb_combined_short__u'.


```{r}
db_participants_address__needed_short__erv_erb_combined_short__u_temp <-
  col_part_names |>
  purrr::map(\(curr_col_part)  {
    # browser() # Commented out browser function for debugging

    old_col_name <- stringr::str_glue("db_{curr_col_part}")
    new_col_name <- stringr::str_glue("db_{curr_col_part}_u")
    cat(new_col_name, sep = "\n")

    db_participants_address__needed_short__erv_erb_combined_short |>
      dplyr::group_by(official_number) |>
      dplyr::mutate(!!new_col_name := list(paste(sort(unique(stringr::str_trim(purrr::list_flatten(!!sym(old_col_name))))))),
             .keep = "none" ) |>
      dplyr::ungroup() |>
      dplyr::select(-official_number)
  })

# glimpse(db_participants_address__needed_short__erv_erb_combined_short)
```

Explanation:

This code creates a new dataframe by combining columns with suffix "_u" from two existing dataframes (`db_participants_address__needed_short__erv_erb_combined_short` and `db_participants_address__needed_short__erv_erb_combined_short__u_temp`).

1. **Binding Columns Together:**

   - `dplyr::bind_cols(...)`: It binds columns from two dataframes together. The columns from `db_participants_address__needed_short__erv_erb_combined_short` and `db_participants_address__needed_short__erv_erb_combined_short__u_temp` are combined horizontally.

2. **Selecting Columns:**

   - `dplyr::select(official_number, dplyr::all_of(dplyr::ends_with("_u")))`: After binding columns, it selects the `official_number` column along with all columns that end with "_u" using `select` from dplyr.

3. **Removing Duplicate Rows:**

   - `dplyr::distinct()`: It removes duplicate rows from the dataframe to ensure each row is unique.

This code essentially creates a new dataframe containing selected columns from two existing dataframes and ensures that there are no duplicate rows in the resulting dataframe.


```{r}
db_participants_address__needed_short__erv_erb_combined_short__u <-
  dplyr::bind_cols(
    db_participants_address__needed_short__erv_erb_combined_short,
    db_participants_address__needed_short__erv_erb_combined_short__u_temp
  ) |>
  dplyr::select(official_number, dplyr::all_of(dplyr::ends_with("_u"))) |>
  dplyr::distinct()
```

check

```{r}
# db_participants_address__needed_short__erv_erb_combined_short__u |>
#   filter(official_number == "1235397") |>
#   glimpse()
```

#### Convert to characters 

```{r Convert to characters}
### Convert to characters ----
```

Explanation:

This code modifies the dataframe `db_participants_address__needed_short__erv_erb_combined_short__u` by concatenating the elements of list-type columns into a single string separated by semicolons. Here's a detailed explanation:

1. **Row-wise Operation:**

   - `dplyr::rowwise()`: It sets the dataframe to be processed row-wise, meaning each operation will be applied independently to each row.

2. **Mutating List-type Columns:**

   - `dplyr::mutate_if(is.list, ~ paste(unlist(.), collapse = '; '))`: This line applies a mutation to each column of the dataframe that is of list type.

     - `is.list`: Checks if a column is of list type.

     - `paste(unlist(.), collapse = '; ')`: For each list-type column, it converts the list elements into a single string by unlisting them and concatenating them together with a semicolon as the separator.

3. **Ungrouping:**

   - `dplyr::ungroup()`: It removes the grouping previously applied to the dataframe, returning it to its original state.

This code effectively transforms list-type columns in the dataframe into character vectors, concatenating their elements into a single string with semicolons as separators.


```{r}
db_participants_address__needed_short__erv_erb_combined_short__u_no_c <-
  db_participants_address__needed_short__erv_erb_combined_short__u |>
  dplyr::rowwise() |>
  dplyr::mutate_if(is.list, ~ paste(unlist(.), collapse = '; ')) |>
  dplyr::ungroup()
```

check

```{r}
# db_participants_address__needed_short__erv_erb_combined_short__u_no_c |>
#   filter(official_number == "1235397") |>
#   glimpse()
# $ db_mailing_state_u     <chr> "NY"
# $ db_mailing_city_u      <chr> "ISLANDIA; SOUTH ISLANDIA"
```

### Rename fields 

```{r Rename fields}
## Rename fields ----
```

Explanation:

Remove the "_u" suffix from column names in the dataframe

1. **Renaming Columns:**

   - `dplyr::rename_with( ~ stringr::str_replace(.x, pattern = "_u$", replacement = ""))`: It renames the columns of the dataframe using a function provided by `rename_with`.

     - `~`: It indicates the start of an anonymous function.

     - `stringr::str_replace(.x, pattern = "_u$", replacement = "")`: For each column name (`x`), it applies the `str_replace` function from the `stringr` package to replace the pattern "_u" at the end of the column name with an empty string, effectively removing it.

This code removes the "_u" suffix from the column names in the dataframe.


```{r}
db_participants_address__needed_short__erv_erb_combined_short__u_ok <-
  db_participants_address__needed_short__erv_erb_combined_short__u_no_c |>
  dplyr::rename_with(~ stringr::str_replace(.x, pattern = "_u$",
                                            replacement = ""))
```

## Join FHIER and Oracle db addresses 

```{r Join FHIER and Oracle db addresses}
# Join FHIER and Oracle db addresses ----
```

Combine FHIER and Oracle database address information using vessel official number


```{r}
compl_corr_to_investigation__corr_date__hailing_port__fhier_addr__db_addr <-
  compl_corr_to_investigation__corr_date__hailing_port__fhier_addr |>
  dplyr::left_join(
    db_participants_address__needed_short__erv_erb_combined_short__u_ok,
    dplyr::join_by(vessel_official_number == official_number)
  )

# check
# compl_corr_to_investigation__corr_date__hailing_port__fhier_addr__db_addr |>
#   filter(vessel_official_number == "1235397") |>
#   glimpse()
# $ db_mailing_state       <chr> "NY"
# $ db_mailing_city        <chr> "ISLANDIA; SOUTH ISLANDIA"

cat("Result: ",
    "compl_corr_to_investigation__corr_date__hailing_port__fhier_addr__db_addr",
    sep = "\n")
```


Results are in compl_corr_to_investigation__corr_date__hailing_port__fhier_addr__db_addr

### 3. Mark vessels already in the know list 

```{r 3 Mark vessels already in the know list}
## 3. Mark vessels already in the know list ----
```

Identify and mark vessels that have been previously marked as egregious, to track repeat offenders

From the email:

The first column (report created) indicates the vessels that we have created a case for. My advice would be not to exclude those vessels. EOs may have provided compliance assistance and/or warnings already. If that is the case and they continue to be non-compliant after that, they will want to know and we may need to reopen those cases.


get vessel ids from the previous result 

```{r}
vessels_to_mark_ids <-
  prev_result |>
  dplyr::select(vessel_official_number)
```


Check the amount 

```{r}
dim(vessels_to_mark_ids)
```

##### Mark these vessels 

```{r Mark these vessels}
#### Mark these vessels ----
```


Create a new column to distinguish between previously processed vessels and new entries

Explainations for the following code:

Create a new column 'duplicate_w_last_time' in the dataframe 'compl_corr_to_investigation_short'.

This column is marked with "duplicate" for rows where 'vessel_official_number' is present in the list of vessel IDs ('vessels_to_mark_ids').

For all other rows, it is marked as "new".


```{r}
compl_corr_to_investigation__corr_date__hailing_port__fhier_addr__db_addr__dup_marked <-
  compl_corr_to_investigation__corr_date__hailing_port__fhier_addr__db_addr |>
  dplyr::mutate(
    duplicate_w_last_time =
      dplyr::case_when(
        vessel_official_number %in%
          vessels_to_mark_ids$vessel_official_number ~ "duplicate",
        .default = "new"
      )
  )
```

#### Check 

```{r Check}
### Check ----
```

Perform validation checks on the processed data
Check that number of vessels didn't change.

```{r}
dplyr::n_distinct(compl_corr_to_investigation__corr_date__hailing_port__fhier_addr__db_addr__dup_marked$vessel_official_number) ==
  num_of_vsl_to_investigate
```


Check that there is one row per vessel.

```{r}
nrow(compl_corr_to_investigation__corr_date__hailing_port__fhier_addr__db_addr__dup_marked) == 
  num_of_vsl_to_investigate 
```


Count how many duplicates 

```{r}
compl_corr_to_investigation__corr_date__hailing_port__fhier_addr__db_addr__dup_marked |>
  dplyr::count(duplicate_w_last_time)
# 1 duplicate               108
# 2 new                      48
```

### 4. How many are duals? 

```{r 4 How many are duals}
## 4. How many are duals? ----
```

Identify and count vessels with dual permits (both SA and GOM)
Explainations for the following code:

Create a new dataframe 

Use the 'mutate' function to add a new column 'permit_region' based on conditions.

If 'permitgroup' contains any of the specified patterns ("RCG", "HRCG", "CHG", "HCHG"),

set 'permit_region' to "dual". Otherwise, set 'permit_region' to "sa_only".

If none of the conditions are met, set 'permit_region' to "other".

The resulting dataframe includes the original columns from 'compl_corr_to_investigation_short_dup_marked'
along with the newly added 'permit_region' column.


```{r}
compl_corr_to_investigation_short_dup_marked__permit_region <-
  compl_corr_to_investigation__corr_date__hailing_port__fhier_addr__db_addr__dup_marked |> 
  dplyr::mutate(permit_region =
           dplyr::case_when(
             grepl("RCG|HRCG|CHG|HCHG", permitgroup) ~ "dual",
             !grepl("RCG|HRCG|CHG|HCHG", permitgroup) ~ "sa_only",
             .default = "other"
           ))
```



Explainations for the following code:

Use the 'select' function to extract the columns 'vessel_official_number' and 'permit_region'

from the dataframe 'compl_corr_to_investigation_short_dup_marked__permit_region'.

Use the 'distinct' function to keep only unique combinations of 'vessel_official_number' and 'permit_region'.

Use the 'count' function to count the occurrences of each unique 'permit_region'.

The resulting count provides the frequency of each 'permit_region'.


```{r}
region_counts <-
  compl_corr_to_investigation_short_dup_marked__permit_region |>
  dplyr::select(vessel_official_number, permit_region) |>
  dplyr::distinct() |>
  dplyr::count(permit_region)

dplyr::n_distinct(compl_corr_to_investigation_short_dup_marked__permit_region$vessel_official_number)
```

#### Dual permitted cnts 

```{r Dual permitted cnts}
### Dual permitted cnts ----
```

Calculate the percentage of dual-permitted vessels

```{r}
region_counts$n[[1]] / (region_counts$n[[2]] + region_counts$n[[1]]) * 100
```

## Print out results 

```{r Print out results}
# Print out results ----
```

### Add additional columns in front 

```{r Add additional columns in front}
## Add additional columns in front ----
```


Create a variable with a long column name for confirmation status


```{r}
additional_column_name1 <-
  stringr::str_glue(
    "Confirmed Egregious? (permits must still be active till {permit_expired_check_date}, missing past 6 months, and (1) they called/emailed us (incoming), or (2) at least 2 contacts (outgoing) with at least 1 call/other (voicemail counts) and at least 1 email)"
  )
```


Explanation:

This code adds new columns to the dataframe `compl_corr_to_investigation_short_dup_marked__permit_region`. Here's what each part does:

1. **Add Columns Function:**

   - `tibble::add_column()`: This function from the `tibble` package is used to add new columns to a dataframe.


2. **Column Specifications:**

   - `!!(additional_column_name1) := NA`: Adds a new column with a name in `additional_column_name1` variable filled with NA values.

     - `!!`: This is a tidy evaluation feature that allows the use of non-standard evaluation. It evaluates the expression `additional_column_name1` dynamically.

     - `:= NA`: Assigns NA values to the new column.

   - `Notes = NA`: Adds another new column named "Notes" filled with NA values.

   - `.before = 2`: Specifies that the new columns should be inserted before the second column in the dataframe.

This code effectively adds two new columns, with names from `additional_column_name1` and "Notes", filled with NA values, to the dataframe.


```{r}
compl_corr_to_investigation_short_dup_marked__permit_region__add_columns <-
  compl_corr_to_investigation_short_dup_marked__permit_region |>

  tibble::add_column(
    !!(additional_column_name1) := NA,
    Notes = NA,
    .before = 2
  )
```


Don't remove the "year" column, in case there are 2 years in the current period.


Check and display the updated column names


```{r}
auxfunctions::print_df_names(compl_corr_to_investigation_short_dup_marked__permit_region__add_columns)
```

Generate output file name with current date

The file name is split into a basename and a full name with extension.

The basename will be used for the spreadsheet saved on Google Drive.
 

```{r}
out_file_basename <- 
  stringr::str_glue("egregious_violators_to_investigate_{lubridate::today()}")

out_file_name <-
  stringr::str_glue("{out_file_basename}.csv")

result_path <- 
  file.path(current_project_output_path,
            out_file_name)
```

Write the results to a CSV file

```{r}
compl_corr_to_investigation_short_dup_marked__permit_region__add_columns |>
  readr::write_csv(result_path)
```

### Write to google sheets 

```{r Write to google sheets}
## Write to google sheets ----
```


Explainations for the following code:

Define a function to write results to Google Sheets

This function performs the following steps:

1. Renames the existing 'Egregious Violators Current' spreadsheet to a spreadsheet with the name with a date from its tab (e.g. "egregious_violators_to_investigate_2024-06-18"

2. Creates a new 'current' spreadsheet

3. Writes the new results to the new 'Egregious Violators Current' spreadsheet in the same Google drive directory ("Egregious violators/output")

4. Removes the default empty sheet

5. Opens the new spreadsheet in the browser for verification

6. Returns a shareable link to the new spreadsheet

It need to be a function, this way we can call it if needed, not every time we run the code.


```{r}
write_res_to_google_sheets <- 
  function() {
  
    # Define the current result Google Sheets name
    current_result_google_ss_name <- "Egregious Violators Current"
    
    # my_current_ss contains information about the existing 'Egregious Violators Current' file
    my_current_ss <-
      googledrive::drive_ls(
        path = googledrive::as_id(output_egr_violators_googledrive_folder_path),
        pattern = current_result_google_ss_name,
        type = "spreadsheet",
        n_max = 1
      )

    # An example of my_current_ss:
    #   name                        id                                           drive_resource
    # <chr>                       <drv_id>                                     <list>
    # 1 Egregious Violators Current ...--o6BpLWpb4-... <named list [36]>
    
    # Next:
    # 1) load it to R;
    # 2) create a new spreadsheet with the date of the loaded worksheet and dump the content into it;
    # 3) create a new worksheet in the current spreadsheet with today's date;
    # 4) write the code output into it;
    # 5) check in browser.
    
    # 1) load it to R
    previous_current_content <- googlesheets4::read_sheet(my_current_ss)
    
    # 2) create a new spread sheet with the date of loaded worksheet and dump the content into it
    # a) get the previous spreadsheet name
    
    # ss_info contains detailed information about the current spreadsheet, including sheet names
    ss_info <- googlesheets4::gs4_get(my_current_ss)
    
    # grep for the pattern in case there are additional tabs 
    previous_current_spread_sheet_name <- 
      grep("egregious_violators_to_investigate_20\\d\\d-\\d\\d-\\d\\d", ss_info$sheets$name, value = T)
    # E.g. "egregious_violators_to_investigate_2024-06-18"
    
    # Rename the file from "current" to the previous_current_spread_sheet_name with the previous date.
    # Note. The next line will rise an error if a file with this name already exists, to change that behavior remove 'overwrite = FALSE,"
    googledrive::drive_mv(
      my_current_ss,
      path = googledrive::as_id(output_egr_violators_googledrive_folder_path),
      overwrite = FALSE,
      name = previous_current_spread_sheet_name
    )
    # E.g.
    # Original file:
    # • Egregious Violators Current
    # Has been renamed:
    # • output/egregious_violators_to_investigate_2024-06-18

    # Create a new empty spreadsheet in the Google Drive output folder to replace the renamed one
    # And save its properties into current_result_google_ss_name_info
    
    current_result_google_ss_name_info <- 
    googledrive::drive_create(
      name = current_result_google_ss_name,
      path = googledrive::as_id(output_egr_violators_googledrive_folder_path),
      type = "spreadsheet",
      overwrite = FALSE
    )
    
  # Write our results into the newly created spreadsheet "Egregious Violators Current"
  # into a sheet/tab with a name defined in out_file_basename
    googlesheets4::write_sheet(
      compl_corr_to_investigation_short_dup_marked__permit_region__add_columns,
      ss = current_result_google_ss_name_info,
      sheet = out_file_basename
    )
    
    # See sheets/tabs to check
    googlesheets4::sheet_properties(ss = current_result_google_ss_name_info)
    
      # Remove the empty Sheet1 created automatically by googledrive::drive_create()
    googlesheets4::sheet_delete(ss = current_result_google_ss_name_info, "Sheet1")
    
    # Check the existing tabs again
    googlesheets4::sheet_properties(ss = current_result_google_ss_name_info)$name
    # Should be only one name now, like
    # [1] "egregious_violators_to_investigate_2024-07-15"
    
    # See in browser to check
    googledrive::drive_browse(current_result_google_ss_name_info)
    
    # Generate a shareable link for the new spreadsheet
    current_output_file_link <- googledrive::drive_link(current_result_google_ss_name_info)

    print(current_output_file_link)
    
    # The function returns the current output file link
    return(current_output_file_link)
    
  }
```


Un-comment to write results directly to Google drive

```{r}
current_output_file_link <- write_res_to_google_sheets()
```


Print result names to console 

```{r}
cat("Results:",
    "compl_corr_to_investigation_short_dup_marked__permit_region__add_columns",
    out_file_name,
    sep = "\n")
```

