---
title: Egregious Violators
---

# Setup 

```{r Setup}
# Setup ----
```

This script identifies and processes "egregious violators" in the SEFHIER program.

SEFHIER stands for Southeast For-Hire Integrated Electronic Reporting.

It combines compliance and correspondence data, applies specific filters,
and prepares a report of vessels requiring further investigation.

The Setup section includes necessary libraries, functions, and data loading.

### The "egregious violator" definition 

```{r The egregious violator definition}
## The "egregious violator" definition ----
```


1. No reports for all 26 weeks back from week ago today;

2. Permits have not expired and were active for the same period as (1);

3. The grace period is 7 days back from today.

4. It needs to be that we called at least 1 time and emailed at least 1 time. Or they contacted us at least once.

5. Not counting any correspondence (regardless of the type - email/call, voicemail or not) that includes "No contact made" in the text of the entry as an actual "direct" contact for any egregious vessel (May 6 2024)

### New requirement 2023-08-09 

```{r New requirement 20230809}
## New requirement 2023-08-09 ----
```

6. It should be at least 2 contact "attempts". i.e., if they are ignoring our calls and emails then they cannot continue to go on in perpetuity without reporting and never be seen as egregious. So, at least 1 call (could be a voicemail) and also at a 2nd call (could be a voicemail) or an email. So, if we called 1x and left a voicemail and then attempted an email, then we have tried enough at this point and they need to be passed to OLE.

at least 1 call (could be a voicemail) and also at a 2nd call (could be a voicemail) or an email. So, if we called 1x and left a voicemail and then attempted an email, then we have tried enough

### New requirement 2024-02-26 

```{r New requirement 20240226}
## New requirement 2024-02-26 ----
```

7. It needs to be that we called at least 1 time and emailed at least 1 time. Or they contacted us at least once.

### New requirement 2024-05-06 

```{r New requirement 20240506}
## New requirement 2024-05-06 ----
```

8. Exclude any correspondence (regardless of the type - email/call, voicemail or not) that includes "No contact made" in the text of the entry as a actual "direct" contact for any egregious vessel.

About comments:

There are several types of comments in the following code.

1) If the comment starts with "Note." or "Manually:" it is essential for running the code.

2) '<<<<' and '>>>>' mark the start and the end of definitions and help documents for helper functions. Another way to see an R help document is to type a question mark in the Console followed by the function name, e.g. ?mutate

3) All other comments explain the logic and the syntax.


Note. Update (download) all input files every time before run.

If there is no comment with the word "manually" before the code, it will work automatically.

### Install packages if needed 

```{r Install packages if needed}
## Install packages if needed ----
# <<<<

# Explanations for the following code:

# This function provides a simple way to obtain the username of the user executing the R code, which can be useful for personalization, authentication, or logging purposes. 
#  
# The `get_username` function is designed to retrieve the username of the current user.
# 
# 1. **Function Definition:**
#    - `get_username <- function() { .. }`: Defines a function named `get_username` with no arguments.
# 
# 2. **Using System Information:**
#    - `Sys.info()`: Calls the `Sys.info` function, which provides various system information about the current R session.
#    - `Sys.info()["user"]`: Retrieves the username of the current user from the system information.
# 
# 3. **Converting to Character:**
#    - `as.character()`: Converts the username to a character vector. This step ensures consistency in the data type of the returned value.
# 
# 4. **Return Result:**
#    - The function returns the username of the current user as a character string.
# 

 get_username  <-  function () 
{
    return(as.character(Sys.info()["user"]))
}
# >>>>

# 
```

If any package suggests updates it is safe to choose option 1 (update all). Or run the whole code from "Source".

We don't load most of the packages to the current session namespace with library(), instead, functions are called from their packages with "::" notation. 

Install packages not yet installed


```{r}
needed_packages <- c(
  "devtools", # Collection of package development tools.
  "zoo", # Handling time series data.
  "diffdf", # Compares dataframes and identifies differences.
  # packages for Google Sheets and Google Drive
  #
  # Refer to this guide: https://felixanalytix.medium.com/how-to-read-write-append-google-sheet-data-using-r-programming-ecf278108691#:~:text=There%20are%203%20ways%20to%20read%20this%20Google%20sheet%20into%20R.&text=Just%20to%20take%20the%20URL,URL%20but%20just%20the%20ID).
  #
  "googlesheets4", # Google Sheets via the Sheets API v4
  "googledrive" # Interact with Google Drive
)
```

Explanations for the following code:

```{r}
# - `needed_packages %in% rownames(installed.packages())` checks which packages from `needed_packages` are installed:
#   - `installed.packages()` returns a matrix of information about all installed packages.
#   - `rownames(installed.packages())` extracts the names of the installed packages.
#   - `needed_packages %in% ...` checks if each package in `needed_packages` is in the list of installed packages, returning a logical vector indicating the presence of each package.
# - `if (any(installed_packages == FALSE)) { ... }` checks if any package is not installed:
#   - `any(installed_packages == FALSE)` returns `TRUE` if at least one element in `installed_packages` is `FALSE`.
#   - `install.packages(packages[!installed_packages])` installs the packages that are not installed:
#     - `packages[!installed_packages]` selects the packages from `packages` that are not installed.
#     - `install.packages()` installs the selected packages.  
installed_packages <-
  needed_packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}
```

Helper functions for SEFHIER data analysis.

Explainations for the following code:

The installation details depend on the username.

For most users, install from the main branch if not already installed.

One doesn't have to have a GitHub account to use it.

For the package developer, install from the development branch.

- `if (!require("auxfunctions"))` checks if the `auxfunctions` package is installed and loaded:

  - `require("auxfunctions")` attempts to load the `auxfunctions` package.

  - The `!` operator negates the result, so the condition is true if the package is not installed or cannot be loaded.

- `devtools::install_github("AShipunova1/R_code/auxfunctions")` installs the `auxfunctions` package from the specified GitHub repository:

  - `devtools::install_github()` is a function from the `devtools` package that installs an R package directly from a GitHub repository.

  - `"AShipunova1/R_code/auxfunctions"` specifies the repository and subdirectory containing the package.

This code checks if the `auxfunctions` package is available, and if not, it installs it from the GitHub repository `AShipunova1/R_code/auxfunctions`.


```{r}
# Check if the username is not "anna.shipunova"
if (!get_username() == "anna.shipunova") {
    # If the auxfunctions package is not installed, install it from GitHub
  if (!require('auxfunctions')) {
    # devtools::install_github("AShipunova1/R_code/auxfunctions")
  }
} else {
  # For a developer, rebuild the package from the development branch. To force the installation change to 'force = TRUE'
  # devtools::install_github("AShipunova1/R_code/auxfunctions@development", force = FALSE)
  # restart R session to pick up changes
  # .rs.restartR()
}

# Load the ROracle package for database interactions with Oracle databases.
library(ROracle)
# Load the magrittr package for piping operation %>%. In some cases the base R "|>" works differently. See more https://www.tidyverse.org/blog/2023/04/base-vs-magrittr-pipe/
library(magrittr)
```

```{r no cache setup, results='hide', message=FALSE, warning=FALSE, cache=FALSE, include=FALSE}

## Quarto Setup

# Quarto enables you to weave together content and executable code into a finished document.
 
# Running Code
 
# The **Run** button allows you to run individual or bunch of chunks as a regular R script.
 
# When you click the **Render** button a document will be generated that includes both content and the output of embedded code.

# Load libraries required for Quatro
# A general-purpose tool for dynamic report generation in R
library(knitr)

# Adds features to a kable output
library(kableExtra)

# Format R code automatically
library(styler)
```

```{r df format setup}
#| include: false

# Customize the appearance of dataframes in HTML

# Uncomment if using tabs
# kable <- function(data) {
#   knitr::kable(data, booktabs = true, digits = 2) %>%
#     kable_styling('striped', full_width = FALSE)
# }

# Define a custom print function for data frames in knitr
knit_print.data.frame = function(x, ...) {
  res = paste(c(
    '',
    '',
    knitr::kable(x, digits = 2) |>
      kableExtra::kable_styling('striped', full_width = FALSE)
  ),
  collapse = '
')
  knitr::asis_output(res)
}

# Register the custom print function for data frames in the knitr namespace
registerS3method(
  'knit_print', 'data.frame', knit_print.data.frame,
  envir = asNamespace('knitr')
)

# Set global chunk options in knitr if needed
# knitr::opts_chunk$set(echo = TRUE)

# Set the table format for knitr to HTML if needed
# options(knitr.table.format = 'HTML')

# End of Quarto setup

```

### Define dates 

```{r Define dates}
## Define dates ----
```


Define start and end years for the analysis period

Manually: Values for `my_year1` and `my_year2` may be adjusted as needed


```{r}
# start year for the analysis
my_year1 <- "2023"
my_beginning1 <- stringr::str_glue("{my_year1}-01-01")
my_end1 <- stringr::str_glue("{my_year1}-12-31")

# last year for the analysis
my_year2 <- "2024"
my_beginning2 <- stringr::str_glue("{my_year2}-01-01")
my_end2 <- stringr::str_glue("{my_year2}-12-31")
```


Following are the definitions of dates used throughout the code.

Set the current date as the data file date

```{r}
data_file_date <- 
  lubridate::today()
```

How many weeks and days to take in to the account?
The 26-week period is used to define long-term non-compliance

```{r}
number_of_weeks_for_non_compliancy = 26
```

Calculate number of days in non compl weeks

```{r}
days_in_non_compl_weeks <- 
  number_of_weeks_for_non_compliancy * 7

# test, should be TRUE
days_in_non_compl_weeks == 182
```


The 7-day grace period allows for recent reports that may not yet be processed

```{r}
grace_period = 7 # days

# Calculate the date 26 weeks (plus grace period) before the current date
half_year_ago <-
  data_file_date - days_in_non_compl_weeks - grace_period
```


Check the week number and day of the week for the period's start

This can be used to verify the calculation against a calendar


```{r}
lubridate::week(half_year_ago)

lubridate::wday(half_year_ago, label = T)
```


Set the minimum date for permit expiration (30 days from today)

```{r}
permit_expired_check_date <- data_file_date + 30
```


Define the start of the last week, excluding it from analysis

```{r}
last_week_start <- data_file_date - grace_period
```


### Set up paths 

```{r Set up paths}
## Set up paths ----
# <<<<

# Explanations for the following code:

# Pretty message print in the console.
# 
# Provides a way to print messages with enhanced visual formatting, making them stand out for better readability or emphasis.
#  
#  
# The `function_message_print` function is a utility function for printing messages with a cyan background color.
# 
# 1. **Function Definition:**
#    - `function_message_print <-function (text_msg) { .. }`: Defines a function named `function_message_print` that takes one argument, `text_msg`, representing the message to be printed.
# 
# 2. **Printing Message with Cyan Background:**
#    - `crayon::bgCyan$bold(text_msg)`: Formats the `text_msg` with a cyan background color and bold font using the `crayon` package. This creates a visually distinct message when printed.
#    - `cat(..., sep = "n")`: Prints the formatted message to the console. `cat` is a function used for printing, and `sep = "n"` specifies that each message should be printed on a new line.
# 

 function_message_print  <-  function (text_msg) 
{
    cat(crayon::bgCyan$bold(text_msg), sep = "\n")
}
# >>>>

# <<<<

# Explanations for the following code:

# This function sets the working directory to the user's home directory, defines directory paths for inputs, outputs, and GitHub R code, changes the working directory to the main R directory, and returns these paths as a list.
# 
# It assumes a specific directory structure, where data are separated from the code, so the latter can be stored on GitHub.
# 
#  
#  
# 1. **Function Definition:**
# 
#    - `set_work_dir <- function ()`: Defines a function named `set_work_dir` that takes no arguments.
# 
# 2. **Setting Working Directory:**
# 
#    - `setwd("~/")`: Sets the working directory to the user's home directory.
# 
#    - `base_dir <- getwd()`: Retrieves the current working directory and assigns it to the variable `base_dir`.
# 
# 3. **Defining Directory Paths:**
# 
#    - `main_r_dir <- "R_files_local"`: Specifies the directory name for local R files.
# 
#    - `in_dir <- "my_inputs"`: Specifies the directory name for input files.
# 
#    - `full_path_to_in_dir <- file.path(base_dir, main_r_dir, in_dir)`: Combines the base directory, main R directory, and input directory to create the full path to the input directory.
# 
#    - `out_dir <- "my_outputs"`: Specifies the directory name for output files.
# 
#    - `full_path_to_out_dir <- file.path(base_dir, main_r_dir, out_dir)`: Combines the base directory, main R directory, and output directory to create the full path to the output directory.
# 
#    - `git_r_dir <- "R_code_github"`: Specifies the directory name for the GitHub repository containing R code.
# 
#    - `full_path_to_r_git_dir <- file.path(base_dir, git_r_dir)`: Combines the base directory and GitHub R directory to create the full path to the GitHub R directory.
# 
# 4. **Creating Paths List:**
# 
#    - `my_paths <- list(...)`: Creates a list named `my_paths` containing the paths to the input directory, output directory, and GitHub R directory.
# 
# 5. **Returning Paths List:**
# 
#    - `return(my_paths)`: Returns the `my_paths` list containing the paths to the input directory, output directory, and GitHub R directory.
# 
# 

 set_work_dir  <-  function () 
{
    setwd("~/")
    base_dir <- getwd()
    main_r_dir <- "R_files_local"
    in_dir <- "my_inputs"
    full_path_to_in_dir <- file.path(base_dir, main_r_dir, in_dir)
    out_dir <- "my_outputs"
    full_path_to_out_dir <- file.path(base_dir, main_r_dir, out_dir)
    git_r_dir <- "R_code_github"
    full_path_to_r_git_dir <- file.path(base_dir, git_r_dir)
    setwd(file.path(base_dir, main_r_dir))
    my_paths <- list(inputs = full_path_to_in_dir, outputs = full_path_to_out_dir, 
        git_r = full_path_to_r_git_dir)
    return(my_paths)
}
# >>>>

# <<<<

# Explanations for the following code:

# Creates a list of paths to the current project code, input, and output directories. By default uses Anna's computer setup. It also creates input and output directories if they do not exist.
#  
#  

 current_project_paths  <-  function () 
{
    my_paths <- set_work_dir()
    current_project_dir_name <- this.path::this.dir()
    current_project_name <- basename(current_project_dir_name)
    curr_proj_input_path <- file.path(my_paths$inputs, current_project_name)
    create_dir_if_not(curr_proj_input_path)
    curr_proj_output_path <- file.path(my_paths$outputs, current_project_name)
    create_dir_if_not(curr_proj_output_path)
    current_proj_paths <- list(project_name = current_project_name, 
        code = current_project_dir_name, input = curr_proj_input_path, 
        output = curr_proj_output_path)
    return(current_proj_paths)
}
# >>>>

# <<<<

# Explanations for the following code:

# The `create_dir_if_not` function ensures the existence of a directory by creating it if it does not already exist.
# 
# This function provides a convenient way to ensure that a directory exists before performing operations such as writing files or storing data.
#  
#  
# 1. **Function Definition:**
#    - `create_dir_if_not <- function(curr_dir_name) { .. }`: Defines a function named `create_dir_if_not` that takes a single argument, `curr_dir_name`, representing the name of the directory to be created.
# 
# 2. **Checking Directory Existence:**
#    - `if (!dir.exists(curr_dir_name)) { .. }`: Checks if the directory specified by `curr_dir_name` does not exist.
#    - `dir.exists(curr_dir_name)`: Checks whether the directory specified by `curr_dir_name` exists. The `!` negates the result, so the code block inside the `if` statement executes only if the directory does not exist.
# 
# 3. **Creating Directory:**
#    - `dir.create(curr_dir_name)`: If the directory does not exist, this line creates the directory specified by `curr_dir_name` using the `dir.create` function.
# 
# 4. **Return Statement:**
#    - The function does not have a specific return value. It either creates the directory if it does not exist or takes no action if the directory already exists.
# 

 create_dir_if_not  <-  function (curr_dir_name) 
{
    if (!dir.exists(curr_dir_name)) {
        dir.create(curr_dir_name)
    }
}
# >>>>
```


Different methods are used based on the user to accommodate different directory structure.

This allows the script to run correctly on multiple systems without manual path changes.

In the code in this section all user provided values have the word "manually" in the description. Everything else is created automatically.

Manually: Change the following 2 lists (**my_paths** and **current_in_out_paths**) to your environment if needed. The variable _names_ are used throughout the code, so please change only the quoted _values_ inside the lists.


```{r}
# Check if the current username is not "anna.shipunova"
if (!get_username() == "anna.shipunova") {
  function_message_print(
    "Please CHANGE the following 2 lists values to your environment if needed. Use full path to your directories in quotes."
  )
  
  # 1) General directories (to look up additional files, e.g. processed data). It can be left as is if you don't have it. You can provide path to individual files later.
  my_paths <- list(inputs  = "~/my_inputs",
                   outputs = "~/my_outputs",
                   git_r   = "~/R_code")
  
  # 2) Current project code, input and output directories
  current_in_out_paths <-
    list(
      project_name = "validation_survey",
      code = "~/validation_survey/code",
      input = "~/validation_survey/input",
      output = "~/validation_survey/output"
    )
  
} else {
  # If the username is "anna.shipunova", use Anna's directory structure.
  my_paths <- set_work_dir()
  current_in_out_paths <- current_project_paths()
}
```


The following section uses provided directory names lists to automatically create separate variables for future use and create current input/output directories if they do not exists.


Create variables to store shortcuts to project directories


```{r}
# This is usually the current directory name.
current_project_name <- current_in_out_paths$project_name

current_project_path <- current_in_out_paths$code
            
current_project_input_path <- current_in_out_paths$input

current_project_output_path <- current_in_out_paths$output
```

Create input and output directories if they don't exist

```{r}
create_dir_if_not(current_project_input_path)

create_dir_if_not(current_project_output_path)
```

#### Additional individual paths to data files 

```{r Additional individual paths to data files}
### Additional individual paths to data files ----
```

This section sets up paths for specific data files used in the project

##### Compliance and Correspondence 

```{r Compliance and Correspondence}
#### Compliance and Correspondence ----
# <<<<

# Explanations for the following code:

# This function constructs full file paths for a list of filenames by combining an input directory, a subdirectory, and the filenames. If `input_dir_part` is not provided, it uses a default directory set by `set_work_dir()`. The resulting full paths are returned as a list.
#  
#  
# `prepare_csv_full_path <- function(filenames_list, add_path, input_dir_part = NA) list(" ... ")` defines a function `prepare_csv_full_path` that takes a list of filenames (`filenames_list`), a subdirectory path (`add_path`), and an optional input directory part (`input_dir_part`).
# 
# list(list(), "  `if (is.na(input_dir_part)) ", list(" ... "), "` checks if `input_dir_part` is `NA`:n", "n", "  ", list(list(), " `is.na(input_dir_part)` checks if `input_dir_part` is `NA`.n", "n", "  ", list(), " If true, it executes the code block inside the `if` statement:n", "n", "    ", list(list(), " `my_paths <- set_work_dir()` sets the working directory using the `set_work_dir` function from the `auxfunctions` package and assigns the result to `my_paths`.n", "n", "    ", 
#     list(), " `input_dir_part <- my_paths$inputs` assigns the `inputs` element of `my_paths` to `input_dir_part`."), "n", "    "))
# 
# list("n", list(), " `my_list <- sapply(filenames_list, function(x) ", list(" ... "), ")` uses `sapply` to apply a function to each element in `filenames_list`:n", "n", "  ", list(list(), " `sapply(filenames_list, function(x) ", list(" file.path(input_dir_part, add_path, x) "), ")` constructs full file paths by combining `input_dir_part`, `add_path`, and each filename `x` using `file.path`."), "n", "n", list(), "  `return(paste(my_list) |> as.list())` converts the resulting list of paths to a character vector and then to a list, and returns it:n", 
#     "n", "    ", list(list(), " `paste(my_list)` converts the elements of `my_list` to a single character string.n", "n", "  ", list(), " `|> as.list()` converts the character string to a list using the pipe operator (`|>`).n", "  "), "n")
# 

 prepare_csv_full_path  <-  function (filenames_list, add_path, input_dir_part = NA) 
{
    if (is.na(input_dir_part)) {
        my_paths <- set_work_dir()
        input_dir_part <- my_paths$inputs
    }
    my_list <- sapply(filenames_list, function(x) {
        file.path(input_dir_part, add_path, x)
    })
    return(paste(my_list) %>% as.list())
}
# >>>>
```


Download from FHIER first.

Manually: Provide full paths here, changing _values_ inside the quotes:


```{r}
correspondence_csv_path <- "Your full path to correspondence.csv"
fhier_compliance_csv_path_list <- 
  list("Your full path to fhier_compliance.csv year 1",
       "Your full path to fhier_compliance.csv year 2")
```


Depending on a user name who runs the code, the file paths are constructed here.


```{r}
# Check if the username is not "anna.shipunova"
if (!get_username() == "anna.shipunova") {
    # Combine correspondence CSV path and compliance CSV paths into one list
  all_csv_full_paths_list <-
    c(correspondence_csv_path, fhier_compliance_csv_path_list)
} else {
  # For Anna Shipunova
  
  # Manually: Change file names to the last download
  # Using raw string literals (r"()") to handle backslashes in file paths

  all_csv_names_list = c(
    "Correspondence_2024_06_17.csv",
    r"(2024_06_17\FHIER_Compliance_2023__06_17_2024.csv)",
    r"(2024_06_17\FHIER_Compliance_2024__06_17_2024.csv)"
  )
  
  # Add a full path in front of each file name for correspondence CSV
  # prepare_csv_full_path function constructs full file paths by combining base path, additional path, and file names

  corresp_full_path <-
    prepare_csv_full_path(all_csv_names_list[[1]],
                          add_path = "from_Fhier/Correspondence",
                          input_dir_part = my_paths$inputs)
  
  # Add a full path in front of each file name for compliance CSVs
  compliance_full_paths <-
    prepare_csv_full_path(all_csv_names_list[2:3],
                                        add_path = "from_Fhier/FHIER Compliance",
                                        input_dir_part = my_paths$inputs)
  
  # Combine correspondence full path and compliance full paths into one list
  all_csv_full_paths_list <-
    c(corresp_full_path,
      compliance_full_paths)

  # all_csv_full_paths_list contains full paths to all required CSV files for correspondence and compliance

  # Check if all the specified files exist
  purrr::map(all_csv_full_paths_list, file.exists)
}
```

##### Processed Metrics Tracking (permits from FHIER) 

```{r Processed Metrics Tracking permits from FHIER}
#### Processed Metrics Tracking (permits from FHIER) ----
```


processed_metrics_tracking_file_names contains paths to RDS files with SEFHIER permitted vessels data for different years. Created separately, see Get data.

Manually: Add your full path to processed Metrics tracking for each year instead of "Your full path here".

Define paths for processed Metrics tracking CSVs.

Depending on a user name who runs the code, the file paths are constructed here.



```{r}
# Check if the username is not "anna.shipunova"
if (!get_username() == "anna.shipunova") {
  processed_metrics_tracking_file_names <-
    c(
      stringr::str_glue(
        "Your full path here/SEFHIER_permitted_vessels_nonSRHS_{my_year1}.rds"
      ),
      stringr::str_glue(
        "Your full path here/SEFHIER_permitted_vessels_nonSRHS_{my_year2}.rds"
      )
    )
} else {
  # for Anna Shipunova
  processed_input_data_path <-
    file.path(my_paths$inputs, "processing_logbook_data", "Outputs")

  # check
  dir.exists(processed_input_data_path)
  # if not TRUE: Check your provided path and/or create manually.
  
  # Get file names for all years
  processed_metrics_tracking_file_names_all <-
  list.files(path = processed_input_data_path,
             pattern = "SEFHIER_permitted_vessels_nonSRHS_*",
             recursive = TRUE,
             full.names = TRUE)

  # Exclude links (shortcuts) to ensure we're only working with actual data files
  processed_metrics_tracking_file_names <-
  grep(
    processed_metrics_tracking_file_names_all,
    pattern = "Shortcut.lnk",
    invert = TRUE,
    value = TRUE
  )

}
```


Check if all processed metrics tracking files exist

```{r}
purrr::map(processed_metrics_tracking_file_names, file.exists)
```


if not TRUE: Check your provided path and/or create manually.

##### Physical Address List from FHIER path 

```{r Physical Address List from FHIER path}
#### Physical Address List from FHIER path ----
```

Download first from REPORTS / For-hire Primary Physical Address List.

Set the path for FHIER addresses based on the user.

Manually: Add your full path instead of "Your full path here".


```{r}
# Check if the username is not "anna.shipunova"
if (!get_username() == "anna.shipunova") {
  fhier_addresses_path <- "Your full path here"
} else {
  # for Anna Shipunova, update file name's date
  fhier_addresses_path <-
    file.path(
      my_paths$inputs,
      r"(from_Fhier\address\For-hire Primary Physical Address List_06_17_2024.csv)"
    )
}
```


fhier_addresses_path points to a CSV file containing the primary physical addresses of for-hire vessels

Verify that the FHIER addresses file exists at the specified path, correct the path if it is doesn't exist


```{r}
file.exists(fhier_addresses_path)
```

##### Home port processed city and state path 

```{r Home port processed city and state path}
#### Home port processed city and state path ----
```

Download first from Google drive.

Set the path for processed PIMS home ports based on the user.

Manually: Add your full path instead of "Your full path here".


```{r}
# Check if the username is not "anna.shipunova"
if (!get_username() == "anna.shipunova") {
  processed_pims_home_ports_path <- "Your full path here"
} else {
  # for Anna Shipunova, update file name's date
  processed_pims_home_ports_path <-
    file.path(my_paths$outputs,
              "home_ports",
              "vessels_from_pims_ports_2024-06-18.csv")
}
```

processed_pims_home_ports_path points to a CSV file containing processed data about vessel home ports, including city and state information

Verify that the processed PIMS home ports file exists at the specified path, correct the path if it is doesn't exist.

```{r}
file.exists(processed_pims_home_ports_path)
```

##### Data from the previous results of "egregious violators for investigation" path 

```{r Data from the previous results of egregious violators for investigation path}
#### Data from the previous results of "egregious violators for investigation" path ----
```


Depending on a user name who runs the code, define the path to the previous results file.

Manually: Add your full path instead of "Your full path here".


```{r}
# Check if the username is not "anna.shipunova"
if (!get_username() == "anna.shipunova") {
  prev_result_path <- "Your full path here"
} else {
  # for Anna Shipunova, update file name's date
  prev_result_path <-
    file.path(current_project_input_path,
              "egregious_violators_to_investigate_2024-05-17.xlsx")
}
```

Check if the previous results file exists.
If it is doesn't exist download it first and correct the path, or load directly from Google drive, see Get data.

```{r}
file.exists(prev_result_path)
```

#### Set up Google Drive paths 

```{r Set up Google Drive paths}
### Set up Google Drive paths ----
```

Hard coded Google drive folder names, manually change here if changing in Google drive.

```{r}
egr_violators_googledrive_folder_name <- "Egregious violators"
output_egr_violators_googledrive_folder_name <- "output"
```


Get the path to the main Egregious violators folder on Google Drive

It is used to read the previous result and for saving the new result.

When asked for the authentication the first time choose the appropriate option and follow the instructions. 

If there is an option with your google email account (like your.name@noaa.gov) you can choose that option (usually #2) and it will confirm your access automatically.

`n_max = 1` means we will use the first result, assuming we have only one folder with such name on Google dive.


```{r}
egr_violators_googledrive_folder_path <-
  googledrive::drive_find(pattern =
                            egr_violators_googledrive_folder_name,
                          type = "folder",
                          n_max = 1)
```


Get the path to the output folder within the Egregious violators folder on Google Drive

Explainations for the following code:

- `output_egr_violators_googledrive_folder_path <- ...` assigns the result of the `googledrive::drive_ls` function call to the variable `output_egr_violators_googledrive_folder_path`.

- `googledrive::drive_ls(...)` lists the contents of a Google Drive folder based on the specified parameters:

  - `path = googledrive::as_id(egr_violators_googledrive_folder_path)` specifies the path to the folder using its ID:

    - `googledrive::as_id(egr_violators_googledrive_folder_path)` converts `egr_violators_googledrive_folder_path` into a format recognized as an ID by the `googledrive` package.

  - `pattern = output_egr_violators_googledrive_folder_name` specifies a pattern to match folder names against, using `output_egr_violators_googledrive_folder_name`.

  - `type = "folder"` specifies that only folders should be listed.

  - `n_max = 1` specifies that only the first matching folder should be returned.
  

```{r}
output_egr_violators_googledrive_folder_path <-
  googledrive::drive_ls(
    path = googledrive::as_id(egr_violators_googledrive_folder_path),
    pattern = output_egr_violators_googledrive_folder_name,
    type = "folder",
    n_max = 1
  )
```

### Other setup 

```{r Other setup}
## Other setup ----
# Set options to prevent converting long numbers to scientific notation for input/output in spreadsheets and csv files
# This ensures vessel numbers and other large integers are displayed in full

options(scipen = 999)

# Synchronize timezone settings between R and Oracle to ensure consistent date-time handling
Sys.setenv(TZ = Sys.timezone())
Sys.setenv(ORA_SDTZ = Sys.timezone())
```

# Prepare data

### Get data 

```{r Get data}
## Get data ----
```



```{r}
# This is used only with source()
get_data_path <- 
  file.path(current_project_path, "egregious_violators_get_data.R")
# get data for egregious violators
# use from egregious_violators_start.R
```

This section loads and prepares various datasets required for the egregious violators analysis

- Compliance records

- Correspondence logs

- Permit information

- Address data

The data is loaded from CSV files, databases, and previously processed results.

This section outlines the different data sources and their purposes in the analysis.
 
The following data are loaded from files or from the Oracle database.

1) compliance data
Download files from FHIER / Reports / FHIER COMPLIANCE REPORT 

For the last 6 month

FHIER_Compliance_...csv

2) correspondence data

Download files from FHIER / Home / Correspondence

Actions / Download 

For the whole period, starting 01/01/2021

"~\my_inputs\from_Fhier\Correspondence\Correspondence_2024_02_15.csv"

3) processed Metrics tracking

From a separate code
 
For the last 6 month

SEFHIER_permitted_vessels_nonSRHS_YEAR.csv

4) Physical Address List from FHIER

Downloaded from REPORTS / For-hire Primary Physical Address List

For the whole period, starting 01/01/2021

"For-hire Primary Physical Address List.csv"

5) home port cleaned city and state from PIMS

"~\R_files_local\my_outputs\home_ports\vessels_from_pims_ports_DATE.csv"

6) address information from Oracle db

"db_participants_address.rds"

7) Previous results (from google drive)

There are two options here, to downloaded the file as .xlsx or to get it directly from Google drive into R.

"~\egregious_violators\egregious_violators_to_investigate_2024-05-17.xlsx"

## FHIER 

```{r FHIER}
# FHIER ----
```

This section focuses on data from the For-Hire Integrated Electronic Reporting Program (FHIER)

Compliance

Correspondence

Permit info from processed metrics tracking

### Compliance and Correspondence data 

```{r Compliance and Correspondence data}
## Compliance and Correspondence data ----
```


```{r}

# <<<<
my_headers_case_function  <-  function (x) 
{
    if (!is.character(x)) 
        x <- as.character(x)
    .Internal(tolower(x))
}
# get_my_used_function_helps(add_f)
# $my_headers_case_function
# [1] "\n# Use this aux function in case we want to change the case in all functions.\n# \n# Used in other functions.\n#  \n#  "

# >>>>
# <<<<

# Explanations for the following code:

# Cleans column names of uploaded files.
# 
# Provides a comprehensive approach to cleaning and standardizing column names within a dataframe, making them more consistent and suitable for further analysis or processing.
#  
#  
# 
# The `fix_names` function is designed to clean and standardize column names within a dataframe.
# 
# 1. **Function Definition:**
# 
#    - `fix_names <- function (x) { .. }`: Defines a function named `fix_names` that takes one argument, `x`, which represents the input dataframe.
# 
# 2. **Cleaning Column Names:**
# 
#    - `x %>% str_replace_all("\\.", "")`: Removes all occurrences of periods (`.`) from the column names using `str_replace_all` from the `stringr` package.
# 
#    - `str_replace_all("[^A-z0-9]", "_")`: Replaces all non-alphanumeric characters (excluding underscores) with underscores (`_`). This step ensures that the column names only contain letters, numbers, and underscores.
# 
#    - `str_replace_all("^(_*)(.+)", "\2\1")`: Ensures that column names do not start with multiple underscores. It captures the leading underscores (if any) and the rest of the column name using regular expressions, then rearranges them to place the underscores at the end of the column name.
# 
#    - `my_headers_case_function()`: Applies a custom function `my_headers_case_function` to further standardize the case of the column names. This function is defined in auxfunctions and is responsible for ensuring consistent capitalization of column names.
# 
# 3. **Return Statement:**
# 
#    - The function returns the modified dataframe with cleaned and standardized column names.
# 
# 

 fix_names  <-  function (x) 
{
    my_headers_case_function(stringr::str_replace_all(stringr::str_replace_all(stringr::str_replace_all(x, 
        "\\.", ""), "[^A-z0-9]", "_"), "^(_*)(.+)", "\\2\\1"))
}
# >>>>

# <<<<

# Explanations for the following code:

# The clean_headers function cleans and standardizes the column names of a given dataframe (my_df)
# 
# using the fix_names() function, which removes special characters and spaces, and unifies the case.
# 
#  
#  

 clean_headers  <-  function (my_df) 
{
    new_names <- fix_names(colnames(my_df))
    colnames(my_df) <- new_names
    return(my_df)
}
# >>>>
 
# <<<<
trim_all_vessel_ids_simple <- 
  function (csvs_clean_ws, col_name_to_trim = NA) 
{
    csvs_clean <- lapply(csvs_clean_ws, function(x) {
        if (is.na(col_name_to_trim)) {
            col_name_to_trim <- grep("vessel.*official.*number", 
                tolower(names(x)), value = TRUE)
        }
        col_name_to_trim_s <- rlang::sym(col_name_to_trim)
        x %>% dplyr::mutate(vessel_official_number = trimws(!!col_name_to_trim_s)) %>% 
            return()
    })
    return(csvs_clean)
}
 
# >>>>

# <<<<

# Explanations for the following code:

# Cleaning, regularly done for csvs downloaded from FHIER,
# usually from inside other functions.
# 
# The clean_all_csvs function is defined to clean a list of CSVs (csvs) and has an optional parameter vessel_id_field_name, which specifies the column to trim.
# 
# It returns the list of cleaned CSVs, where each CSV has had its headers unified and the vessel ID column (if specified) trimmed for consistency.
# 
#  
#  

 clean_all_csvs  <-  function (csvs, vessel_id_field_name = NA) 
{
    csvs_clean0 <- lapply(csvs, clean_headers)
    csvs_clean1 <- trim_all_vessel_ids_simple(csvs_clean0, 
        vessel_id_field_name)
    return(csvs_clean1)
}
# >>>>

# <<<<
add_count_contacts <- 
  function (all_data_df_clean) 
{
    contactdate_field_name <- find_col_name(all_data_df_clean, 
        "contact", "date")[1]
    vessel_id_field_name <- find_col_name(all_data_df_clean, 
        "vessel", "number")[1]
    result_df <- all_data_df_clean %>% dplyr::mutate(was_contacted = dplyr::if_else(is.na(!!rlang::sym(contactdate_field_name)), 
        "no", "yes")) %>% dplyr::add_count(!!rlang::sym(vessel_id_field_name), 
        was_contacted, name = "contact_freq")
    return(result_df)
}

# >>>>
# <<<<
find_col_name <- 
  function (mydf, start_part, end_part) 
{
    to_search <- paste0(start_part, ".*", end_part)
    matching_names <- grep(to_search, tolower(names(mydf)), value = TRUE)
    return(matching_names)
}

# >>>>
# <<<<

# Explanations for the following code:

# This function prepares the correspondence data by adding contact frequency information and converting relevant columns to date format.
#  
#  
# 
# The `corresp_cleaning` function processes a list of cleaned CSV files related to correspondence data.
# 
# 1. **Function Definition:**
# 
#    - `corresp_cleaning <- function(csvs_clean1) { .. }`: Defines a function named `corresp_cleaning` that takes a single argument, `csvs_clean1`, which is assumed to be a list containing cleaned CSV files.
# 
# 2. **Extracting Data:**
# 
#    - `corresp_arr <- csvs_clean1[[1]]`: Extracts the first element from the `csvs_clean1` list and assigns it to the `corresp_arr` variable.
# 
# 3. **Adding Contact Counts:**
# 
#    - `corresp_arr_contact_cnts <- add_count_contacts(corresp_arr)`: Calls the `add_count_contacts` function to add a new column (`contact_freq`) to the correspondence data frame, indicating the frequency of contacts associated with each vessel.
# 
# 4. **Finding Column Names:**
# 
#    - `createdon_field_name <- find_col_name(corresp_arr, "created", "on")[1]`: Searches for the column name related to the creation date ("created on") within the correspondence data frame (`corresp_arr`) using the `find_col_name` function. It retrieves the first match and assigns it to `createdon_field_name`.
# 
#    - `contactdate_field_name <- find_col_name(corresp_arr, "contact", "date")[1]`: Searches for the column name related to the contact date ("contact date") within the correspondence data frame (`corresp_arr`). It retrieves the first match and assigns it to `contactdate_field_name`.
# 
# 5. **Converting Columns to Dates:**
# 
#    - `corresp_arr_contact_cnts <- change_to_dates(corresp_arr_contact_cnts, createdon_field_name)`: Calls the `change_to_dates` function to convert the column identified by `createdon_field_name` to date format in the `corresp_arr_contact_cnts` data frame.
# 
#    - `corresp_arr_contact_cnts <- change_to_dates(corresp_arr_contact_cnts, contactdate_field_name)`: Calls the `change_to_dates` function again to convert the column identified by `contactdate_field_name` to date format in the `corresp_arr_contact_cnts` data frame.
# 
# 6. **Return Statement:**
# 
#    - `return(corresp_arr_contact_cnts)`: Returns the processed correspondence data frame (`corresp_arr_contact_cnts`) as the output of the function.
# 
# 

 corresp_cleaning  <-  function (corresp_df) 
{
    corresp_df_contact_cnts <- add_count_contacts(corresp_df)
    createdon_field_name <- find_col_name(corresp_df, 
        "created", "on")[1]
    contactdate_field_name <- find_col_name(corresp_df, 
        "contact", "date")[1]
    corresp_df_contact_cnts <- change_to_dates(corresp_df_contact_cnts, 
        createdon_field_name)
    corresp_df_contact_cnts <- change_to_dates(corresp_df_contact_cnts, 
        contactdate_field_name)
    return(corresp_df_contact_cnts)
}
# >>>>

# <<<<

# Explanations for the following code:

# This function takes a list of data frames, cleans each data frame by extracting specific field names, and converts the corresponding columns to POSIXct date format.
# 
#  
#  
# 
# 1. **Initialization of Variables:**
# 
#    - `compl <- compl_arr`: This line initializes a variable `compl` with the same value as `compl_arr`, which represents a list of compliance data frames.
# 
# 2. **Data Cleaning with `map`:**
# 
#    - `compl_clean <- map(compl, clean_weeks)`: Here, each data frame in the list `compl` undergoes cleaning using the `clean_weeks` function. The `map` function applies `clean_weeks` to each element of `compl`, returning a list of cleaned data frames stored in `compl_clean`.
#    
#    The clean_weeks function splits week column ("52: 12/26/2022 - 01/01/2023") into 3 columns with proper classes, week_num (week order number), week_start and week_end.
# 
# 3. **Column Name Extraction with `map`:**
# 
#    - `permitgroupexpirations <- map(compl, ...`: This line iterates over each data frame in `compl` using `map`.
# 
#    - `(x) { .. }`: An anonymous function is defined to operate on each data frame `x`.
# 
#    - Within the anonymous function:
# 
#       - `grep("permit.*group.*expiration", tolower(names(x)), value = TRUE)`: This function call searches for column names containing the pattern "permit", "group", and "expiration" (case-insensitive) within the names of the current data frame `x`. It returns the matching column names.
# 
# 4. **Data Transformation with `imap`:**
# 
#    - `compl_dates <- compl_clean |>`: The cleaned data frames in `compl_clean` are processed further using the pipe (`|>`) operator.
# 
#    - `imap((x, idx) { .. }`: The `imap` function iterates over each element of `compl_clean`, providing both the element (`x`, a data frame) and its index (`idx`).
# 
#    - Within the `imap` function:
# 
#       - `field_name <- permitgroupexpirations[[idx]]`: The variable `field_name` is assigned the column name extracted earlier, corresponding to the current data frame `x`.
# 
#       - `mutate({{field_name}} := ...)`: This line adds a new column to the data frame `x`, where the column name is dynamically determined by `field_name`.
# 
#       - `as.POSIXct(pull(x[field_name]), format = "%m/%d/%Y")`: The values of the selected column are converted to POSIXct date-time format using the specified date format.
# 
# 5. **Return Statement:**
# 
#    - `return(compl_dates)`: Finally, the function returns a list of data frames (`compl_dates`), where each data frame has undergone the necessary cleaning and transformation steps.
# 
# 
# 

 compliance_cleaning  <-  function (compl_arr) 
{
    compl <- compl_arr
    compl_clean <- purrr::map(compl, clean_weeks)
    permitgroupexpirations <- purrr::map(compl, function(x) {
        grep("permit.*group.*expiration", tolower(names(x)), 
            value = TRUE)
    })
    compl_dates <- purrr::imap(compl_clean, function(x, idx) {
        field_name <- permitgroupexpirations[[idx]]
        dplyr::mutate(x, `:=`({
            {
                field_name
            }
        }, as.POSIXct(dplyr::pull(x[field_name]), format = "%m/%d/%Y")))
    })
    return(compl_dates)
}
# >>>>
```

This section handles the processing of compliance reports and correspondence data from FHIER

It reads Compliance and Correspondence CSV files, cleans them by trimming vessel IDs and cleaning column names, and processes correspondence data to add a column indicating if a contact was made. The file paths and the processing logic differ based on the user running the code.

Read correspondence and compliance csvs

Load CSV files into a list, treating all columns as character type


```{r}
csv_contents <- 
  lapply(all_csv_full_paths_list, 
         readr::read_csv, 
         col_types = readr::cols(.default = 'c'))
```

Clean all CSVs: Trim vessel IDs and clean column names

Apply cleaning functions to standardize data across all CSV files

Replace all non-alphanumeric characters with underscores ('_'), unify the case


```{r}
csvs_clean1 <- 
  clean_all_csvs(csv_contents)
```

Every time processing for Compliance and Correspondence downloaded from FHIER

For correspondence:

Extract the first element (correspondence data) from the cleaned CSV list.

Add a new column named "was_contacted", which indicates whether a contact was made with each vessel based on the presence of a contact date. If the contact date is missing (`NA`), it assigns "no"; otherwise, it assigns "yes".

- The `add_count` function is then used to count the number of contacts per vessel, distinguishing between vessels that were contacted and those that were not. The result is stored in a new column named "contact_freq".

Change to date format `created_on` and `contact_date` fields

Clean and process correspondence data, adding contact information and frequency


```{r}
corresp_contact_cnts_clean0 <- 
  csvs_clean1[[1]] |> 
  corresp_cleaning()

# Display the structure of the cleaned correspondence data
head(corresp_contact_cnts_clean0) |> 
  dplyr::glimpse()
```

For compliance:

Clean and process compliance data for all years

Use all dataframes from the csvs_clean1 list except the first (correspondence)


```{r}
compl_clean_list <-
  csvs_clean1[2:length(csvs_clean1)] |>
  compliance_cleaning()
```

Assign analysis years as names to the compliance data frames for easy reference

```{r}
names(compl_clean_list) <- c(my_year1, my_year2)
```

Check the size of each cleaned compliance data frame to ensure proper data loading and processing

```{r}
purrr::map(compl_clean_list, dim)
```

Example result for dimensions check

```{r}
# $`2023`
# [1] 149731     20
# 
# $`2024`
# [1] 71350    20
```

This step merges the cleaned compliance data from multiple years into a single dataset for easier analysis

```{r}
compl_clean <-
  rbind(compl_clean_list[[my_year1]], compl_clean_list[[my_year2]])
```

Check dimensions of the combined compliance data frame
This helps verify the size of the merged dataset and ensure all data was combined correctly

```{r}
dim(compl_clean)
# [1] 221081     20
```

Check dimensions of the cleaned correspondence data frame

```{r}
dim(corresp_contact_cnts_clean0)
# [1] 34549    22
```

### Get Metrics Tracking (permits from FHIER) 

```{r Get Metrics Tracking permits from FHIER}
## Get Metrics Tracking (permits from FHIER) ----
```

The metrics tracking data contains permit information from FHIER (For-Hire Integrated Electronic Reporting Program)

It is processed using a separate script (processing_metrics_tracking.R) stored on Google Drive. Prepare a new file every time.
 
Read the processed metrics tracking files for all years

```{r}
processed_metrics_tracking_permits <-
  purrr::map_df(processed_metrics_tracking_file_names,
         readr::read_rds)
```

Convert column names to lowercase for consistency

This ensures uniform naming conventions across different datasets


```{r}
names(processed_metrics_tracking_permits) <-
  names(processed_metrics_tracking_permits) |>
  tolower()
```

Example of column names

```{r}
# [1] "vessel_official_number, vessel_name, effective_date, end_date, permits, sa_permits_, gom_permits_, permit_region, permit_sa_gom_dual"
```

Check dimensions of the processed metrics tracking data frame

```{r}
dim(processed_metrics_tracking_permits)
```

An example 

```{r}
# [1] 9977    9
```

### Physical Address List from FHIER 

```{r Physical Address List from FHIER}
## Physical Address List from FHIER ----
```


Download first from REPORTS / For-hire Primary Physical Address List.

Load FHIER addresses from the provided path

This dataset contains physical address information for for-hire vessels.

Read all columns as characters.

Use the same column names convention.


```{r}
fhier_addresses <-
  readr::read_csv(fhier_addresses_path,
           col_types = readr::cols(.default = 'c'),
           name_repair = fix_names)
```

Check dimensions of the loaded FHIER addresses data frame

```{r}
dim(fhier_addresses)
# Example result: [1] 3386    7
```

## PIMS 

```{r PIMS}
# PIMS ----
```


Load processed PIMS (Permit Information Management System) home port data

### Home port processed city and state 

```{r Home port processed city and state}
## Home port processed city and state ----
```

This dataset contains information about vessel home ports, including city and state


```{r}
processed_pims_home_ports <- 
  readr::read_csv(processed_pims_home_ports_path)

# Example
dim(processed_pims_home_ports)
# [1] 23303     3
```

## Load from Oracle db 

```{r Load from Oracle db}
# Load from Oracle db ----
```

### Get owners addresses 

```{r Get owners addresses}
## Get owners addresses ----
# <<<<

# Explanations for the following code:

# This function encapsulates the process of connecting to the "SECPR" Oracle database securely by retrieving credentials from the keyring and establishing a connection using the ROracle package. It ensures that sensitive information such as passwords is not exposed in the code, enhancing security.
#  
#  
# This `connect_to_secpr` function establishes a connection to an Oracle database named "SECPR" using credentials stored securely.
# 
# 1. **Function Definition:**
#    - `connect_to_secpr <- function() { .. }`: Defines a function named `connect_to_secpr` with no input parameters.
# 
# 2. **Retrieve Username:**
#    - `my_username <- keyring::key_list("SECPR")[1, 2]`: Retrieves the username associated with the Oracle database "SECPR" from the keyring package. It accesses the first entry (row) and the second column, assuming the username is stored in the second column.
# 
# 3. **Establish Database Connection:**
#    - `con <- dbConnect(...)`:
#      - `dbDriver("Oracle")`: Creates a database driver object specifically for Oracle databases.
#      - `username = my_username`: Specifies the retrieved username for the database connection.
#      - `password = keyring::key_get("SECPR", my_username)`: Retrieves the corresponding password for the username from the keyring package, ensuring secure access to the credentials.
#      - `dbname = "SECPR"`: Specifies the name of the Oracle database to connect to, which is "SECPR" in this case.
#      - `dbConnect(...)`: Establishes a connection to the Oracle database using the provided driver, username, password, and database name.
# 
# 4. **Return Statement:**
#    - `return(con)`: Returns the established database connection (`con`) as the output of the function.
# 

 connect_to_secpr  <-  function () 
{
    my_username <- keyring::key_list("SECPR")[1, 2]
    con <- DBI::dbConnect(DBI::dbDriver("Oracle"), username = my_username, 
        password = keyring::key_get("SECPR", my_username), dbname = "SECPR")
    return(con)
}
# >>>>

# <<<<

# Explanations for the following code:

# This function, `read_rds_or_run`, is designed to read data from an RDS file if it exists or run a specified function to obtain the data from the Oracle database and save it as an RDS file if the file does not exist or if the `force_from_db` parameter is set.
#  
#  
# RDS (R Data Serialization) files are a common format for saving R objects in RStudio, and they allow you to preserve the state of an object between R sessions. Saving your R object as an RDS file in R can be useful for sharing your work with others, replicating your analysis, or simply storing your work for later use.
# 
# 1. **Check File Existence**: The function first checks if the file specified by `my_file_path` exists and, if so, retrieves its modification time.
# 
# 2. **Read or Run**: Depending on the existence of the file and the `force_from_db` flag:
#     - **File Exists and `force_from_db` is not set**: If the file exists and `force_from_db` is not set, the function reads the data from the RDS file using `readr::read_rds(my_file_path)` and assigns it to `my_result`.
#     - **File Does Not Exist or `force_from_db` is set**: If the file does not exist or `force_from_db` is set, the function follows these steps:
#         - Prints a message indicating the file doesn't exist and data will be pulled from the database.
#         - Times the function execution using `tictoc::tic()` and starts with a message indicating the date and purpose of the run.
#         - Runs the specified function (`my_function`) on the provided `my_data` to generate the result (`my_result`), e.g., downloading data from the Oracle database.
#         - Stops timing the function execution using `tictoc::toc()`.
#         - Saves the result as an RDS file to the specified `my_file_path` for future use using `readr::write_rds(my_result, my_file_path)`. A `try` block is used to handle potential errors in writing the file.
#         - Prints a message indicating that the new data is being saved into a file.
# 
# 3. **Print File Information**: After obtaining the data, the function prints the file name and modification time to provide information on when the data was last downloaded or modified.
# 
# 4. **Return**: The function returns the generated or read data (`my_result`).
# 
# 

 read_rds_or_run  <-  function (my_file_path, my_data = as.data.frame(""), my_function, 
    force_from_db = NULL) 
{
    if (file.exists(my_file_path)) {
        modif_time <- file.info(my_file_path)$mtime
    }
    if (file.exists(my_file_path) & is.null(force_from_db)) {
        function_message_print("File already exists, reading.")
        my_result <- readr::read_rds(my_file_path)
    }
    else {
        function_message_print(c("File", my_file_path, "doesn't exists, pulling data from database.", 
            "Must be on VPN."))
        msg_text <- paste(today(), "run for", basename(my_file_path))
        tictoc::tic(msg_text)
        my_result <- my_function(my_data)
        tictoc::toc()
        function_message_print(c("Saving new data into a file here: ", 
            my_file_path))
        try(readr::write_rds(my_result, my_file_path))
        modif_time <- date()
    }
    my_file_name <- basename(my_file_path)
    function_message_print(stringr::str_glue("File: {my_file_name} modified {modif_time}"))
    return(my_result)
}
# >>>>

# <<<<

# Explanations for the following code:

# This function effectively removes columns from the input data frame `my_df` that contain only missing values.
#  
#  
# 1. **Function Definition:**
#    - `remove_empty_cols <- function (my_df)`: Defines a function named `remove_empty_cols` that takes a single argument `my_df`, which is expected to be a data frame.
# 
# 2. **Inner Function Definition:**
#    - `not_all_na <- function(x) any(!is.na(x))`: Defines an inner function named `not_all_na`. This function takes a vector `x` as input and returns `TRUE` if there is at least one non-missing value in the vector, otherwise it returns `FALSE`. This function will be used as a predicate to check if any column contains non-missing values.
# 
# 3. **Selecting Columns:**
#    - `select(my_df, where(not_all_na))`: Uses the `dplyr` function `select` to filter columns of `my_df` based on a condition. The condition is specified using the `where` function, which applies the `not_all_na` function to each column of `my_df`. Columns for which `not_all_na` returns `TRUE` (i.e., columns with at least one non-missing value) are retained, while columns with all missing values are removed.
# 
# 4. **Returning Result:**
#    - `%>% return()`: Pipes the result of the `select` operation into the `return` function, which ensures that the modified data frame is returned as the output of the `remove_empty_cols` function.
# 

 remove_empty_cols  <-  function (my_df) 
{
    not_all_na <- function(x) any(!is.na(x))
    dplyr::select(my_df, tidyselect::where(not_all_na)) %>% return()
}
# >>>>

```

Create parameters for `read_rds_or_run` function to read or download "participants address"


```{r}
# Define the SQL query to fetch participant address data
db_participants_address_query <-
  "select * from
SRH.MV_SERO_VESSEL_ENTITY@secapxdv_dblk
"

# Set the file path for storing or reading the participant address data

# It uses the predefined path to the input directory and a file name to read or write to.
db_participants_address_file_path <-
  file.path(current_project_input_path,
            "db_participants_address.rds")
```


Attempt to establish a connection to Oracle database

Print an error message if no connection, but keep running the code.

```{r}
try(con <- connect_to_secpr())
```

Define a parameter for a function to fetch participant address data from the database

```{r}
db_participants_address_fun <-
  function(db_participants_address) {
    # browser() # Commented out browser function for debugging
    return(dbGetQuery(con,
                      db_participants_address))
  }
```


Fetch or load participant address data, clean it, and prepare for analysis, using the parameters.

Read the file with db_participants_address if exists, 

load from the Oracle database if not,

remove empty columns,

change column names the same way as everything else.


```{r}
db_participants_address <-
  read_rds_or_run(
    db_participants_address_file_path,
    db_participants_address_query,
    db_participants_address_fun,
    #' If you want to update the existing file, change the NULL to "yes" 
    force_from_db = NULL
  ) |>
  remove_empty_cols() |>
  clean_headers()
```

## Data from the previous results of "egregious violators for investigation" 

```{r Data from the previous results of egregious violators for investigation}
# Data from the previous results of "egregious violators for investigation" ----
# <<<<

# Explanations for the following code:

# This function reads data from an Excel file, optionally starting from a specified row, and applies header cleaning to ensure consistent formatting of column names.
#  
#  
# 1. **Function Definition:**
#    - `my_read_xlsx <- function(file_path, sheet_n, start_row = 1) { ... }`: Defines a function named `my_read_xlsx` with three arguments: `file_path` (the path to the Excel file), `sheet_n` (the index of the sheet to read), and `start_row` (the starting row from which to read data, with a default value of 1).
# 
# 2. **Reading Excel File:**
#    - `read.xlsx(file_path, sheet_n, startRow = start_row, detectDates = TRUE, colNames = TRUE, sep.names = "_")`: Uses the `read.xlsx` function from the `openxlsx` package to read data from the specified Excel file.
#      - `file_path`: The path to the Excel file to be read.
#      - `sheet_n`: The index of the sheet to read from.
#      - `startRow`: The row number from which to start reading data.
#      - `detectDates`: A logical value indicating whether to automatically detect and convert date-like columns to date format.
#      - `colNames`: A logical value indicating whether the first row of the Excel sheet contains column names.
#      - `sep.names`: A character used to separate column names with multiple parts (e.g., spaces or underscores).
# 
# 3. **Cleaning Headers:**
#    - `clean_headers(...)`: Applies the `clean_headers` function to the data frame read from the Excel file. This function performs tasks such as removing leading/trailing spaces, converting column names to lowercase, and replacing spaces with underscores.
# 
# 4. **Return Result:**
#    - `return(res_df)`: Returns the cleaned data frame obtained from reading the Excel file. This data frame contains the data read from the specified Excel sheet, with any necessary header cleaning applied.
# 
# 

 my_read_xlsx  <-  function (file_path, sheet_n, start_row = 1) 
{
    res_df <- clean_headers(openxlsx::read.xlsx(file_path, sheet_n, 
        startRow = start_row, detectDates = TRUE, colNames = TRUE, 
        sep.names = "_"))
    return(res_df)
}
# >>>>
```

The following section deals with data from previous "egregious violators for investigation" results

Instructions for retrieving previous results

There are 2 functions,

get_previous_result_from_local_file() assumes you have downloaded the previous results and 

get_previous_result_from_google_drive() gets data directly from Google drive

Run only one of them and save the dataframe in `prev_result` variable. 

Function to retrieve and process previous results from a downloaded file

```{r}
get_previous_result_from_local_file <- function() {
  
  # Download first as .xlsx from Google drive
  
  # Read,
  # remove empty columns,
  # change column names the same way as everything else.
  prev_result0 <-
    my_read_xlsx(prev_result_path) |>
    remove_empty_cols() |>
    clean_headers()
  
  # An example
  dim(prev_result0)
  # [1] 151  42
  
  # clean excel number conversions, remove ".0" at the end
  prev_result <-
    prev_result0 |>
    dplyr::mutate(vessel_official_number =
                    stringr::str_replace(vessel_official_number, "\\.0$", ""))
  
  return(prev_result)
}
```

Function to retrieve and process previous results directly from Google Drive

```{r}
get_previous_result_from_google_drive <- function() {
  
  previous_result_google_ss_name <-
    # Takes the base name of the file from `prev_result_path` (removes directory path)
    basename(prev_result_path) |>
    # Removes the file extension from the base name
    tools::file_path_sans_ext()
  
  # When asked for the authentication the first time choose the appropriate option and follow the instructions. If you are writing again in the same R session you can choose the option 2 and it will confirm your access automatically.
  # We assuming that there is only one file with that name in your Google drive.
  
  my_previous_ss <- googlesheets4::gs4_find(previous_result_google_ss_name, n_max = 1)
  
  # Load it to R.
  # And clean it as usual, changing headers to lower case with underscores and removing empty columns
  previous_result <-
    googlesheets4::read_sheet(my_previous_ss) |>
    remove_empty_cols() |>
    clean_headers()
  
  return(previous_result)
}
```

Manually: Un-comment and run one of the functions.


```{r}
# prev_result <- get_previous_result_from_local_file()
# or
prev_result <- get_previous_result_from_google_drive()
```

## Results 

```{r Results}
# Results ----
# <<<<

# Explanations for the following code:

# This function is designed to print text with a specified title and an optional ending marker. It first prints the title, then the main text, and finally the ending marker.
#  
#  
# 1. **Function Definition:**
#    - `pretty_print <- function(my_text, my_title, the_end = "---") { ... }`: Defines a function named `pretty_print` with three arguments: `my_text`, `my_title`, and `the_end`.
# 
# 2. **Printing the Title:**
#    - `title_message_print(my_title)`: Calls a function `title_message_print` to print the title `my_title`. This function formats and prints the title message.
#    
# 3. **Printing the Text:**
#    - `cat(c(my_text, the_end), sep = "n")`: Concatenates `my_text` and `the_end` into a character vector and prints them using `cat`. Each element is separated by a newline (`n`). This line prints the main text followed by the ending marker.
# 
# 4. **Default Argument:**
#    - `the_end = "---"`: Defines a default value for the argument `the_end`. If the argument is not provided when calling the function, it defaults to `"---"`. This is used as a marker to denote the end of the printed content.
# 

 pretty_print  <-  function (my_text, my_title, the_end = "---") 
{
    title_message_print(my_title)
    cat(c(my_text, the_end), sep = "\n")
}
# >>>>
```

Create a vector of data frame names containing the results


```{r}
results <-
  c(
    "compl_clean",
    "corresp_contact_cnts_clean0",
    "processed_metrics_tracking_permits",
    "fhier_addresses",
    "processed_pims_home_ports",
    "db_participants_address",
    "prev_result"
  )

pretty_print(results, "Data are in:")
```


Data are in:

compl_clean

corresp_contact_cnts_clean0

processed_metrics_tracking_permits

fhier_addresses

processed_pims_home_ports

db_participants_address

prev_result

# Find egregious violators

## Preparing compliance info 

```{r Preparing compliance info}
# Preparing compliance info ----
```

### Permit Expiration 

```{r Permit Expiration}
## Permit Expiration ----
```

#### Add permit_expired column 

```{r Add permit_expired column}
### Add permit_expired column ----
```

Explainations for the following code:

1. Add a new column 'permit_expired' using 'mutate'.

2. Use 'case_when' to determine if 'permit_groupexpiration' is greater than permit_expired_check_date defined earlier.

3. If true, set 'permit_expired' to "no", otherwise set it to "yes".


```{r}
compl_clean_w_permit_exp <-
  compl_clean |>
  # if permit group expiration is after permit_expired_check_date than "not expired"
  dplyr::mutate(permit_expired =
           dplyr::case_when(
             permit_groupexpiration > permit_expired_check_date ~ "no",
             .default = "yes"
           ))

# check
# dplyr::glimpse(compl_clean_w_permit_exp)
```

#### Get only not expired last 27 weeks of data minus grace period (total 26 weeks) 

```{r Get only not expired last 27 weeks of data minus grace period total 26 weeks}
### Get only not expired last 27 weeks of data minus grace period (total 26 weeks) ----
```


27 weeks are used to account for a one-week grace period, resulting in 26 weeks of usable data


```{r}
compl_clean_w_permit_exp__not_exp <-
  compl_clean_w_permit_exp |>
  # the last 27 week
  dplyr::filter(week_start > half_year_ago) |>
  # before the last week (a report's grace period)
  dplyr::filter(week_end < last_week_start) |>
  # not expired
  dplyr::filter(tolower(permit_expired) == "no")
```


Check if the dates make sense

```{r}
min(compl_clean_w_permit_exp__not_exp$permit_groupexpiration)
# E.g.
# [1] "2024-08-31 EDT"

min(compl_clean_w_permit_exp__not_exp$week_start)
# E.g.
# [1] "2024-01-08"

max(compl_clean_w_permit_exp__not_exp$week_start)
# [1] "2024-06-17"

max(compl_clean_w_permit_exp__not_exp$week_end)
# [1] "2024-06-23"
```

#### Add year_month column from week_start 

```{r Add year_month column from week_start}
### Add year_month column from week_start ----

# as.yearmon converts dates to a year-month format (e.g., "Jan 2024")
compl_clean_w_permit_exp_last_half_year <-
  compl_clean_w_permit_exp__not_exp |>
  dplyr::mutate(year_month = zoo::as.yearmon(week_start)) |>
  # keep entries for the current check period only
  dplyr::filter(year_month >= zoo::as.yearmon(half_year_ago))
```


Compare dimensions to verify the filtering has reduced the dataset as expected

```{r}
dim(compl_clean_w_permit_exp)
# [1] 221081     21

dim(compl_clean_w_permit_exp_last_half_year)
# [1] 57296    22
```

#### Have only SA and dual permits 

```{r Have only SA and dual permits}
### Have only SA and dual permits ----
```

This section filters the data to include only SA and dual permits.

Filter rows where 'permitgroup' contains "CDW", "CHS", or "SC"


```{r}
compl_clean_w_permit_exp_last_half_year__sa <-
  compl_clean_w_permit_exp_last_half_year |>
  dplyr::filter(grepl("CDW|CHS|SC", permitgroup))

# Check the dimensions of the resulting dataframe
dim(compl_clean_w_permit_exp_last_half_year__sa)
# [1] 38761    22
```

#### Keep fewer columns in the compliance df 

```{r Keep fewer columns in the compliance df}
### Keep fewer columns in the compliance df ----
```

Define a vector of column names to be removed from the compliance dataframe

```{r}
remove_columns_from_compliance <- c(
  "name",
  "gom_permitteddeclarations__",
  "captainreports__",
  "negativereports__",
  "complianceerrors__",
  "set_permits_on_hold_",
  "override_date",
  "override_by",
  "contactedwithin_48_hours_",
  "submittedpower_down_",
  "permit_expired"
)
```


Remove specified columns and keep only unique rows


```{r}
compl_clean_w_permit_exp_last_half_year__sa__short <-
  compl_clean_w_permit_exp_last_half_year__sa |>
  dplyr::select(-tidyselect::any_of(remove_columns_from_compliance)) |> 
  dplyr::distinct()
```

Check the dimensions of the resulting dataframe

```{r}
dim(compl_clean_w_permit_exp_last_half_year__sa__short)
# [1] 38761    11
```


Work with the whole period now

#### Add compliant_after_overr 

```{r Add compliant_after_overr}
### Add compliant_after_overr ----
# <<<<

# Explanations for the following code:

# Explanations:
# 1. Create a new variable 'res' to store the result.
# 2. Use 'rowwise' to perform operations row by row.
# 3. Use 'mutate' to create a new column 'compliant_after_override' based on conditions specified in 'case_when'.
#    - If 'is_comp' is 0 and 'overridden' is 0, set 'compliant_after_override' to "no".
#    - If 'is_comp' is 1 or 'overridden' is 1, set 'compliant_after_override' to "yes".
#    - If 'is_comp' is NA, set 'compliant_after_override' to NA.
#    - For all other cases, set 'compliant_after_override' to the string representation of 'is_comp'.
# 4. Use 'ungroup' to remove grouping from the data frame.
#  
#  

 add_compliant_after_override  <-  function (my_compl_df, overridden_col_name = "overridden", compliance_col_name = "is_comp") 
{
    res <- ungroup(dplyr::mutate(dplyr::rowwise(my_compl_df), 
        compliant_after_override = dplyr::case_when(!!rlang::sym(compliance_col_name) %in% 
            c(0, "NO") & !!rlang::sym(overridden_col_name) %in% c(0, 
            "NO") ~ "no", !!rlang::sym(compliance_col_name) %in% c(1, 
            "YES") ~ "yes", !!rlang::sym(overridden_col_name) %in% c(1, 
            "YES") ~ "yes", is.na(!!rlang::sym(compliance_col_name)) ~ 
            NA, .default = toString(!!rlang::sym(compliance_col_name)))))
    return(res)
}
# >>>>
```

use tictoc package for benchmarking 

Apply the add_compliant_after_override function to add a new column indicating compliance status after overrides


```{r}
tictoc::tic("compl_overr")
compl_clean_w_permit_exp_last_half_year__sa__short__comp_after_overr <-
  compl_clean_w_permit_exp_last_half_year__sa__short |>
  add_compliant_after_override(overridden_col_name = "overridden_",
                                             compliance_col_name = "compliant_")
tictoc::toc()
# compl_overr: 8.76 sec elapsed
```


check compliant/overridden combinations' counts

```{r}
compl_clean_w_permit_exp_last_half_year__sa__short__comp_after_overr |> 
  dplyr::select(compliant_, overridden_, compliant_after_override) |>
  dplyr::count(compliant_, overridden_, compliant_after_override)
# E.g.
#   compliant_ overridden_ compliant_after_override     n
#   <chr>      <chr>       <chr>                    <int>
# 1 NO         NO          no                       10768
# 2 NO         YES         yes                        199
# 3 YES        NO          yes                      27794
```


Verify that the compliant_after_override column contains only "yes" and "no" values

```{r}
compl_clean_w_permit_exp_last_half_year__sa__short__comp_after_overr$compliant_after_override |>
  unique() == c("yes", "no")

dim(compl_clean_w_permit_exp_last_half_year__sa__short__comp_after_overr)
# E.g.
# [1] 38761    12
```


Ensure that the number of unique vessels remains the same after data transformations


```{r}
dplyr::n_distinct(compl_clean_w_permit_exp_last_half_year__sa$vessel_official_number) ==
  dplyr::n_distinct(
    compl_clean_w_permit_exp_last_half_year__sa__short__comp_after_overr$vessel_official_number
  )
# T
```

#### Get only non-compliant entries for the past half year 

```{r Get only noncompliant entries for the past half year}
### Get only non-compliant entries for the past half year ----
compl_clean_w_permit_exp_last_half_year__sa_non_c <-
  compl_clean_w_permit_exp_last_half_year__sa__short__comp_after_overr |>
  # not compliant
  dplyr::filter(tolower(compliant_after_override) == "no")

# check
dim(compl_clean_w_permit_exp_last_half_year__sa_non_c)
# E.g.
# [1] 10768    12
```

#### Keep only vessels with info for all weeks in the period 

```{r Keep only vessels with info for all weeks in the period}
### Keep only vessels with info for all weeks in the period ----
```


That should eliminate entries for vessels having permits only a part of the period

Calculate the total number of distinct weeks in the dataset


```{r}
all_weeks_num <-
  compl_clean_w_permit_exp_last_half_year__sa_non_c |>
  dplyr::select(week) |>
  dplyr::distinct() |>
  nrow()
```


Explainations for the following code:

1. Group the data frame by 'vessel_official_number'.

2. Filter the groups based on the condition that the number of distinct weeks is greater than or equal to 'all_weeks_num'.

3. Remove the grouping from the data frame.

4. Exclude the 'week' column from the resulting data frame, we don't need it anymore.


```{r}
compl_clean_w_permit_exp_last_half_year__sa_non_c__all_weeks_present <-
  compl_clean_w_permit_exp_last_half_year__sa_non_c |>
  dplyr::group_by(vessel_official_number) |>
  dplyr::filter(dplyr::n_distinct(week) >= all_weeks_num) |> 
  dplyr::ungroup() |> 
  dplyr::select(-week)
```


Check how many entries were removed

```{r}
dim(compl_clean_w_permit_exp_last_half_year__sa_non_c)
# E.g.
# [1] 10768    12

dim(compl_clean_w_permit_exp_last_half_year__sa_non_c__all_weeks_present)
# E.g.
# [1] 3278   11
```

#### Check the last report date 

```{r Check the last report date}
### Check the last report date ----
```

##### Get ids only 

```{r Get ids only}
#### Get ids only ----
```

Create a dataframe with unique vessel official numbers

```{r}
compl_clean_w_permit_exp_last_half_year__sa_non_c__all_weeks_present__vesl_ids <-
  compl_clean_w_permit_exp_last_half_year__sa_non_c__all_weeks_present |>
  dplyr::select(vessel_official_number) |>
  dplyr::distinct()
```


check vessel's number

```{r}
dim(compl_clean_w_permit_exp_last_half_year__sa_non_c__all_weeks_present__vesl_ids)
# E.g.
# 149 
```

##### Check these ids in the full compliance information 

```{r Check these ids in the full compliance information}
#### Check these ids in the full compliance information ----
```

Check if there are any new submitted reports for previously non-compliant vessels


```{r}
compl_clean_w_permit_exp_last_half_year__sa |>
  dplyr::filter(
    vessel_official_number %in% compl_clean_w_permit_exp_last_half_year__sa_non_c__all_weeks_present__vesl_ids$vessel_official_number
  ) |>
  dplyr::group_by(vessel_official_number) |>
  dplyr::filter(
    tolower(compliant_) == "yes" &
      tolower(overridden_) == "yes" &
      # not the current month
      year_month < zoo::as.yearmon(data_file_date)
  ) |>
  nrow()
```

A result of 0 indicates no new compliant reports have been submitted for selected vessels

End of Compliance preparations 

Results: processed Compliance is in `compl_clean_w_permit_exp_last_half_year__sa_non_c__all_weeks_present`

## Preparing Correspondence 

```{r Preparing Correspondence}
# Preparing Correspondence ----
```

### Remove 999999 

```{r Remove 999999}
## Remove 999999 ----
```


Remove vessels with official numbers starting with "99999" as they are placeholder or test entries


```{r}
corresp_contact_cnts_clean <-
  corresp_contact_cnts_clean0 |>
  dplyr::filter(!grepl("^99999", vessel_official_number))
```


check number of vessels in correspondence 

```{r}
dplyr::n_distinct(corresp_contact_cnts_clean$vesselofficial_number)
```

E.g.
4281

check call type, voicemail and contact type combinations

```{r}
corresp_contact_cnts_clean |>
  dplyr::select(calltype, voicemail, contacttype) |>
  dplyr::distinct() |> head(10)
```

### Correspondence Filters 

```{r Correspondence Filters}
## Correspondence Filters ----
```


This section defines various filters for correspondence data using quosures.

Quosures are a part of tidy evaluation in R, allowing expressions to be captured without evaluation, which is useful for creating functions with flexible inputs.

Define a filter to check if any entry was an outgoing call


```{r}
we_called_filter <-
  dplyr::quo(any(tolower(contacttype) == "call" &
        tolower(calltype) == "outgoing"))
```


Define a filter to check if any entry was an outgoing email or other contact type


```{r}
we_emailed_once_filter <-
  dplyr::quo(any(
    tolower(contacttype) %in% c("email", "other") &
      tolower(calltype) == "outgoing"
  ))
```


Explainations for the following code:

**Expression inside quo()**:

   - `!grepl("No contact made", contactcomments, ignore.case = TRUE)`: This expression is a negation of the `grepl` function, which is used to search for a pattern ("No contact made") in the `contactcomments` column.

   - `grepl()` returns `TRUE` for each element of `contactcomments` that contains the pattern, and `FALSE` otherwise.

   - The `!` operator negates the result, so the filter condition will be `TRUE` for rows where "No contact made" is not found in the `contactcomments` column.

The `exclude_no_contact_made_filter` function creates a filter condition to exclude rows where "No contact made" is found in the `contactcomments` column.


```{r}
exclude_no_contact_made_filter <-
  dplyr::quo(!grepl("No contact made", 
            contactcomments, 
            ignore.case = TRUE))
```


Don't need a second contact if any contact was incoming

Define a filter to check if any contact was incoming


```{r}
they_contacted_direct_filter <-
  dplyr::quo(
    any(
      tolower(calltype) == "incoming"
      )
  )
```

#### Use the filters 

```{r Use the filters}
### Use the filters ----
```

Apply filters to create a subset of correspondence data meeting specific criteria

Explainations for the following code:

- `corresp_contact_cnts_clean |> ...` starts the pipeline with the data frame `corresp_contact_cnts_clean`.

- `dplyr::filter(tolower(calltype) == "incoming" | ...)` filters rows based on the conditions specified:

  - `dplyr::filter()` is used to subset rows in the data frame.

  - `tolower(calltype) == "incoming"` converts the `calltype` column to lowercase and checks if it equals "incoming".

  - The `|` operator means logical OR, so it includes rows where the condition on either side is true.

  - `(contact_freq > 1 & (!!we_called_filter & !!we_emailed_once_filter))`:

    - `contact_freq > 1` checks if the `contact_freq` column is greater than 1.

    - `&` is the logical AND operator, so it requires both conditions to be true.

    - `!!we_called_filter` evaluates the `we_called_filter` variable as a logical expression.

    - `!!we_emailed_once_filter` evaluates the `we_emailed_once_filter` variable as a logical expression.

    - The parentheses around this subexpression ensure it is evaluated as a single logical unit.

- `dplyr::filter(!!exclude_no_contact_made_filter)` applies another filter condition:

  - `!!exclude_no_contact_made_filter` evaluates the `exclude_no_contact_made_filter` variable as a logical expression.

This code filters the `corresp_contact_cnts_clean` data frame to include rows where either the `calltype` is "incoming" or the `contact_freq` is greater than 1 and both `we_called_filter` and `we_emailed_once_filter` are true. Additionally, it filters rows based on the `exclude_no_contact_made_filter` condition.


```{r}
corresp_contact_cnts_clean_direct_cnt_2atmps <-
  corresp_contact_cnts_clean |>
  dplyr::filter(tolower(calltype) == "incoming" |
           (
             contact_freq > 1 &
               (!!we_called_filter &
                  !!we_emailed_once_filter)
           )) |> 
  dplyr::filter(!!exclude_no_contact_made_filter)
```


Check the dimensions of the original and filtered datasets


```{r}
dim(corresp_contact_cnts_clean)
# E.g. [1] 33001    22
dim(corresp_contact_cnts_clean_direct_cnt_2atmps)
# E.g. [1] 31061    22
```


Check how many vessels left after filtering 

```{r}
dplyr::n_distinct(corresp_contact_cnts_clean_direct_cnt_2atmps$vesselofficial_number)
# E.g.
# [1] 3865
```

### Fix dates 

```{r Fix dates}
## Fix dates ----
```

Prepare to clean and standardize date formats

check how the dates look

```{r}
head(corresp_contact_cnts_clean_direct_cnt_2atmps$contact_date, 1) |> str()
# chr "02/15/2024 03:15PM"
```



Explainations for the following code:

Mutate new columns 'created_on_dttm' and 'contact_date_dttm' by parsing 'created_on' and 'contact_date' using lubridate package.

The date-time formats considered are "mdY R".

1. Use the pipe operator to pass 'corresp_contact_cnts_clean_direct_cnt_2atmps' as the left-hand side of the next expression.

2. Use 'mutate' to create new columns with parsed date-time values.

3. Use 'lubridate::parse_date_time' to parse the date-time values using the specified formats.


```{r}
corresp_contact_cnts_clean_direct_cnt_2atmps_clean_dates <-
  corresp_contact_cnts_clean_direct_cnt_2atmps |>
  dplyr::mutate(
    created_on_dttm =
      lubridate::parse_date_time(created_on,
                                 c("mdY R")),
    contact_date_dttm =
      lubridate::parse_date_time(contact_date,
                                 c("mdY R"))
  )
```


check how the dates look now

```{r}
head(corresp_contact_cnts_clean_direct_cnt_2atmps_clean_dates$contact_date_dttm, 1) |> 
  str()
# E.g.
# POSIXct[1:1], format: "2024-06-17 15:24:00"
```


End of Correspondence preparations 

Processed Correspondence is in 
`corresp_contact_cnts_clean_direct_cnt_2atmps_clean_dates`

## Join correspondence with compliance 

```{r Join correspondence with compliance}
# Join correspondence with compliance ----
```


Combine correspondence and compliance data for further analysis.

An inner join is used to combine correspondence and compliance data.

This ensures we only keep vessels that appear in both datasets, effectively filtering for vessels with both compliance and correspondence records

Explainations for the following code:

Create a new dataframe 'compl_corr_to_investigation' by performing an inner join between

'correspondence' and 'compliance'.

The join is performed on the column 'vessel_official_number'.

Use 'multiple = "all"' and 'relationship = "many-to-many"' to handle multiple matches during the join.

1. Use the 'inner_join' function from the dplyr package to combine the two dataframes based on the specified columns.

2. Pass the column names and other parameters to the 'by', 'multiple', and 'relationship' arguments.


```{r}
compl_corr_to_investigation <-
  dplyr::inner_join(
    corresp_contact_cnts_clean_direct_cnt_2atmps_clean_dates,
    compl_clean_w_permit_exp_last_half_year__sa_non_c__all_weeks_present,
    by = c("vessel_official_number"),
    multiple = "all",
    relationship = "many-to-many"
  )
```


Verify the dimensions and content of the joined dataset


```{r}
dim(compl_corr_to_investigation)
# E.g.
# [1] 30844    32

dplyr::n_distinct(compl_corr_to_investigation$vesselofficial_number)
# E.g.
# 141

head(compl_corr_to_investigation) |> 
  dplyr::glimpse()
```

### Store the count of unique vessels 

```{r Store the count of unique vessels}
## Store the count of unique vessels ----
```

For later verification and reporting

```{r}
num_of_vsl_to_investigate <- 
  dplyr::n_distinct(compl_corr_to_investigation$vesselofficial_number)
```


Results: Egregious violators Compliance & Correspondence joined together are in
`compl_corr_to_investigation`

# Prepare output

## Output needed investigation 

```{r Output needed investigation}
# Output needed investigation ----
```


"Investigation" in this context refers to vessels that meet the criteria for egregious violations and require further action.

Steps:

1. Remove unused columns.

2. Create additional columns.

3. Mark vessels already in the know list (prev_result).

4. Duals vs. sa_only

### 1. Remove extra columns 

```{r 1 Remove extra columns}
## 1. Remove extra columns ----
# <<<<

# Explanations for the following code:

# 1. Extract unique non-NA elements from the input vector 'x' using 'unique'.
# 2. Concatenate these unique elements into a single string with ", " as the separator using 'paste0' and 'collapse'.
#  
#  

 concat_unique  <-  function (x) 
{
    paste0(unique(x[!is.na(x)]), collapse = ", ")
}
# >>>>
```

List of columns to be excluded from the final output for simplification and focus on relevant data. Commented are column names to retain.


```{r}
unused_fields <- c(
  "vesselofficial_number",
  "primary",
  # "contact_date",
  "follow_up",
  "log_group",
  "calltype",
  "voicemail",
  # "contacttype",
  "contact_reason",
  # "contactrecipientname",
  # "contactphone_number",
  # "contactemailaddress",
  "contactcomments",
  "srfhuser",
  "created_on",
  "follow_up_nbr",
  "srhs_vessel",
  # "vessel_official_number",
  "was_contacted",
  "contact_freq",
  "created_on_dttm",
  # "contact_date_dttm",
  # "name",
  # "permit_expired",
  # "permitgroup",
  # "permit_groupexpiration",
  "compliant_after_override")
```



Create a simplified version of the investigation data, removing unused fields and concatenating unique values for each vessel

Explainations for the following code:

1. Exclude columns specified in 'unused_fields' from the data frame.

2. Group the data frame by 'vessel_official_number'.

3. `summarise_all` applies the function 'concat_unique' to all columns to concatenate unique non-missing values into a single string.

4. Remove the grouping from the data frame.


```{r}
compl_corr_to_investigation_short <-
  compl_corr_to_investigation |>
  dplyr::select(-any_of(unused_fields)) |>
  dplyr::group_by(vessel_official_number) |>
  dplyr::summarise_all(concat_unique) |>
  dplyr::ungroup()
```


Visual check if data make sense

```{r}
compl_corr_to_investigation_short |> 
  head() |> 
  dplyr::glimpse()
```


Check if number of vessels didn't change

```{r}
nrow(compl_corr_to_investigation_short) == num_of_vsl_to_investigate
```

### 2. Create additional columns 

```{r 2 Create additional columns}
## 2. Create additional columns ----
```

#### Add list of contact dates and contact type in parentheses  

```{r Add list of contact dates and contact type in parentheses}
### Add list of contact dates and contact type in parentheses  ----
# <<<<

# Explanations for the following code:

# This function provides a flexible way to search for column names within a dataframe based on specified starting and ending patterns. It is useful for identifying columns that match a particular naming convention or pattern. 
#  
# The function is designed to search for column names within a dataframe that contain a specific pattern defined by `start_part` and `end_part`.
# 
# 1. **Function Definition:**
#    -`function (mydf, start_part, end_part) { .. }`: Defines a function with three arguments: `mydf` (the dataframe to search), `start_part` (the starting pattern of the column names to search for), and `end_part` (the ending pattern of the column names to search for).
# 
# 2. **Constructing Regular Expression:**
#    - `to_search <- paste0(start_part, ".*", end_part)`: Constructs a regular expression pattern by concatenating `start_part`, ".*" (which matches any characters zero or more times), and `end_part`. This pattern will be used to search for matching column names.
# 
# 3. **Searching for Matching Column Names:**
#    - `matching_names <- grep(to_search, tolower(names(mydf)), value = TRUE)`: Searches for column names in the dataframe `mydf` that match the regular expression pattern `to_search`. The `tolower` function is used to convert the column names to lowercase for case-insensitive matching. The `grep` function returns the matching column names as a vector.
# 
# 4. **Return Statement:**
#    - `return(matching_names)`: Returns the vector of matching column names found in the dataframe that satisfy the specified pattern.
# 

 find_col_name  <-  function (mydf, start_part, end_part) 
{
    to_search <- paste0(start_part, ".*", end_part)
    matching_names <- grep(to_search, tolower(names(mydf)), value = TRUE)
    return(matching_names)
}
# >>>>
```


Define column name variables for flexibility across different data sources.

Spaces and underscores placements vary from source to source.


```{r}
contactdate_field_name <-
  find_col_name(compl_corr_to_investigation_short, "contact", "date")[1]

contacttype_field_name <-
  find_col_name(compl_corr_to_investigation_short, "contact", "type")[1]

contactphonenumber_field_name <-
  find_col_name(compl_corr_to_investigation_short, ".*contact", "number.*")[1]
```


 Function to create a summary of contact dates and types for each vessel
 
Explainations for the following code:

Define a function 'get_date_contacttype' that takes a dataframe 'compl_corr_to_investigation' as input.

1. Add a new column 'date__contacttype' by concatenating the values from 'contactdate_field_name' and 'contacttype'.

2. Select only the 'vessel_official_number' and 'date__contacttype' columns.

3. Arrange the dataframe by 'vessel_official_number' and 'date__contacttype'.

4. Keep distinct rows based on 'vessel_official_number' and 'date__contacttype'.

5. Group the dataframe by 'vessel_official_number'.

6. Summarize the data by creating a new column 'date__contacttypes' that concatenates all 'date__contacttype' values for each vessel separated by a comma.

7. Return the resulting dataframe.


```{r}
get_date_contacttype <-
  function(my_df) {
  
    res <-
      my_df |>
      # Combine contact date and type into a single column

      dplyr::mutate(date__contacttype =
                      paste(
                        !!rlang::sym(contactdate_field_name),
                        !!rlang::sym(contacttype_field_name)
                      )) |>
      # use 2 columns only
      dplyr::select(vessel_official_number, date__contacttype) |>
      dplyr::distinct() |>
      # sort
      dplyr::arrange(vessel_official_number, date__contacttype) |>
      # for each vessel id...
      dplyr::group_by(vessel_official_number) |>
      # ...combine all date__contacttypes separated by comma in one cell
      dplyr::summarise(date__contacttypes =
                         paste(date__contacttype, collapse = ", ")) |> 
      dplyr::ungroup()
    
    return(res)
  }
```


Apply the get_date_contacttype function

```{r}
date__contacttype_per_id <-
  get_date_contacttype(compl_corr_to_investigation_short)
```


Verify that the number of vessels remains consistent

```{r}
nrow(date__contacttype_per_id) == num_of_vsl_to_investigate
```


Display a sample of the resulting data for verification

```{r}
date__contacttype_per_id |>
  head() |>
  dplyr::glimpse()
```

##### Add the new column back 

```{r Add the new column back}
#### Add the new column back ----
# Join the short compliance/correspondence data with date and contact type information
compl_corr_to_investigation__corr_date <-
  dplyr::left_join(compl_corr_to_investigation_short,
            date__contacttype_per_id) |>
  # Joining with `by = join_by(vessel_official_number)`
  # these columns are not longer needed
  dplyr::select(-dplyr::all_of(c(
    contactdate_field_name,
    contacttype_field_name
  )))
```

check, the last column should be like 

$ date__contacttypes     <chr> "03/13/2024 11:59AM, 09/21/2023 03:41PM, 08/18/2023 10:52AM,…

```{r}
compl_corr_to_investigation__corr_date |> 
  head() |> 
  dplyr::glimpse()
```

#### Add pims home port info 

```{r Add pims home port info}
### Add pims home port info ----
```


Rename columns in the processed PIMS home ports data for consistency


```{r}
processed_pims_home_ports_renamed <- 
  processed_pims_home_ports |> 
  dplyr::rename("hailing_port_city" = city_fixed,
         "hailing_port_state" = state_fixed)
```


Combine compliance/correspondence data with hailing port information


```{r}
compl_corr_to_investigation__corr_date__hailing_port <- 
  dplyr::left_join(
    compl_corr_to_investigation__corr_date,
    processed_pims_home_ports_renamed,
    dplyr::join_by(vessel_official_number)
  )
```

#### Add prepared addresses 

```{r Add prepared addresses}
### Add prepared addresses ----

# Define the path to the address preparation script
# This is used only with source()
prep_addresses_path <-
  file.path(current_project_path,
            stringr::str_glue("{current_project_name}_prep_addresses.R"))
```


Check if the file exists.

```{r}
file.exists(prep_addresses_path)
```

Check and add missing addresses

This part aims to gather and clean up address information for vessels from two main sources: FHIER and an Oracle database. It then combines this information to create a more complete set of address data for each vessel.

The main output is a dataset called `compl_corr_to_investigation__corr_date__hailing_port__fhier_addr__db_addr`, which combines vessel information with address data from both FHIER and the Oracle database.

The code starts by cleaning and simplifying the FHIER address data, keeping only the most relevant information. It then retrieves additional address data from an Oracle database to supplement the FHIER data.

It identifies vessels that are missing address information in the FHIER data.

It then retrieves additional address information for these vessels from the Oracle database.

The code cleans and processes this Oracle database information, combining multiple entries for the same vessel when necessary.

Finally, it joins all this information together, creating a more complete dataset with address information from both sources.

## Addresses from FHIER 

```{r Addresses from FHIER}
# Addresses from FHIER ----
```

### Fewer fields 

```{r Fewer fields}
## Fewer fields ----
# <<<<

# Explanations for the following code:

# - This function `clean_names_and_addresses` cleans the strings in character columns of a dataframe.
# - It uses `mutate` with `across` to apply multiple string cleaning operations to all character columns.
# - The `str_squish` function is used to remove leading, trailing, and extra internal whitespace from each string.
# - `replace_na` function replaces missing values with empty strings.
# - The `str_replace_all` function is used to perform multiple replacements using regular expressions to clean up commas, semicolons, and unwanted substrings like "UN".
# - Finally, it returns the cleaned dataframe. 
#  

 clean_names_and_addresses  <-  function (my_df) 
{
    my_df_cleaned <- dplyr::mutate(my_df, dplyr::across(dplyr::where(is.character), 
        ~stringr::str_squish(.x)), dplyr::across(dplyr::where(is.character), 
        ~tidyr::replace_na(.x, "")), dplyr::across(dplyr::where(is.character), 
        ~stringr::str_replace_all(.x, ", ;", ";")), dplyr::across(dplyr::where(is.character), 
        ~stringr::str_replace_all(.x, "\\s+[,;]", ",")), dplyr::across(dplyr::where(is.character), 
        ~stringr::str_replace_all(.x, ";,+", ";")), dplyr::across(dplyr::where(is.character), 
        ~stringr::str_replace_all(.x, ";;+", ";")), dplyr::across(dplyr::where(is.character), 
        ~stringr::str_replace_all(.x, ",,+", ",")), dplyr::across(dplyr::where(is.character), 
        ~stringr::str_replace_all(.x, "[,;] *\\bUN\\b *", "")), 
        dplyr::across(dplyr::where(is.character), ~stringr::str_replace_all(.x, 
            "\\bUN\\b", "")), dplyr::across(dplyr::where(is.character), 
            ~stringr::str_replace_all(.x, "\\s*\\bUN\\b\\s*", 
                "")), dplyr::across(dplyr::where(is.character), 
            ~stringr::str_replace_all(.x, "^[,;] ", "")), dplyr::across(dplyr::where(is.character), 
            ~stringr::str_replace_all(.x, "^[,;]$", "")), dplyr::across(dplyr::where(is.character), 
            ~stringr::str_replace_all(.x, "[,;]$", "")), dplyr::across(dplyr::where(is.character), 
            ~stringr::str_squish(.x)))
    return(my_df_cleaned)
}
# >>>>
```

fhier_addresses are from get_data (For-hire Primary Physical Address List)

Select relevant columns from FHIER addresses data


```{r}
fhier_addr_short <-
  fhier_addresses |>
  dplyr::select(
    vessel_official_number,
    permit_holder_names,
    physical_address_1,
    physical_address_2,
    physical_city,
    physical_county,
    physical_state,
    physical_zip_code,
    phone_number,
    primary_email
  )
```

Clean and standardize FHIER address data, removing duplicates

```{r}
fhier_addr_short_clean <-
  fhier_addr_short |>
  clean_names_and_addresses() |>
  dplyr::distinct()

# nrow(fhier_addr_short_clean)
```

Address combination code is commented out to preserve individual address fields

```{r}
# fhier_addr_short__comb_addr <-
#   fhier_addr_short |>
#   clean_names_and_addresses() |>
#   mutate(
#     fhier_address =
#       str_glue(
#         "
#         {physical_address_1}, {physical_address_2}, {physical_city}, {physical_county}, {physical_state}, {physical_zip_code}
#       "
#       )
#   ) |>
#   select(
#     -c(
#       physical_address_1,
#       physical_address_2,
#       physical_city,
#       physical_county,
#       physical_state,
#       physical_zip_code
#     )
#   ) |>
#   clean_names_and_addresses() |>
#   distinct()

# dim(fhier_addr_short__comb_addr)
# [1] 2390    5

# dim(fhier_addr_short_clean)
```

### Add addresses from FHIER 

```{r Add addresses from FHIER}
## Add addresses from FHIER ----
# <<<<

# Explanations for the following code:

# This function prints the comma separated column names of a dataframe, with an option to limit the number of names displayed. It's useful for quickly inspecting the structure of a dataframe, especially when dealing with datasets with a large number of columns.
#  
#  
# 1. **Function Definition:**
#    - `print_df_names <- function (my_df, names_num = 100) { ... }`: Defines a function named `print_df_names` with two arguments: `my_df` (the dataframe whose column names will be printed) and `names_num` (the maximum number of names to display, defaulted to 100).
# 
# 2. **Extracting Column Names:**
#    - `names(my_df)`: Retrieves the column names of the input dataframe `my_df`.
# 
# 3. **Selecting Subset of Names:**
#    - `%>% head(names_num)`: Uses the pipe operator (`%>%`) to pass the column names to the `head` function, which selects the first `names_num` names. This is useful when the dataframe has a large number of columns, and we want to limit the display to a manageable number.
# 
# 4. **Formatting as a String:**
#    - `%>% paste0(collapse = ", ")`: Concatenates the selected column names into a single string, separated by commas. This creates a more readable output.
# 
# 5. **Returning the Result:**
#    - `%>% return()`: Returns the concatenated string of column names as the output of the function.
# 

 print_df_names  <-  function (my_df, names_num = 100) 
{
    names(my_df) %>% head(names_num) %>% paste0(collapse = ", ") %>% 
        return()
}
# >>>>
```

Join FHIER address data with the existing dataset

```{r}
compl_corr_to_investigation__corr_date__hailing_port__fhier_addr <-
  dplyr::left_join(compl_corr_to_investigation__corr_date__hailing_port,
            fhier_addr_short_clean)
```

Joining with `by = join_by(vessel_official_number)`
Result: Dataset now includes FHIER address information for each vessel


```{r}
# print_df_names(compl_corr_to_investigation__corr_date__hailing_port__fhier_addr)
```

Verify completeness of contact information


```{r}
compl_corr_to_investigation__corr_date__hailing_port__fhier_addr |>
  dplyr::filter(
    is.na(contactrecipientname) |
      is.na(contactphone_number) |
      is.na(contactemailaddress)
  ) |> nrow()
# 0
```

### Vessels with no addresses 

```{r Vessels with no addresses}
## Vessels with no addresses ----

# print_df_names(compl_corr_to_investigation__corr_date__hailing_port__fhier_addr)
```

Explanations for the following code:

This code snippet creates a dataframe `no_addr_vsl_ids` containing unique `vessel_official_number` values based on certain conditions.

1. **Starting with the DataFrame:**

   - `compl_corr_to_investigation__corr_date__hailing_port__fhier_addr |>`: Pipes the dataframe `compl_corr_to_investigation__corr_date__hailing_port__fhier_addr` into the next function.

2. **Filtering Rows:**

   - `dplyr::filter(physical_address_1 %in% is_empty)`: Uses the `filter` function from the `dplyr` package to keep rows where the `physical_address_1` column is empty.

     - `physical_address_1 %in% is_empty`: This condition checks if the values in the `physical_address_1` column are empty.

3. **Selecting Columns:**

   - `dplyr::select(vessel_official_number)`: Selects only the `vessel_official_number` column from the filtered dataframe.

4. **Removing Duplicate Rows:**

   - `dplyr::distinct()`: Removes duplicate rows from the dataframe, ensuring that each `vessel_official_number` appears only once in the final result.

The resulting `no_addr_vsl_ids` dataframe contains unique `vessel_official_number` values where the corresponding `physical_address_1` column is empty in the `compl_corr_to_investigation__corr_date__hailing_port__fhier_addr` dataframe.

Define a vector of values considered as empty

```{r}
is_empty <- c(NA, "NA", "", "UN", "N/A")

# Create a dataframe of unique vessel official numbers with empty physical addresses
no_addr_vsl_ids <-
  compl_corr_to_investigation__corr_date__hailing_port__fhier_addr |>
  dplyr::filter(physical_address_1 %in% is_empty) |>
  dplyr::select(vessel_official_number) |>
  dplyr::distinct()

# Count the number of unique vessel official numbers with empty addresses
dplyr::n_distinct(no_addr_vsl_ids$vessel_official_number)
# 109
# 71
```

## Addresses from Oracle db 

```{r Addresses from Oracle db}
# Addresses from Oracle db ----
```

Explanations for the following code:

This code snippet processes the `db_participants_address` dataframe by filtering rows based on the `official_number` and ensuring distinct rows in the resulting dataframe.

1. **Starting with the DataFrame:**

   - `db_participants_address |>`: Starts with the `db_participants_address` dataframe and pipes it into the next function.


2. **Filtering Rows:**

   - `dplyr::filter(official_number %in% no_addr_vsl_ids$vessel_official_number)`:

     - `dplyr::filter(...)`: The `filter` function from the `dplyr` package is used to keep rows that meet certain conditions.

     - `official_number %in% no_addr_vsl_ids$vessel_official_number`: This condition keeps only the rows where the `official_number` is found in the `vessel_official_number` column of the `no_addr_vsl_ids` dataframe.

       - `%in%`: The `%in%` operator checks if elements of `official_number` are present in `no_addr_vsl_ids$vessel_official_number`.

3. **Removing Duplicate Rows:**

   - `dplyr::distinct()`: The `distinct` function from the `dplyr` package removes duplicate rows from the filtered dataframe, ensuring each row is unique.

The result is a new dataframe `db_participants_address__needed` that contains only the rows from `db_participants_address` where the `official_number` is present in the `no_addr_vsl_ids$vessel_official_number` column, and all duplicate rows are removed.


```{r}
db_participants_address__needed <-
  db_participants_address |>
  dplyr::filter(official_number %in% no_addr_vsl_ids$vessel_official_number) |>
  dplyr::distinct()

dim(db_participants_address__needed)
# [1] 139  37

dplyr::n_distinct(db_participants_address__needed$official_number)
# 71
```

### Keep fewer columns in db_participants_address__needed 

```{r Keep fewer columns in db_participants_address__needed}
## Keep fewer columns in db_participants_address__needed ----
```

Define a vector of column names to keep in the final dataframe

```{r}
col_names_to_keep <-
  c(
    "official_number",
    "entity_name",
    "primary_email",
    # "is_primary",
    "ph_area",
    "ph_number",
    "entity_name",
    "physical_city",
    "physical_county",
    "physical_state",
    "physical_zip_code",
    "mailing_address1",
    "mailing_address2",
    "mailing_city",
    "mailing_county",
    "mailing_country",
    "mailing_state",
    "mailing_zip_code"
  )
```

Explanations for the following code:

This code snippet processes the `db_participants_address__needed` dataframe by selecting specific columns, removing duplicate rows, and arranging the rows based on the `official_number` column.

1. **Creating a Regular Expression Pattern for Column Names:**

   - `my_cols_ends <- paste0(col_names_to_keep, '$', collapse = '|')`:

     - `col_names_to_keep`: This variable contains a list or vector of column name prefixes you want to keep.

     - `paste0(...)`: This function concatenates the elements of `col_names_to_keep` with a `$` at the end of each element, creating a regular expression pattern to match column names that end with any of the specified prefixes.

     - `collapse = '|'`: The `collapse` parameter ensures that the elements are joined by a `|`, which is the OR operator in regular expressions.

2. **Selecting Specific Columns:**

   - `db_participants_address__needed |>`: Starts with the `db_participants_address__needed` dataframe and pipes it into the next function.

   - `dplyr::select(tidyselect::matches(my_cols_ends))`: Uses the `select` function from `dplyr` and the `matches` function from `tidyselect` to select columns whose names match the regular expression pattern stored in `my_cols_ends`.

3. **Removing Duplicate Rows:**

   - `dplyr::distinct()`: Removes duplicate rows from the selected columns.

The result is a new dataframe `db_participants_address__needed_short1` that contains only the columns matching the specified pattern, with duplicates removed.

Create a regular expression pattern to match column names ending with the specified prefixes


```{r}
my_cols_ends <- paste0(col_names_to_keep,
                  '$',
                  collapse = '|')

# Create a new dataframe with selected columns and remove duplicates
db_participants_address__needed_short <-
  db_participants_address__needed |>
  dplyr::select(tidyselect::matches(my_cols_ends)) |>
  dplyr::distinct()
```

Verify the number of rows in the final dataset

```{r}
nrow(compl_corr_to_investigation__corr_date__hailing_port__fhier_addr)
# 199
```

Confirm unique vessel official numbers is the same

```{r}
dplyr::n_distinct(compl_corr_to_investigation__corr_date__hailing_port__fhier_addr$vessel_official_number)
# 199
#
```

one vessel per row, OK

have to combine rows

```{r}
dim(db_participants_address__needed_short)
# 106
dplyr::n_distinct(db_participants_address__needed_short$official_number)
# 71
```

### Combine area and phone numbers 

```{r Combine area and phone numbers}
## Combine area and phone numbers ----
```

Explanations for the following code:

This code creates a new dataframe `db_participants_address__needed_short__phone0` by modifying the `db_participants_address__needed_short` dataframe. It adds two new columns (`erv_phone` and `erb_phone`) that concatenate existing columns.

1. **Starting with the Original Dataframe:**

   - `db_participants_address__needed_short |>`: Begins with the `db_participants_address__needed_short` dataframe and pipes it into the next function.

2. **Adding New Columns:**

   - `dplyr::mutate(...)`: The `mutate` function from the `dplyr` package is used to add or modify columns in the dataframe.

     - `erv_phone = paste0(erv_ph_area, erv_ph_number)`: Creates a new column `erv_phone` by concatenating the `erv_ph_area` and `erv_ph_number` columns using `paste0`, which combines strings without any separator.

     - `erb_phone = paste0(erb_ph_area, erb_ph_number)`: Similarly, creates a new column `erb_phone` by concatenating the `erb_ph_area` and `erb_ph_number` columns.

The result is a new dataframe `db_participants_address__needed_short__phone0` that contains all the original columns from `db_participants_address__needed_short` plus two new columns (`erv_phone` and `erb_phone`) that contain concatenated phone numbers.


```{r}
db_participants_address__needed_short__phone0 <-
  db_participants_address__needed_short |>
  dplyr::mutate(erv_phone = paste0(erv_ph_area, erv_ph_number),
         erb_phone = paste0(erb_ph_area, erb_ph_number))
```

### Make erv and erb combinations 

```{r Make erv and erb combinations}
## Make erv and erb combinations ----
# <<<<

# Explanations for the following code:

# Overall, this function ensures that the elements in the input list are cleaned, unique, and sorted, providing a tidy and organized output.
#  
#  
# The `list_sort_uniq` function is designed to take a list of character vectors, clean them, remove duplicates, sort them, and return a single vector with unique, sorted elements.
# 
# 1. **Function Definition:**
#    - `list_sort_uniq <- function(my_lists) { .. }`: Defines a function named `list_sort_uniq` with one argument, `my_lists`, representing the list of character vectors to process.
# 
# 2. **Data Transformation Pipeline:**
#    - `my_lists |> ...`: Utilizes the pipe operator (`|>`) to pass the `my_lists` argument through a series of data transformation steps.
#    
# 3. **Cleaning and Transformation:**
#    - `str_trim()`: Removes leading and trailing whitespace from each element of the list.
#    - `unique()`: Removes duplicate elements from the list.
#    - `sort()`: Sorts the elements of the list in ascending order.
#    - `list()`: Converts the sorted vector into a list.
#    - `flatten()`: Flattens the resulting list into a single vector.
# 
# 4. **Return Result:**
#    - The function returns the processed vector stored in `res`.
# 

 list_sort_uniq  <-  function (my_lists) 
{
    res <- purrr::flatten(list(sort(unique(stringr::str_trim(my_lists)))))
    return(res)
}
# >>>>
```

Define a vector of column name parts to be used for creating new combined columns


```{r}
col_part_names <-
  c(
    "entity_name",
    "primary_email",
    # "ph_is_primary",
    # "ph_area",
    # "ph_number",
    "physical_city",
    "physical_county",
    "physical_state",
    "physical_zip_code",
    "mailing_address1",
    "mailing_address2",
    "mailing_city",
    "mailing_county",
    # "mailing_country",
    "mailing_state",
    "mailing_zip_code"
  )
```

Create a new dataframe that combines ERV and ERB information for each column part

Explanations for the following code:

1. **Mapping Over Column Parts:**

   - `col_part_names |> purrr::map(\(curr_col_part) { ... })`: It iterates over each element in `col_part_names` using the `map` function from the purrr package. For each column part (`curr_col_part`), it executes the code inside the curly braces `{ ... }`.

2. **Generating New Column Names:**

   - `new_col_name <- stringr::str_glue("db_{curr_col_part}")`: It creates a new column name by combining the prefix "db_" with the current column part (`curr_col_part`) using `str_glue` from the stringr package.
   
 Use !!new_col_name := to dynamically create new column names based on curr_col_part

3. **Grouping and Mutating Data:**

   - `db_participants_address__needed_short__phone0 |> dplyr::group_by(official_number) |> ...`: It groups the dataframe `db_participants_address__needed_short__phone0` by the column `official_number` using `group_by` from dplyr. Then, it proceeds with further data manipulation operations.

4. **Applying Purrr::pmap Function:**

   - `purrr::pmap(dplyr::across(dplyr::ends_with(curr_col_part)), ...)`: It applies the `pmap` function from the purrr package to iterate over columns that end with the current column part (`curr_col_part`). Within the `pmap` call, a custom function (`list_sort_uniq`) is applied to each corresponding set of columns.

5. **Ungrouping and Selecting Columns:**

   - `... |> dplyr::ungroup() |> dplyr::select(-official_number)`: After the mutation step, it ungroups the dataframe and removes the `official_number` column using `ungroup()` and `select()` functions from dplyr, respectively.

6. **Binding Columns Together:**

   - `dplyr::bind_cols(db_participants_address__needed_short__phone0, .)`: Finally, it binds the original dataframe `db_participants_address__needed_short__phone0` with the transformed columns obtained from the mapping operation using `bind_cols` from dplyr.

This code dynamically generates new columns in the dataframe based on the provided column parts, applies a custom function to each set of corresponding columns, and then binds the resulting columns back to the original dataframe.


```{r}
tictoc::tic("map all pairs")
db_participants_address__needed_short__erv_erb_combined3 <-
  col_part_names |>
  purrr::map(\(curr_col_part)  {
    new_col_name <- stringr::str_glue("db_{curr_col_part}")
    # cat(new_col_name, sep = "\n")

    db_participants_address__needed_short__phone0 |>
      dplyr::group_by(official_number) |>
      dplyr::mutate(!!new_col_name :=
                      purrr::pmap(dplyr::across(dplyr::ends_with(curr_col_part)),
                                    ~ list_sort_uniq(.)),
             .keep = "none" ) |>
      dplyr::ungroup() |>
      dplyr::select(-official_number)

  }) %>%
  dplyr::bind_cols(db_participants_address__needed_short__phone0, .)
tictoc::toc()
# map all pairs: 14.31 sec elapsed
```

#### Shorten db_participants_address__needed_short__erv_erb_combined3 

```{r Shorten db_participants_address__needed_short__erv_erb_combined3}
### Shorten db_participants_address__needed_short__erv_erb_combined3 ----
```

Explanations for the following code:

Create a shortened version of the dataframe, keeping only the official_number and db_ columns

1. **Dataframe Selection and Transformation:**

   - `db_participants_address__needed_short__erv_erb_combined3 |>`: Starts with the input dataframe `db_participants_address__needed_short__erv_erb_combined3` and pipes it into the subsequent functions.

2. **Selecting Specific Columns:**

   - `dplyr::select(official_number, tidyselect::all_of(tidyselect::starts_with("db_")))`: Uses `dplyr::select` to retain only the `official_number` column and any columns whose names start with "db_".

     - `tidyselect::all_of(tidyselect::starts_with("db_"))`: Uses the `tidyselect` package to identify all column names that start with "db_". The `all_of` function ensures that the selected columns exist within the dataframe.

3. **Ensuring Unique Rows:**
   - `dplyr::distinct()`: Ensures that the resulting dataframe contains only unique rows, removing any duplicate rows based on the selected columns.

The result is a new dataframe `db_participants_address__needed_short__erv_erb_combined_short` that contains only the `official_number` column and columns starting with "db_", with all duplicate rows removed.


```{r}
db_participants_address__needed_short__erv_erb_combined_short <-
  db_participants_address__needed_short__erv_erb_combined3 |>
  dplyr::select(official_number,
                tidyselect::all_of(tidyselect::starts_with("db_"))) |>
  dplyr::distinct()

dim(db_participants_address__needed_short__erv_erb_combined_short)
# 94 17

dplyr::n_distinct(db_participants_address__needed_short__erv_erb_combined_short$official_number)
# 71
```

check

```{r}
# db_participants_address__needed_short__erv_erb_combined_short |>
#   dplyr::filter(official_number == "1235397") |>
#   dplyr::glimpse()
# $ db_physical_city     <list> ["SOUTH ISLANDIA"], ["ISLANDIA"]
```

### Combine similar fields 

```{r Combine similar fields}
## Combine similar fields ----
```


Create a new dataframe with combined and unique values for each participant field

Explanations for the following code:

1. Iterate over each participant column using 'col_part_names'.

col_part_names is a list of participant column name suffixes (e.g., "physical_city", "physical_state", etc.)

   - 'map' applies the provided function to each element of the list.

2. Define the old and new column names based on the current participant column.

   - 'str_glue' is used for string interpolation to create column names.

3. Group the DataFrame by 'official_number' using 'group_by'.

4. For each group, create a new column with unique sorted values for the current participant.

   - 'list_sort_uniq' ensures unique values and sorts them.

5. Ungroup the DataFrame and remove the 'official_number' column.

   - 'ungroup' removes grouping structure.

   - 'select' is used to exclude the 'official_number' column and keep only the new column.

6. Bind the resulting columns to 'db_participants_address__needed_short__erv_erb_combined_short'.

   - 'bind_cols' combines columns horizontally.

7. Select only the 'official_number' and columns ending with '_u'.

The '_u' suffix in new column names indicates that these columns contain unique, combined values

8. Keep only distinct rows in the final DataFrame using 'distinct'.

9. The resulting DataFrame is stored in 'db_participants_address__needed_short__erv_erb_combined_short__u'.


```{r}
db_participants_address__needed_short__erv_erb_combined_short__u_temp <-
  col_part_names |>
  purrr::map(\(curr_col_part)  {
    # browser() # Commented out browser function for debugging

    old_col_name <- stringr::str_glue("db_{curr_col_part}")
    new_col_name <- stringr::str_glue("db_{curr_col_part}_u")
    cat(new_col_name, sep = "\n")

    db_participants_address__needed_short__erv_erb_combined_short |>
      dplyr::group_by(official_number) |>
      dplyr::mutate(!!new_col_name := list(paste(sort(unique(stringr::str_trim(purrr::list_flatten(!!rlang::sym(old_col_name))))))),
             .keep = "none" ) |>
      dplyr::ungroup() |>
      dplyr::select(-official_number)
  })

# dplyr::glimpse(db_participants_address__needed_short__erv_erb_combined_short)
```

Explanations for the following code:

This code creates a new dataframe by combining columns with suffix "_u" from two existing dataframes (`db_participants_address__needed_short__erv_erb_combined_short` and `db_participants_address__needed_short__erv_erb_combined_short__u_temp`).

1. **Binding Columns Together:**

   - `dplyr::bind_cols(...)`: It binds columns from two dataframes together. The columns from `db_participants_address__needed_short__erv_erb_combined_short` and `db_participants_address__needed_short__erv_erb_combined_short__u_temp` are combined horizontally.

2. **Selecting Columns:**

   - `dplyr::select(official_number, dplyr::all_of(dplyr::ends_with("_u")))`: After binding columns, it selects the `official_number` column along with all columns that end with "_u" using `select` from dplyr.

3. **Removing Duplicate Rows:**

   - `dplyr::distinct()`: It removes duplicate rows from the dataframe to ensure each row is unique.

This code essentially creates a new dataframe containing selected columns from two existing dataframes and ensures that there are no duplicate rows in the resulting dataframe.


```{r}
db_participants_address__needed_short__erv_erb_combined_short__u <-
  dplyr::bind_cols(
    db_participants_address__needed_short__erv_erb_combined_short,
    db_participants_address__needed_short__erv_erb_combined_short__u_temp
  ) |>
  dplyr::select(official_number, dplyr::all_of(dplyr::ends_with("_u"))) |>
  dplyr::distinct()
```

check

```{r}
# db_participants_address__needed_short__erv_erb_combined_short__u |>
#   dplyr::filter(official_number == "1235397") |>
#   dplyr::glimpse()
```

#### Convert to characters 

```{r Convert to characters}
### Convert to characters ----
```

Explanations for the following code:

This code modifies the dataframe `db_participants_address__needed_short__erv_erb_combined_short__u` by concatenating the elements of list-type columns into a single string separated by semicolons. Here's a detailed explanation:

1. **Row-wise Operation:**

   - `dplyr::rowwise()`: It sets the dataframe to be processed row-wise, meaning each operation will be applied independently to each row.

2. **Mutating List-type Columns:**

   - `dplyr::mutate_if(is.list, ~ paste(unlist(.), collapse = '; '))`: This line applies a mutation to each column of the dataframe that is of list type.

     - `is.list`: Checks if a column is of list type.

     - `paste(unlist(.), collapse = '; ')`: For each list-type column, it converts the list elements into a single string by unlisting them and concatenating them together with a semicolon as the separator.

3. **Ungrouping:**

   - `dplyr::ungroup()`: It removes the grouping previously applied to the dataframe, returning it to its original state.

This code effectively transforms list-type columns in the dataframe into character vectors, concatenating their elements into a single string with semicolons as separators.


```{r}
db_participants_address__needed_short__erv_erb_combined_short__u_no_c <-
  db_participants_address__needed_short__erv_erb_combined_short__u |>
  dplyr::rowwise() |>
  dplyr::mutate_if(is.list, ~ paste(unlist(.), collapse = '; ')) |>
  dplyr::ungroup()
```

check

```{r}
# db_participants_address__needed_short__erv_erb_combined_short__u_no_c |>
#   dplyr::filter(official_number == "1235397") |>
#   dplyr::glimpse()
# $ db_mailing_state_u     <chr> "NY"
# $ db_mailing_city_u      <chr> "ISLANDIA; SOUTH ISLANDIA"
```

### Rename fields 

```{r Rename fields}
## Rename fields ----
```

Explanations for the following code:

Remove the "_u" suffix from column names in the dataframe

1. **Renaming Columns:**

   - `dplyr::rename_with( ~ stringr::str_replace(.x, pattern = "_u$", replacement = ""))`: It renames the columns of the dataframe using a function provided by `rename_with`.

     - `~`: It indicates the start of an anonymous function.

     - `stringr::str_replace(.x, pattern = "_u$", replacement = "")`: For each column name (`x`), it applies the `str_replace` function from the `stringr` package to replace the pattern "_u" at the end of the column name with an empty string, effectively removing it.

This code removes the "_u" suffix from the column names in the dataframe.


```{r}
db_participants_address__needed_short__erv_erb_combined_short__u_ok <-
  db_participants_address__needed_short__erv_erb_combined_short__u_no_c |>
  dplyr::rename_with(~ stringr::str_replace(.x, pattern = "_u$",
                                            replacement = ""))
```

## Join FHIER and Oracle db addresses 

```{r Join FHIER and Oracle db addresses}
# Join FHIER and Oracle db addresses ----
```

Combine FHIER and Oracle database address information using vessel official number


```{r}
compl_corr_to_investigation__corr_date__hailing_port__fhier_addr__db_addr <-
  compl_corr_to_investigation__corr_date__hailing_port__fhier_addr |>
  dplyr::left_join(
    db_participants_address__needed_short__erv_erb_combined_short__u_ok,
    dplyr::join_by(vessel_official_number == official_number)
  )

# check
# compl_corr_to_investigation__corr_date__hailing_port__fhier_addr__db_addr |>
#   dplyr::filter(vessel_official_number == "1235397") |>
#   dplyr::glimpse()
# $ db_mailing_state       <chr> "NY"
# $ db_mailing_city        <chr> "ISLANDIA; SOUTH ISLANDIA"

cat("Result: ",
    "compl_corr_to_investigation__corr_date__hailing_port__fhier_addr__db_addr",
    sep = "\n")
```


Results are in compl_corr_to_investigation__corr_date__hailing_port__fhier_addr__db_addr

### 3. Mark vessels already in the know list 

```{r 3 Mark vessels already in the know list}
## 3. Mark vessels already in the know list ----
```

Identify and mark vessels that have been previously marked as egregious, to track repeat offenders

From the email:

The first column (report created) indicates the vessels that we have created a case for. My advice would be not to exclude those vessels. EOs may have provided compliance assistance and/or warnings already. If that is the case and they continue to be non-compliant after that, they will want to know and we may need to reopen those cases.


get vessel ids from the previous result 

```{r}
vessels_to_mark_ids <-
  prev_result |>
  dplyr::select(vessel_official_number)
```


Check the amount 

```{r}
dim(vessels_to_mark_ids)
```

##### Mark these vessels 

```{r Mark these vessels}
#### Mark these vessels ----
```


Create a new column to distinguish between previously processed vessels and new entries

Explainations for the following code:

Create a new column 'duplicate_w_last_time' in the dataframe 'compl_corr_to_investigation_short'.

This column is marked with "duplicate" for rows where 'vessel_official_number' is present in the list of vessel IDs ('vessels_to_mark_ids').

For all other rows, it is marked as "new".


```{r}
compl_corr_to_investigation__corr_date__hailing_port__fhier_addr__db_addr__dup_marked <-
  compl_corr_to_investigation__corr_date__hailing_port__fhier_addr__db_addr |>
  dplyr::mutate(
    duplicate_w_last_time =
      dplyr::case_when(
        vessel_official_number %in%
          vessels_to_mark_ids$vessel_official_number ~ "duplicate",
        .default = "new"
      )
  )
```

#### Check 

```{r Check}
### Check ----
```

Perform validation checks on the processed data
Check that number of vessels didn't change.

```{r}
dplyr::n_distinct(compl_corr_to_investigation__corr_date__hailing_port__fhier_addr__db_addr__dup_marked$vessel_official_number) ==
  num_of_vsl_to_investigate
```


Check that there is one row per vessel.

```{r}
nrow(compl_corr_to_investigation__corr_date__hailing_port__fhier_addr__db_addr__dup_marked) == 
  num_of_vsl_to_investigate 
```


Count how many duplicates 

```{r}
compl_corr_to_investigation__corr_date__hailing_port__fhier_addr__db_addr__dup_marked |>
  dplyr::count(duplicate_w_last_time)
# 1 duplicate               108
# 2 new                      48
```

### 4. How many are duals? 

```{r 4 How many are duals}
## 4. How many are duals? ----
```

Identify and count vessels with dual permits (both SA and GOM)
Explainations for the following code:

Create a new dataframe 

Use the 'mutate' function to add a new column 'permit_region' based on conditions.

If 'permitgroup' contains any of the specified patterns ("RCG", "HRCG", "CHG", "HCHG"),

set 'permit_region' to "dual". Otherwise, set 'permit_region' to "sa_only".

If none of the conditions are met, set 'permit_region' to "other".

The resulting dataframe includes the original columns from 'compl_corr_to_investigation_short_dup_marked'
along with the newly added 'permit_region' column.


```{r}
compl_corr_to_investigation_short_dup_marked__permit_region <-
  compl_corr_to_investigation__corr_date__hailing_port__fhier_addr__db_addr__dup_marked |> 
  dplyr::mutate(permit_region =
           dplyr::case_when(
             grepl("RCG|HRCG|CHG|HCHG", permitgroup) ~ "dual",
             !grepl("RCG|HRCG|CHG|HCHG", permitgroup) ~ "sa_only",
             .default = "other"
           ))
```



Explainations for the following code:

Use the 'select' function to extract the columns 'vessel_official_number' and 'permit_region'

from the dataframe 'compl_corr_to_investigation_short_dup_marked__permit_region'.

Use the 'distinct' function to keep only unique combinations of 'vessel_official_number' and 'permit_region'.

Use the 'count' function to count the occurrences of each unique 'permit_region'.

The resulting count provides the frequency of each 'permit_region'.


```{r}
region_counts <-
  compl_corr_to_investigation_short_dup_marked__permit_region |>
  dplyr::select(vessel_official_number, permit_region) |>
  dplyr::distinct() |>
  dplyr::count(permit_region)

dplyr::n_distinct(compl_corr_to_investigation_short_dup_marked__permit_region$vessel_official_number)
```

#### Dual permitted cnts 

```{r Dual permitted cnts}
### Dual permitted cnts ----
```

Calculate the percentage of dual-permitted vessels

```{r}
region_counts$n[[1]] / (region_counts$n[[2]] + region_counts$n[[1]]) * 100
```

## Print out results 

```{r Print out results}
# Print out results ----
```

### Add additional columns in front 

```{r Add additional columns in front}
## Add additional columns in front ----
```


Create a variable with a long column name for confirmation status


```{r}
additional_column_name1 <-
  stringr::str_glue(
    "Confirmed Egregious? (permits must still be active till {permit_expired_check_date}, missing past 6 months, and (1) they called/emailed us (incoming), or (2) at least 2 contacts (outgoing) with at least 1 call/other (voicemail counts) and at least 1 email)"
  )
```


Explanations for the following code:

This code adds new columns to the dataframe `compl_corr_to_investigation_short_dup_marked__permit_region`. Here's what each part does:

1. **Add Columns Function:**

   - `tibble::add_column()`: This function from the `tibble` package is used to add new columns to a dataframe.


2. **Column Specifications:**

   - `!!(additional_column_name1) := NA`: Adds a new column with a name in `additional_column_name1` variable filled with NA values.

     - `!!`: This is a tidy evaluation feature that allows the use of non-standard evaluation. It evaluates the expression `additional_column_name1` dynamically.

     - `:= NA`: Assigns NA values to the new column.

   - `Notes = NA`: Adds another new column named "Notes" filled with NA values.

   - `.before = 2`: Specifies that the new columns should be inserted before the second column in the dataframe.

This code effectively adds two new columns, with names from `additional_column_name1` and "Notes", filled with NA values, to the dataframe.


```{r}
compl_corr_to_investigation_short_dup_marked__permit_region__add_columns <-
  compl_corr_to_investigation_short_dup_marked__permit_region |>

  tibble::add_column(
    !!(additional_column_name1) := NA,
    Notes = NA,
    .before = 2
  )
```


Don't remove the "year" column, in case there are 2 years in the current period.


Check and display the updated column names


```{r}
print_df_names(compl_corr_to_investigation_short_dup_marked__permit_region__add_columns)
```

Generate output file name with current date

The file name is split into a basename and a full name with extension.

The basename will be used for the spreadsheet saved on Google Drive.
 

```{r}
out_file_basename <- 
  stringr::str_glue("egregious_violators_to_investigate_{lubridate::today()}")

out_file_name <-
  stringr::str_glue("{out_file_basename}.csv")

result_path <- 
  file.path(current_project_output_path,
            out_file_name)
```

Write the results to a CSV file

```{r}
compl_corr_to_investigation_short_dup_marked__permit_region__add_columns |>
  readr::write_csv(result_path)
```

### Write to google sheets 

```{r Write to google sheets}
## Write to google sheets ----
```


Explainations for the following code:

Define a function to write results to Google Sheets

This function performs the following steps:

1. Renames the existing 'Egregious Violators Current' spreadsheet to a spreadsheet with the name with a date from its tab (e.g. "egregious_violators_to_investigate_2024-06-18"

2. Creates a new 'current' spreadsheet

3. Writes the new results to the new 'Egregious Violators Current' spreadsheet in the same Google drive directory ("Egregious violators/output")

4. Removes the default empty sheet

5. Opens the new spreadsheet in the browser for verification

6. Returns a shareable link to the new spreadsheet

It has to be a function, this way we can call it if needed, not every time we run the code.


```{r}
write_res_to_google_sheets <- 
  function() {
    # Define the current result Google Sheets name
    current_result_google_ss_name <- "Egregious Violators Current"
    
    # my_current_ss contains information about the existing 'Egregious Violators Current' file
    my_current_ss <-
      googledrive::drive_ls(
        path = googledrive::as_id(output_egr_violators_googledrive_folder_path),
        pattern = current_result_google_ss_name,
        type = "spreadsheet",
        n_max = 1
      )
    
    # An example of my_current_ss:
    #   name                        id                                           drive_resource
    # 1 Egregious Violators Current ...--o6BpLWpb4-... <named list [36]>
    
    # Next:
    # 1) load it to R;
    # 2) create a new spreadsheet with the date of the loaded worksheet and dump the content into it;
    # 3) create a new worksheet in the current spreadsheet with today's date;
    # 4) write the code output into it;
    # 5) check in browser.
    
    # 1) load it to R
    previous_current_content <- googlesheets4::read_sheet(my_current_ss)
    
    # 2) create a new spread sheet with the date of loaded worksheet and dump the content into it
    # a) get the previous spreadsheet name
    
    # ss_info contains detailed information about the current spreadsheet, including sheet names
    ss_info <- googlesheets4::gs4_get(my_current_ss)
    
    # grep for the pattern in case there are additional tabs
    previous_current_spread_sheet_name <-
      grep(
        "egregious_violators_to_investigate_20\\d\\d-\\d\\d-\\d\\d",
        ss_info$sheets$name,
        value = T
      )
    # E.g. "egregious_violators_to_investigate_2024-06-18"
    
    # Rename the file from "current" to the previous_current_spread_sheet_name with the previous date.
    # In case of an error print the message and keep going.
    # If there is a file with this name this code will create another one with the same name.

    tryCatch({
      message("Try to rename the file")
      
      googledrive::drive_mv(
        my_current_ss,
        path = googledrive::as_id(output_egr_violators_googledrive_folder_path),
        name = previous_current_spread_sheet_name
      )
      # E.g.
      # Original file:
      # • Egregious Violators Current
      # Has been renamed:
      # • output/egregious_violators_to_investigate_2024-06-18
      
    }, error = function(cond) {
      message(
        paste(
          "Failed to rename this file: ",
          previous_current_spread_sheet_name
        )
      )
      
      message("Here's the original error message:")
      message(conditionMessage(cond))
      # Choose a return value in case of error
    }, warning = function(cond) {
      
    }, finally = {
      # message("Some other message at the end")
    })
    
    # Create a new empty spreadsheet in the Google Drive output folder to replace the renamed one
    # And save its properties into current_result_google_ss_name_info
    # In case of an error print the message and keep going.
    # If there is a file with this name this code will create another one with the same name.
    
    tryCatch({
      message("Try to create a new file")
      
      current_result_google_ss_name_info <-
        googledrive::drive_create(
          name = current_result_google_ss_name,
          path = googledrive::as_id(output_egr_violators_googledrive_folder_path),
          type = "spreadsheet"
        )
      
    }, error = function(cond) {
      message(
        paste(
          "Failed to create a new file: ",
          current_result_google_ss_name
        )
      )
      
      message("Here's the original error message:")
      message(conditionMessage(cond))
      # Choose a return value in case of error
    }, warning = function(cond) {
      
    }, finally = {
      # message("Some other message at the end")
    })
    
    # Write our results into the newly created spreadsheet "Egregious Violators Current"
    # into a sheet/tab with a name defined in out_file_basename
    googlesheets4::write_sheet(
      compl_corr_to_investigation_short_dup_marked__permit_region__add_columns,
      ss = current_result_google_ss_name_info,
      sheet = out_file_basename
    )
    
    # See sheets/tabs to check
    googlesheets4::sheet_properties(ss = current_result_google_ss_name_info)
    
    # Remove the empty Sheet1 created automatically by googledrive::drive_create()
    googlesheets4::sheet_delete(ss = current_result_google_ss_name_info, "Sheet1")
    
    # Check the existing tabs again
    googlesheets4::sheet_properties(ss = current_result_google_ss_name_info)$name
    # Should be only one name now, like
    # [1] "egregious_violators_to_investigate_2024-07-15"
    
    # See in browser to check
    googledrive::drive_browse(current_result_google_ss_name_info)
    
    # Generate a shareable link for the new spreadsheet
    current_output_file_link <- googledrive::drive_link(current_result_google_ss_name_info)
    
    pretty_print(current_output_file_link, "Link to the new spreadsheet:")
    
    # The function returns the current output file link
    return(current_output_file_link)
    
  }
```


Manually: Un-comment to write results directly to Google drive

```{r}
# current_output_file_link <- write_res_to_google_sheets()
```


Print result names to console 

```{r}
cat("Results:",
    "compl_corr_to_investigation_short_dup_marked__permit_region__add_columns",
    out_file_name,
    sep = "\n")
```

