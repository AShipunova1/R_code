---
title: egregious_violators
---

```{r no cache setup, results='hide', message=FALSE, warning=FALSE, cache=FALSE, include=FALSE}

# A general-purpose tool for dynamic report generation in R
library(knitr)

# Adds features to a kable output
library(kableExtra)

# Format R code automatically
library(styler)

# Load the tidyr package for data manipulation
library(tidyr)

# Load the magrittr package for piping operations
library(magrittr)

# Load the stringr package for string manipulation
library(stringr)

# Load the openxlsx package for reading and writing Excel files
library(openxlsx)

```

```{r df format setup}
#| include: false
# kable <- function(data) {
#   knitr::kable(data, booktabs = true, digits = 2) %>%
#     kable_styling('striped', full_width = FALSE)
# }

knit_print.data.frame = function(x, ...) {
  res = paste(c(
    '',
    '',
    knitr::kable(x, digits = 2) |>
      kableExtra::kable_styling('striped', full_width = FALSE)
  ),
  collapse = '
')
  knitr::asis_output(res)
}

registerS3method(
  'knit_print', 'data.frame', knit_print.data.frame,
  envir = asNamespace('knitr')
)

# knitr::opts_chunk$set(echo = TRUE)

# options(knitr.table.format = 'HTML')

```

# Set Up
```{r setup current project, results='hide', message=FALSE, warning=FALSE}
# Read me
# 1) NO reports for all 26 weeks back from week ago today;
# 2) permits have not expired and were active for the same period as (1);
# 3) the grace period is 7 days back from today.
# 4) It needs to be that we called at least 1 time and emailed at least 1 time. Or they contacted us at least once.
# 5) not counting any correspondence (regardless of the type - email/call, voicemail or not) that includes "No contact made" in the text of the entry as a actual "direct" contact for any egregious vessel (May 6 2024)

# NB. Update (download) all input files every time before run.

# set up ----
# <<<<
set_work_dir  <-  function() {

  # Set the working directory to the user's home directory (~)
  setwd("~/")
  base_dir <- getwd()

  # Define 'main_r_dir' as "R_files_local"
  main_r_dir <- "R_files_local"

  # Define 'in_dir' as "my_inputs"
  in_dir <- "my_inputs"

  # Construct the full path to 'my_inputs' directory
  full_path_to_in_dir <- file.path(base_dir, main_r_dir, in_dir)

  # Define 'out_dir' as "my_outputs"
  out_dir <- "my_outputs"

  # Construct the full path to 'my_outputs' directory
  full_path_to_out_dir <- file.path(base_dir, main_r_dir, out_dir)

  # Define 'git_r_dir' as "R_code_github"
  git_r_dir <- "R_code_github"

  # Construct the full path to 'R_code_github' directory
  full_path_to_r_git_dir <- file.path(base_dir, git_r_dir)

  # Change the working directory to 'R_files_local'
  setwd(file.path(base_dir, main_r_dir))

  # Create a list of directory paths for 'inputs,' 'outputs,' and 'git_r'
  my_paths <- list("inputs" = full_path_to_in_dir,
                   "outputs" = full_path_to_out_dir,
                   "git_r" = full_path_to_r_git_dir)

  # Return the list of directory paths
  return(my_paths)
}

# Explanations:

# This function sets the working directory to the user's home directory, defines directory paths for inputs, outputs, and GitHub R code, changes the working directory to the main R directory, and returns these paths as a list.
#  
#  
# Here's the breakdown of the `set_work_dir_local` function:
# 
# 1. **Function Definition:**
#    - `set_work_dir_local <- function ()`: Defines a function named `set_work_dir_local` that takes no arguments.
# 
# 2. **Setting Working Directory:**
#    - `setwd("~/")`: Sets the working directory to the user's home directory.
#    - `base_dir <- getwd()`: Retrieves the current working directory and assigns it to the variable `base_dir`.
# 
# 3. **Defining Directory Paths:**
#    - `main_r_dir <- "R_files_local"`: Specifies the directory name for local R files.
#    - `in_dir <- "my_inputs"`: Specifies the directory name for input files.
#    - `full_path_to_in_dir <- file.path(base_dir, main_r_dir, in_dir)`: Combines the base directory, main R directory, and input directory to create the full path to the input directory.
#    - `out_dir <- "my_outputs"`: Specifies the directory name for output files.
#    - `full_path_to_out_dir <- file.path(base_dir, main_r_dir, out_dir)`: Combines the base directory, main R directory, and output directory to create the full path to the output directory.
#    - `git_r_dir <- "R_code_github"`: Specifies the directory name for the GitHub repository containing R code.
#    - `full_path_to_r_git_dir <- file.path(base_dir, git_r_dir)`: Combines the base directory and GitHub R directory to create the full path to the GitHub R directory.
# 
# 4. **Changing Working Directory:**
#    - `setwd(file.path(base_dir, main_r_dir))`: Sets the working directory to the main R directory.
# 
# 5. **Creating Paths List:**
#    - `my_paths <- list(...)`: Creates a list named `my_paths` containing the paths to the input directory, output directory, and GitHub R directory.
# 
# 6. **Returning Paths List:**
#    - `return(my_paths)`: Returns the `my_paths` list containing the paths to the input directory, output directory, and GitHub R directory.
# 
# 
# >>>>

# Get common functions
install.packages("devtools")
library(devtools)
devtools::install_github("AShipunova1/R_code/auxfunctions")

library(auxfunctions)
library(ROracle)
library(zoo)
library(diffdf)

my_paths <- auxfunctions::set_work_dir()

current_project_path <- this.path::this.dir()

current_project_basename <-
  basename(current_project_path)

curr_proj_output_path <- file.path(my_paths$outputs,
                         current_project_basename)

curr_proj_input_path <- file.path(my_paths$inputs,
                         current_project_basename)

current_project_name <- current_project_basename

all_inputs <- my_paths$inputs

my_year1 <- "2023"
my_beginning1 <- stringr::str_glue("{my_year1}-01-01")
my_end1 <- stringr::str_glue("{my_year1}-12-31")

my_year2 <- "2024"
my_beginning2 <- stringr::str_glue("{my_year2}-01-01")
my_end2 <- stringr::str_glue("{my_year2}-12-31")

data_file_date <- 
  lubridate::today()
  # lubridate::ymd("2024-02-21")
  
number_of_weeks_for_non_compliancy = 26
days_in_non_compl_weeks <- 
  number_of_weeks_for_non_compliancy * 7
# 182

grace_period = 7 #days

half_year_ago <-
  data_file_date - days_in_non_compl_weeks - grace_period
# [1] "2023-10-04"

# check week and day of the period's start
# week("2023-10-04") 
# 40

# wday("2023-10-04",
#      label = T)
# Wed

# 30 days from today
permit_expired_check_date <- data_file_date + 30

last_week_start <- data_file_date - grace_period
```

## get_data 

```{r get_data}
# get_data ----
```

# Prepare data

half_year_ago

```{r}
get_data_path <- 
  file.path(current_project_path, "egregious_violators_get_data.R")
# get data for egregious violators
# use from egregious_violators_start.R

# 1) compliance data
# Download files from FHIER / Reports / FHIER COMPLIANCE REPORT 
# For the last 6 month
# FHIER_Compliance_...csv

# 2) correspondence data
# Download files from FHIER / Home / Correspondence
# Actions / Download 
# For the whole period, starting 01/01/2021
# "~\my_inputs\from_Fhier\Correspondence\Correspondence_2024_02_15.csv"

# 3) processed Metrics tracking
# For the last 6 month
# SEFHIER_permitted_vessels_nonSRHS_YEAR.csv

# 4) Physical Address List from FHIER
# Downloaded from REPORTS / For-hire Primary Physical Address List
# For the whole period, starting 01/01/2021
# "For-hire Primary Physical Address List.csv"

# 5) home port processed city and state from PIMS
# "~\R_files_local\my_outputs\home_ports\vessels_from_pims_ports.csv"

# 6) address information from Oracle db
# "db_participants_address.rds"

# 7) Previous results (from google drive)
# ~\R_files_local\my_inputs\egregious_violators\egregious violators for investigation_DATES...xlsx"
```

## FHIER 

```{r FHIER}
# FHIER ----

# Compliance
# Correspondence
# permit info from processed metrics tracking

# Download from FHIER first
all_csv_names_list = c("Correspondence_2024_05_15.csv",
                         r"(2024_05_15\FHIER_Compliance_2023__05_15_2024.csv)",
                         r"(2024_05_15\FHIER_Compliance_2024__05_15_2024.csv)")
```

### get compliance and correspondence csv data into variables 

```{r get compliance and correspondence csv data into variables}
## get compliance and correspondence csv data into variables ----
# <<<<
get_compl_and_corresp_data  <-  function(my_paths,
           filenames = csv_names_list_22_23,
           vessel_id_field_name = NA) {
    # Add folder names and categorize CSV files into correspondence and compliance.
    csv_names_list <- prepare_csv_names(filenames)

    # Read the contents of all CSV files.
    csv_contents <- load_csv_names(my_paths, csv_names_list)

    # Clean and standardize headers, and trim 'vesselofficialnumber' field if needed.
    csvs_clean1 <- clean_all_csvs(csv_contents, vessel_id_field_name)

    # Specific correspondence manipulations ----
    # Perform cleaning and processing specific to correspondence data.
    corresp_arr_contact_cnts_clean <- corresp_cleaning(csvs_clean1)

    ## Specific compliance manipulations ----
    # Extract compliance data from the cleaned CSVs.
    compl_arr <- csvs_clean1[2:length(csvs_clean1)]

    # Clean and process compliance data.
    compl_clean <- compliance_cleaning(compl_arr)

    # Return a list containing cleaned compliance and correspondence data.
    return(list(compl_clean, corresp_arr_contact_cnts_clean))
  }

# Explanations:

# This function facilitates the extraction, cleaning, and organization of compliance and correspondence data from files, downloaded from FHIER, providing a convenient way to preprocess the data for further analysis or visualization.
#  
#  
# The `get_compl_and_corresp_data` function is responsible for processing and cleaning compliance and correspondence data from CSV files.
# 
# It uses other functions from this package.
# 
# 1. **Function Definition:**
#    - `get_compl_and_corresp_data <- function(my_paths, filenames = csv_names_list_22_23, vessel_id_field_name = NA) { .. }`: Defines a function named `get_compl_and_corresp_data` that takes three arguments: `my_paths` (paths to the CSV files), `filenames` (list of CSV filenames), and `vessel_id_field_name` (name of the vessel ID field).
# 
# 2. **Prepare CSV Names:**
#    - `csv_names_list <- prepare_csv_names(filenames)`: Prepares a list of CSV filenames based on the input list `filenames` using the `prepare_csv_names` function.
# 
# 3. **Load CSV Contents:**
#    - `csv_contents <- load_csv_names(my_paths, csv_names_list)`: Loads the contents of CSV files specified by `my_paths` and `csv_names_list` using the `load_csv_names` function.
# 
# 4. **Clean CSVs:**
#    - `csvs_clean1 <- clean_all_csvs(csv_contents, vessel_id_field_name)`: Cleans all CSV files in `csv_contents` using the `clean_all_csvs` function, with an optional argument `vessel_id_field_name` to specify the vessel ID field.
# 
# 5. **Clean Correspondence Data:**
#    - `corresp_arr_contact_cnts_clean <- corresp_cleaning(csvs_clean1)`: Cleans the correspondence data in `csvs_clean1` using the `corresp_cleaning` function.
# 
# 6. **Extract Compliance Data:**
#    - `compl_arr <- csvs_clean1[2:length(csvs_clean1)]`: Extracts compliance data from `csvs_clean1` by excluding the first element (assumed to be correspondence data).
# 
# 7. **Clean Compliance Data:**
#    - `compl_clean <- compliance_cleaning(compl_arr)`: Cleans the compliance data in `compl_arr` using the `compliance_cleaning` function.
# 
# 8. **Return Result:**
#    - `return(list(compl_clean, corresp_arr_contact_cnts_clean))`: Returns a list containing the cleaned compliance data (`compl_clean`) and cleaned correspondence data (`corresp_arr_contact_cnts_clean`).
# 
# >>>>

from_fhier_data_path <-
  file.path(my_paths$inputs)

temp_var <-
  auxfunctions::get_compl_and_corresp_data(from_fhier_data_path, all_csv_names_list)

compl_clean_list <- temp_var[[1]]
corresp_contact_cnts_clean0 <- temp_var[[2]]

names(compl_clean_list) <- c(my_year1, my_year2)

# check
purrr::map(compl_clean_list, dim)

# combine years in one df
compl_clean <-
  rbind(compl_clean_list[[my_year1]], compl_clean_list[[my_year2]])

dim(compl_clean)

dim(corresp_contact_cnts_clean0)
```

### get Metric Tracking (permits from FHIER) 

```{r get Metric Tracking permits from FHIER}
## get Metric Tracking (permits from FHIER) ----
processed_input_data_path <- 
  file.path(my_paths$inputs,
            "processing_logbook_data",
            "Outputs")
dir.exists(processed_input_data_path)
# T  

# file names for all years
processed_metrics_tracking_file_names_all <-
  list.files(path = processed_input_data_path,
             pattern = "SEFHIER_permitted_vessels_nonSRHS_*",
             recursive = TRUE,
             full.names = TRUE)

# exclude links
processed_metrics_tracking_file_names <-
  grep(
    processed_metrics_tracking_file_names_all,
    pattern = "Shortcut.lnk",
    invert = TRUE,
    value = TRUE
  )

# read the rest
processed_metrics_tracking_permits <-
  purrr::map_df(processed_metrics_tracking_file_names,
         readr::read_rds)

# lower names case
names(processed_metrics_tracking_permits) <-
  names(processed_metrics_tracking_permits) |>
  tolower()

# [1] "vessel_official_number, vessel_name, effective_date, end_date, permits, sa_permits_, gom_permits_, permit_region, permit_sa_gom_dual"

dim(processed_metrics_tracking_permits)
```

### get Physical Address List from FHIER 

```{r get Physical Address List from FHIER}
## get Physical Address List from FHIER ----
# REPORTS / For-hire Primary Physical Address List

fhier_addresses_path <-
  file.path(
    my_paths$inputs,
    r"(from_Fhier\address\For-hire Primary Physical Address List_05_15_2024.csv)"
  )

file.exists(fhier_addresses_path)

fhier_addresses <-
  readr::read_csv(fhier_addresses_path,
           # read all as characters
           col_types = readr::cols(.default = 'c'),
           name_repair = auxfunctions::fix_names)

dim(fhier_addresses)
```

## PIMS 

```{r PIMS}
# PIMS ----
```

### get home port processed city and state 

```{r get home port processed city and state}
## get home port processed city and state ----

processed_pims_home_ports_path <-
  file.path(my_paths$outputs,
              "home_ports",
              "vessels_from_pims_ports.csv")

processed_pims_home_ports <- 
  readr::read_csv(processed_pims_home_ports_path)

# View(processed_pims_home_ports)
# View(vessels_from_pims) - more fields
```

## Oracle db 

```{r Oracle db}
# Oracle db ----
```

### get owners addresses 

```{r get owners addresses}
## get owners addresses ----
# <<<<
connect_to_secpr  <-  function() {
    # Retrieve the username associated with the "SECPR" database from the keyring.
    my_username <- keyring::key_list("SECPR")[1, 2]

    # Use 'dbConnect' to establish a database connection with the specified credentials.
    con <- DBI::dbConnect(
        DBI::dbDriver("Oracle"),  # Use the Oracle database driver.
        username = my_username,  # Use the retrieved username.
        password = keyring::key_get("SECPR", my_username),  # Retrieve the password from the keyring.
        dbname = "SECPR"  # Specify the name of the database as "SECPR."
    )

    # Return the established database connection.
    return(con)
}

# Explanations:

# This function encapsulates the process of connecting to the "SECPR" Oracle database securely by retrieving credentials from the keyring and establishing a connection using the ROracle package. It ensures that sensitive information such as passwords is not exposed in the code, enhancing security.
#  
#  
# This `connect_to_secpr` function establishes a connection to an Oracle database named "SECPR" using credentials stored securely.
# 
# 1. **Function Definition:**
#    - `connect_to_secpr <- function() { .. }`: Defines a function named `connect_to_secpr` with no input parameters.
# 
# 2. **Retrieve Username:**
#    - `my_username <- keyring::key_list("SECPR")[1, 2]`: Retrieves the username associated with the Oracle database "SECPR" from the keyring package. It accesses the first entry (row) and the second column, assuming the username is stored in the second column.
# 
# 3. **Establish Database Connection:**
#    - `con <- dbConnect(...)`:
#      - `dbDriver("Oracle")`: Creates a database driver object specifically for Oracle databases.
#      - `username = my_username`: Specifies the retrieved username for the database connection.
#      - `password = keyring::key_get("SECPR", my_username)`: Retrieves the corresponding password for the username from the keyring package, ensuring secure access to the credentials.
#      - `dbname = "SECPR"`: Specifies the name of the Oracle database to connect to, which is "SECPR" in this case.
#      - `dbConnect(...)`: Establishes a connection to the Oracle database using the provided driver, username, password, and database name.
# 
# 4. **Return Statement:**
#    - `return(con)`: Returns the established database connection (`con`) as the output of the function.
# 
# >>>>

# <<<<
read_rds_or_run  <-  function(my_file_path,
                            my_data = as.data.frame(""),
                            my_function,
                            force_from_db = NULL) {

  if (file.exists(my_file_path)) {
    modif_time <- file.info(my_file_path)$mtime
  }

    # Check if the file specified by 'my_file_path' exists and 'force_from_db' is not set.
    if (file.exists(my_file_path) &
        is.null(force_from_db)) {
        # If the file exists and 'force_from_db' is not set, read the data from the RDS file.

        function_message_print("File already exists, reading.")

        my_result <- readr::read_rds(my_file_path)

    } else {

      # If the file doesn't exist or 'force_from_db' is set, perform the following steps:

      # 0. Print this message.
      function_message_print(c(
        "File",
        my_file_path,
        "doesn't exists, pulling data from database.",
        "Must be on VPN."
      ))

      # 1. Generate a message indicating the date and the purpose of the run for "tic".
      msg_text <-
        paste(today(), "run for", basename(my_file_path))
      tictoc::tic(msg_text)  # Start timing the operation.

      # 2. Run the specified function 'my_function' on the provided 'my_data' to generate the result. I.e. download data from the Oracle database. Must be on VPN.

      my_result <- my_function(my_data)

      tictoc::toc()  # Stop timing the operation.

      # 3. Save the result as an RDS binary file to 'my_file_path' for future use.
      # try is a wrapper to run an expression that might fail and allow the user's code to handle error-recovery.

      # 4. Print this message.
      function_message_print(c("Saving new data into a file here: ",
                       my_file_path))

      try(readr::write_rds(my_result,
                           my_file_path))

      modif_time <- date()
    }

  # Print out the formatted string with the file name ('my_file_name') and the modification time ('modif_time') to keep track of when the data were downloaded.
  my_file_name <- basename(my_file_path)
  function_message_print(
    stringr::str_glue("File: {my_file_name} modified {modif_time}"))

    # Return the generated or read data.
    return(my_result)
}

# Explanations:

# This function, `read_rds_or_run`, is designed to read data from an RDS file if it exists or run a specified function to obtain the data from the Oracle database and save it as an RDS file if the file does not exist or if the `force_from_db` parameter is set.
#  
#  
# 1. **Check File Existence**: The function first checks if the file specified by `my_file_path` exists and, if so, retrieves its modification time.
# 
# 2. **Read or Run**: Depending on the existence of the file and the `force_from_db` flag:
#     - **File Exists and `force_from_db` is not set**: If the file exists and `force_from_db` is not set, the function reads the data from the RDS file using `readr::read_rds(my_file_path)` and assigns it to `my_result`.
#     - **File Does Not Exist or `force_from_db` is set**: If the file does not exist or `force_from_db` is set, the function follows these steps:
#         - Prints a message indicating the file doesn't exist and data will be pulled from the database.
#         - Times the function execution using `tictoc::tic()` and starts with a message indicating the date and purpose of the run.
#         - Runs the specified function (`my_function`) on the provided `my_data` to generate the result (`my_result`), e.g., downloading data from the Oracle database.
#         - Stops timing the function execution using `tictoc::toc()`.
#         - Saves the result as an RDS file to the specified `my_file_path` for future use using `readr::write_rds(my_result, my_file_path)`. A `try` block is used to handle potential errors in writing the file.
#         - Prints a message indicating that the new data is being saved into a file.
# 
# 3. **Print File Information**: After obtaining the data, the function prints the file name and modification time to provide information on when the data was last downloaded or modified.
# 
# 4. **Return**: The function returns the generated or read data (`my_result`).
# 
# 
# >>>>

# <<<<
remove_empty_cols  <-  function(my_df) {
  # Define an inner function "not_all_na" that checks if any value in a vector is not NA.
  not_all_na <- function(x) any(!is.na(x))

  my_df |>
    # Select columns from "my_df" where the result of the "not_all_na" function is true,
    # i.e., select columns that have at least one non-NA value.
    dplyr::select(tidyselect::where(not_all_na)) %>%
    # Return the modified data frame, which contains only the selected columns.
    return()
}

# Explanations:

# This function effectively removes columns from the input data frame `my_df` that contain only missing values.
#  
#  
# 1. **Function Definition:**
#    - `remove_empty_cols <- function (my_df)`: Defines a function named `remove_empty_cols` that takes a single argument `my_df`, which is expected to be a data frame.
# 
# 2. **Inner Function Definition:**
#    - `not_all_na <- function(x) any(!is.na(x))`: Defines an inner function named `not_all_na`. This function takes a vector `x` as input and returns `TRUE` if there is at least one non-missing value in the vector, otherwise it returns `FALSE`. This function will be used as a predicate to check if any column contains non-missing values.
# 
# 3. **Selecting Columns:**
#    - `select(my_df, where(not_all_na))`: Uses the `dplyr` function `select` to filter columns of `my_df` based on a condition. The condition is specified using the `where` function, which applies the `not_all_na` function to each column of `my_df`. Columns for which `not_all_na` returns `TRUE` (i.e., columns with at least one non-missing value) are retained, while columns with all missing values are removed.
# 
# 4. **Returning Result:**
#    - `%>% return()`: Pipes the result of the `select` operation into the `return` function, which ensures that the modified data frame is returned as the output of the `remove_empty_cols` function.
# 
# >>>>

# <<<<
clean_headers  <-  function(my_df) {
    # Use the 'fix_names' function to clean and fix the column names of the dataframe.
    new_names <-
        colnames(my_df) |>
        fix_names()

    colnames(my_df) <- 
        new_names
    
    # Return the dataframe with cleaned and fixed column names.
    return(my_df)
}

# Explanations:

# The clean_headers function is designed to clean and fix the column names (using fix_names()) of a given dataframe (my_df).
#  
#  
# >>>>

db_participants_address_query <-
  "select * from
SRH.MV_SERO_VESSEL_ENTITY@secapxdv_dblk
"

db_participants_address_file_path <-
  file.path(my_paths$inputs,
            current_project_name,
            "db_participants_address.rds")
 
# dir.exists(file.path(my_paths$inputs,
#             current_project_name))

# err msg if no connection, but keep running
if (!exists("con")) {
  try(con <- auxfunctions::connect_to_secpr())
}

db_participants_address_fun <-
  function(db_participants_address) {
    # browser()
    return(dbGetQuery(con,
                      db_participants_address))
  }

db_participants_address <-
  auxfunctions::read_rds_or_run(
    db_participants_address_file_path,
    db_participants_address_query,
    db_participants_address_fun
    # ,
    # force_from_db = "yes"
  ) |>
  auxfunctions::remove_empty_cols() |>
  auxfunctions::clean_headers()
# 2024-04-09 run for db_participants_address.rds: 52.22 sec elapsed

dim(db_participants_address)
```

## Data from the previous results of "egregious violators for investigation" 

```{r Data from the previous results of egregious violators for investigation}
# Data from the previous results of "egregious violators for investigation" ----
# <<<<
my_read_xlsx  <-  function(file_path, sheet_n, start_row = 1) {
  res_df <-
    openxlsx::read.xlsx(
      file_path,
      sheet_n,
      startRow = start_row,
      detectDates = TRUE,
      colNames = TRUE,
      sep.names = "_"
    ) |>
    clean_headers()

  return(res_df)
}

# Explanations:

# This function reads data from an Excel file, optionally starting from a specified row, and applies header cleaning to ensure consistent formatting of column names.
#  
#  
# 1. **Function Definition:**
#    - `my_read_xlsx <- function(file_path, sheet_n, start_row = 1) { ... }`: Defines a function named `my_read_xlsx` with three arguments: `file_path` (the path to the Excel file), `sheet_n` (the index of the sheet to read), and `start_row` (the starting row from which to read data, with a default value of 1).
# 
# 2. **Reading Excel File:**
#    - `read.xlsx(file_path, sheet_n, startRow = start_row, detectDates = TRUE, colNames = TRUE, sep.names = "_")`: Uses the `read.xlsx` function from the `openxlsx` package to read data from the specified Excel file.
#      - `file_path`: The path to the Excel file to be read.
#      - `sheet_n`: The index of the sheet to read from.
#      - `startRow`: The row number from which to start reading data.
#      - `detectDates`: A logical value indicating whether to automatically detect and convert date-like columns to date format.
#      - `colNames`: A logical value indicating whether the first row of the Excel sheet contains column names.
#      - `sep.names`: A character used to separate column names with multiple parts (e.g., spaces or underscores).
# 
# 3. **Cleaning Headers:**
#    - `clean_headers(...)`: Applies the `clean_headers` function to the data frame read from the Excel file. This function likely performs tasks such as removing leading/trailing spaces, converting column names to lowercase, and replacing spaces with underscores.
# 
# 4. **Return Result:**
#    - `return(res_df)`: Returns the cleaned data frame obtained from reading the Excel file. This data frame contains the data read from the specified Excel sheet, with any necessary header cleaning applied.
# 
# 
# >>>>

# Download first as .xlsx

# get previous results ---
prev_result_path <- 
  file.path(my_paths$inputs,
            current_project_basename,
            "egregious_violators_to_investigate_2024-04-10.xlsx")

file.exists(prev_result_path)

prev_result <-
  auxfunctions::my_read_xlsx(prev_result_path) |> 
  auxfunctions::remove_empty_cols() |>
  auxfunctions::clean_headers()

dim(prev_result)
```

## Results 

```{r Results}
# Results ----
results <-
  c(
    "compl_clean",
    "corresp_contact_cnts_clean0",
    "prev_result",
    "processed_metrics_tracking_permits",
    "fhier_addresses",
    "processed_pims_home_ports",
    "db_participants_address"
  )

cat(c("Data are in:",
      results),
    sep = "\n")


# Data are in:
# compl_clean
# corresp_contact_cnts_clean0
# prev_result
# processed_metrics_tracking_permits
# fhier_addresses
# processed_pims_home_ports
# db_participants_address
```

## Preparing compliance info 

```{r Preparing compliance info}
# Preparing compliance info ----
```

### Permit Expiration 

```{r Permit Expiration}
## Permit Expiration ----
```

#### add permit_expired column 

```{r add permit_expired column}
### add permit_expired column ----
# Explanations:
# 1. Add a new column 'permit_expired' using 'mutate'.
# 2. Use 'case_when' to determine if 'permit_groupexpiration' is greater than permit_expired_check_date.
# 3. If true, set 'permit_expired' to "no", otherwise set it to "yes".

compl_clean_w_permit_exp <-
  compl_clean |>
  # if permit group expiration is after permit_expired_check_date than "not expired"
  dplyr::mutate(permit_expired =
           dplyr::case_when(
             permit_groupexpiration > permit_expired_check_date ~ "no",
             .default = "yes"
           ))

# glimpse(compl_clean_w_permit_exp)
```

#### get only not expired last 27 weeks of data minus grace period 

```{r get only not expired last 27 weeks of data minus grace period}
### get only not expired last 27 weeks of data minus grace period ----
compl_clean_w_permit_exp__not_exp <-
  compl_clean_w_permit_exp |>
  # the last 27 week
  dplyr::filter(week_start > half_year_ago) |>
  # before the last week (a report's grace period)
  dplyr::filter(week_end < last_week_start) |>
  # not expired
  dplyr::filter(tolower(permit_expired) == "no")

min(compl_clean_w_permit_exp__not_exp$permit_groupexpiration)
# [1] "2024-02-29 EST"

min(compl_clean_w_permit_exp__not_exp$week_start)
# [1] "2023-08-14"

max(compl_clean_w_permit_exp__not_exp$week_start)
# [1] "2024-01-29"

max(compl_clean_w_permit_exp__not_exp$week_end)
# [1] "2024-02-04"
```

### add year_month column 

```{r add year_month column}
## add year_month column ----

compl_clean_w_permit_exp_last_half_year <-
  compl_clean_w_permit_exp__not_exp |>
  dplyr::mutate(year_month = as.yearmon(week_start)) |>
  # keep entries for the last check period
  dplyr::filter(year_month >= as.yearmon(half_year_ago))

dim(compl_clean_w_permit_exp)

dim(compl_clean_w_permit_exp_last_half_year)
```

### Have only SA and dual permits 

```{r Have only SA and dual permits}
## Have only SA and dual permits ----
# Use 'filter' to select rows where 'permitgroup' contains "CDW", "CHS", or "SC".
compl_clean_w_permit_exp_last_half_year__sa <-
  compl_clean_w_permit_exp_last_half_year |>
  dplyr::filter(grepl("CDW|CHS|SC", permitgroup))

# lubridate::today()
# [1] "2023-08-01"
# [1] "2023-07-10"
# [1] "2023-08-10"
# [1] "2024-02-16"
# [1] "2024-04-09"
# [1] "2024-05-16"

dim(compl_clean_w_permit_exp_last_half_year__sa)
```

### fewer columns 

```{r fewer columns}
## fewer columns ----
remove_columns <- c(
  "name",
  "gom_permitteddeclarations__",
  "captainreports__",
  "negativereports__",
  "complianceerrors__",
  "set_permits_on_hold_",
  "override_date",
  "override_by",
  "contactedwithin_48_hours_",
  "submittedpower_down_",
  "permit_expired"
)

# Explanations:
# 1. Use 'select' to remove columns specified in 'remove_columns'.
# 2. Use 'distinct' to keep only unique rows in the resulting data frame.
compl_clean_w_permit_exp_last_half_year__sa__short <-
  compl_clean_w_permit_exp_last_half_year__sa |>
  dplyr::select(-tidyselect::any_of(remove_columns)) |> 
  dplyr::distinct()

dim(compl_clean_w_permit_exp_last_half_year__sa__short)
```

### work with the whole period 

```{r work with the whole period}
## work with the whole period ----
```

### add compliant_after_overr 

```{r add compliant_after_overr}
## add compliant_after_overr ----
# <<<<
add_compliant_after_override  <-  function(my_compl_df,
           overridden_col_name = "overridden",
           compliance_col_name = "is_comp") {
  # browser()
  res <-
    my_compl_df |>
    dplyr::rowwise() |>
    dplyr::mutate(
      compliant_after_override =
        dplyr::case_when(
          !!sym(compliance_col_name) %in% c(0, "NO") &
            !!sym(overridden_col_name) %in% c(0, "NO")  ~ "no",
          !!sym(compliance_col_name) %in% c(1, "YES") ~ "yes",
          !!sym(overridden_col_name) %in% c(1, "YES") ~ "yes",
          is.na(!!sym(compliance_col_name)) ~ NA,
          .default = toString(!!sym(compliance_col_name))
        )
    ) |>
    ungroup()

  return(res)
}

# Explanations:

# Explanations:
# 1. Create a new variable 'res' to store the result.
# 2. Use 'rowwise' to perform operations row by row.
# 3. Use 'mutate' to create a new column 'compliant_after_override' based on conditions specified in 'case_when'.
#    - If 'is_comp' is 0 and 'overridden' is 0, set 'compliant_after_override' to "no".
#    - If 'is_comp' is 1 or 'overridden' is 1, set 'compliant_after_override' to "yes".
#    - If 'is_comp' is NA, set 'compliant_after_override' to NA.
#    - For all other cases, set 'compliant_after_override' to the string representation of 'is_comp'.
# 4. Use 'ungroup' to remove grouping from the data frame.
#  
#  
# >>>>


tictoc::tic("compl_overr")
compl_clean_w_permit_exp_last_half_year__sa__short__comp_after_overr <-
  compl_clean_w_permit_exp_last_half_year__sa__short |>
  auxfunctions::add_compliant_after_override(overridden_col_name = "overridden_",
                                             compliance_col_name = "compliant_")
tictoc::toc()
# compl_overr: 8.76 sec elapsed

# check
compl_clean_w_permit_exp_last_half_year__sa__short__comp_after_overr |> 
  dplyr::select(compliant_, overridden_, compliant_after_override) |>
  dplyr::count(compliant_, overridden_, compliant_after_override)
#   compliant_ overridden_ compliant_after_override     n
#   <chr>      <chr>       <chr>                    <int>
# 1 NO         NO          no                       11258
# 2 NO         YES         yes                         70
# 3 YES        NO          yes                      29628

# check
compl_clean_w_permit_exp_last_half_year__sa__short__comp_after_overr$compliant_after_override |> 
  unique()
# [1] "yes" "no" 

dim(compl_clean_w_permit_exp_last_half_year__sa__short__comp_after_overr)

# check
dplyr::n_distinct(compl_clean_w_permit_exp_last_half_year__sa$vessel_official_number) ==
  dplyr::n_distinct(
    compl_clean_w_permit_exp_last_half_year__sa__short__comp_after_overr$vessel_official_number
  )
# T
```

### get only non-compliant for the past half year 

```{r get only noncompliant for the past half year}
## get only non-compliant for the past half year ----
compl_clean_w_permit_exp_last_half_year__sa_non_c <-
  compl_clean_w_permit_exp_last_half_year__sa__short__comp_after_overr |>
  # not compliant
  dplyr::filter(tolower(compliant_after_override) == "no")

dim(compl_clean_w_permit_exp_last_half_year__sa_non_c)
```

### keep only vessels with info for all weeks in the period 

```{r keep only vessels with info for all weeks in the period}
## keep only vessels with info for all weeks in the period ----
all_weeks_num <-
  compl_clean_w_permit_exp_last_half_year__sa_non_c |>
  dplyr::select(week) |>
  dplyr::distinct() |>
  nrow()

# Explanations:
# 1. Group the data frame by 'vessel_official_number'.
# 2. Filter the groups based on the condition that the number of distinct weeks is greater than or equal to 'all_weeks_num'.
# 3. Remove the grouping from the data frame.
# 4. Exclude the 'week' column from the resulting data frame, we don't need it anymore.

compl_clean_w_permit_exp_last_half_year__sa_non_c__all_weeks_present <-
  compl_clean_w_permit_exp_last_half_year__sa_non_c |>
  dplyr::group_by(vessel_official_number) |>
  dplyr::filter(dplyr::n_distinct(week) >= all_weeks_num) |> 
  dplyr::ungroup() |> 
  dplyr::select(-week)

compl_clean_w_permit_exp_last_half_year__sa_non_c |> dim()

dim(compl_clean_w_permit_exp_last_half_year__sa_non_c__all_weeks_present)
```

### check the last report date 

```{r check the last report date}
## check the last report date ----
```

#### get ids only 

```{r get ids only}
### get ids only ----
compl_clean_w_permit_exp_last_half_year__sa_non_c__all_weeks_present__vesl_ids <-
  compl_clean_w_permit_exp_last_half_year__sa_non_c__all_weeks_present |>
  dplyr::select(vessel_official_number) |>
  dplyr::distinct()

dim(compl_clean_w_permit_exp_last_half_year__sa_non_c__all_weeks_present__vesl_ids)
```

#### check these ids in the full compliance information 

```{r check these ids in the full compliance information}
### check these ids in the full compliance information ----
compl_clean_w_permit_exp_last_half_year__sa |>
  dplyr::filter(
    vessel_official_number %in% compl_clean_w_permit_exp_last_half_year__sa_non_c__all_weeks_present__vesl_ids$vessel_official_number
  ) |>
  # dim()
  # [1] 3146   23
  # [1] 1938   22
  dplyr::group_by(vessel_official_number) |>
  dplyr::filter(
    tolower(compliant_) == "yes" &
      tolower(overridden_) == "yes" &
      # not the current month
      year_month < as.yearmon(data_file_date)
  ) |>
  nrow()
# 0 OK!

# Results: prepared Compliance is in compl_clean_w_permit_exp_last_half_year__sa_non_c__all_weeks_present
```

## Preparing Correspondence 

```{r Preparing Correspondence}
# Preparing Correspondence ----
```

### remove 999999 

```{r remove 999999}
## remove 999999 ----
# Explanations:
# Create a new data frame 'corresp_contact_cnts_clean' by filtering 'corresp_contact_cnts_clean0' based on the condition.
# 1. Use 'filter' to select rows where 'vessel_official_number' does not start with "99999".
corresp_contact_cnts_clean <-
  corresp_contact_cnts_clean0 |>
  dplyr::filter(!grepl("^99999", vessel_official_number))

dplyr::n_distinct(corresp_contact_cnts_clean$vesselofficial_number)

# "2023-08-09"
# Michelle
# It should be at least 2 contact "attempts". i.e., if they are ignoring our calls and emails then they cannot continue to go on in perpetuity without reporting and never be seen as egregious. So, at least 1 call (could be a voicemail) and also at a 2nd call (could be a voicemail) or an email. So, if we called 1x and left a voicemail and then attempted an email, then we have tried enough at this point and they need to be passed to OLE.
```

### new requirement 2023-08-09 

```{r new requirement 20230809}
## new requirement 2023-08-09 ----
# at least 1 call (could be a voicemail) and also at a 2nd call (could be a voicemail) or an email. So, if we called 1x and left a voicemail and then attempted an email, then we have tried enough
```

### new requirement 2024-02-26 

```{r new requirement 20240226}
## new requirement 2024-02-26 ----
# It needs to be that we called at least 1 time and emailed at least 1 time. Or they contacted us at least once.
```

### new requirement 2024-05-06 

```{r new requirement 20240506}
## new requirement 2024-05-06 ----
# Exclude any correspondence (regardless of the type - email/call, voicemail or not) that includes "No contact made" in the text of the entry as a actual "direct" contact for any egregious vessel.

# check
corresp_contact_cnts_clean |>
  dplyr::select(calltype, voicemail, contacttype) |>
  dplyr::distinct() |> head(10)
```

### Filters 

```{r Filters}
## Filters ----
# The functions below are creating filter conditions using quosures. Quosures are a part of tidy evaluation in R, allowing expressions to be captured without evaluation, which is useful for creating functions with flexible inputs.

we_called_filter <-
  dplyr::quo(any(tolower(contacttype) == "call" &
        tolower(calltype) == "outgoing"))

we_emailed_once_filter <-
  dplyr::quo(any(
    tolower(contacttype) %in% c("email", "other") &
      tolower(calltype) == "outgoing"
  ))

# Explanations:
# 
# **Expression inside quo()**:
#    - `!grepl("No contact made", contactcomments, ignore.case = TRUE)`: This expression is a negation of the `grepl` function, which is used to search for a pattern ("No contact made") in the `contactcomments` column.
#    - `grepl()` returns `TRUE` for each element of `contactcomments` that contains the pattern, and `FALSE` otherwise.
#    - The `!` operator negates the result, so the filter condition will be `TRUE` for rows where "No contact made" is not found in the `contactcomments` column.
# 
# The `exclude_no_contact_made_filter` function effectively creates a filter condition that can be used to exclude rows where "No contact made" is found in the `contactcomments` column when applied to a dataset.
exclude_no_contact_made_filter <-
  dplyr::quo(!grepl("No contact made", 
            contactcomments, 
            ignore.case = TRUE))

# don't need a second contact
they_contacted_direct_filter <-
  dplyr::quo(
    any(
      tolower(calltype) == "incoming"
      )
  )

# corresp_filter <-
#   quo(!!they_contacted_direct_filter |
#         (
#           contact_freq > 1 &
#             (!!we_called_filter &
#                !!we_emailed_once_filter)
#         ))

# calltype voicemail contacttype

# two_attempts_filter <-
#   quo(contact_freq > 1 &
#         any(tolower(contacttype) == "call"))
```

#### use the filters 

```{r use the filters}
### use the filters ----
corresp_contact_cnts_clean_direct_cnt_2atmps <-
  corresp_contact_cnts_clean |>
  # select(calltype) |> distinct()
  dplyr::filter(tolower(calltype) == "incoming" |
           (
             contact_freq > 1 &
               (!!we_called_filter &
                  !!we_emailed_once_filter)
           )) |> 
  dplyr::filter(!!exclude_no_contact_made_filter)

dim(corresp_contact_cnts_clean)
dim(corresp_contact_cnts_clean_direct_cnt_2atmps)

dplyr::n_distinct(corresp_contact_cnts_clean_direct_cnt_2atmps$vesselofficial_number)
```

### fix dates 

```{r fix dates}
## fix dates ----
# check
head(corresp_contact_cnts_clean_direct_cnt_2atmps$contact_date, 1) |> str()
 # chr "02/15/2024 03:15PM"

# Explanations:
# Mutate new columns 'created_on_dttm' and 'contact_date_dttm' by parsing 'created_on' and 'contact_date' using lubridate package.
# The date-time formats considered are "mdY R".
# 1. Use the pipe operator to pass 'corresp_contact_cnts_clean_direct_cnt_2atmps' as the left-hand side of the next expression.
# 2. Use 'mutate' to create new columns with parsed date-time values.
# 3. Use 'lubridate::parse_date_time' to parse the date-time values using the specified formats.

corresp_contact_cnts_clean_direct_cnt_2atmps_clean_dates <-
  corresp_contact_cnts_clean_direct_cnt_2atmps |>
  dplyr::mutate(
    created_on_dttm =
      lubridate::parse_date_time(created_on,
                                 c("mdY R")),
    contact_date_dttm =
      lubridate::parse_date_time(contact_date,
                                 c("mdY R"))
  )

# check
str(corresp_contact_cnts_clean_direct_cnt_2atmps_clean_dates$contact_date_dttm)
# POSIXct[1:29089], format: "2024-02-15 15:15:00" 

# preprared Correspondence is in 
# corresp_contact_cnts_clean_direct_cnt_2atmps_clean_dates
```

## Join correspondence with compliance 

```{r Join correspondence with compliance}
# Join correspondence with compliance ----
# Explanations:
# Create a new dataframe 'compl_corr_to_investigation' by performing an inner join between
# 'correspondence' and 'compliance'.
# The join is performed on the column 'vessel_official_number'.
# Use 'multiple = "all"' and 'relationship = "many-to-many"' to handle multiple matches during the join.
# 1. Use the 'inner_join' function from the dplyr package to combine the two dataframes based on the specified columns.
# 2. Pass the column names and other parameters to the 'by', 'multiple', and 'relationship' arguments.

compl_corr_to_investigation <-
  dplyr::inner_join(
    corresp_contact_cnts_clean_direct_cnt_2atmps_clean_dates,
    compl_clean_w_permit_exp_last_half_year__sa_non_c__all_weeks_present,
    by = c("vessel_official_number"),
    multiple = "all",
    relationship = "many-to-many"
  )

dim(compl_corr_to_investigation)

# check
dplyr::n_distinct(compl_corr_to_investigation$vesselofficial_number)

# View(compl_corr_to_investigation)
```

### save number of vessels to investigate for checks 

```{r save number of vessels to investigate for checks}
## save number of vessels to investigate for checks ----
num_of_vsl_to_investigate <- 
  dplyr::n_distinct(compl_corr_to_investigation$vesselofficial_number)

# Results: Compl & corresondence together are in
# compl_corr_to_investigation
```

## output needed investigation 

```{r output needed investigation}
# output needed investigation ----
```

# Prepare output


```{r}
# 1. remove unused columns
# 2. create additional columns
# 3. mark vessels already in the know list (prev_result)
# 4. duals vs. sa_only
```

### 1. remove extra columns 

```{r 1 remove extra columns}
## 1. remove extra columns ----

# Explanations:
# Group the dataframe by the 'vessel_official_number' column and then apply the 'summarise_all' function.
# The 'summarise_all' function applies the specified function (in this case, 'concat_unique') to each column.

colnames(compl_corr_to_investigation) |> 
  cat(sep = '",\n"')

unused_fields <- c(
  "vesselofficial_number",
  "primary",
  # "contact_date",
  "follow_up",
  "log_group",
  "calltype",
  "voicemail",
  # "contacttype",
  "contact_reason",
  # "contactrecipientname",
  # "contactphone_number",
  # "contactemailaddress",
  "contactcomments",
  "srfhuser",
  "created_on",
  "follow_up_nbr",
  "srhs_vessel",
  # "vessel_official_number",
  "was_contacted",
  "contact_freq",
  "created_on_dttm",
  # "contact_date_dttm",
  # "name",
  # "permit_expired",
  # "permitgroup",
  # "permit_groupexpiration",
  "compliant_after_override")

# Explanations:
# 1. Exclude columns specified in 'unused_fields' from the data frame.
# 2. Group the data frame by 'vessel_official_number'.
# 3. Apply the custom function 'concat_unique' to all columns to concatenate unique non-missing values into a single string.
# 4. Remove the grouping from the data frame.

compl_corr_to_investigation_short <-
  compl_corr_to_investigation |>
  # compl_corr_to_investigation_w_non_compliant_weeks_n_date__contacttype_per_id |>
  dplyr::select(-any_of(unused_fields)) |>
  dplyr::group_by(vessel_official_number) |>
  dplyr::summarise_all(auxfunctions::concat_unique) |>
  dplyr::ungroup()

# print_df_names(compl_corr_to_investigation_short)

compl_corr_to_investigation_short |> dplyr::glimpse()

dim(compl_corr_to_investigation_short)
```

### 2. create additional columns 

```{r 2 create additional columns}
## 2. create additional columns ----
```

#### add list of contact dates and contact type in parentheses  -

```{r add list of contact dates and contact type in parentheses}
### add list of contact dates and contact type in parentheses  -----
# <<<<
find_col_name  <-  function(mydf, start_part, end_part) {
  # Create a regular expression pattern to search for column names that start with 'start_part'
  # and end with 'end_part'.
  to_search <- paste0(start_part, ".*", end_part)

  # Use 'grep' to search for column names in lowercase that match the pattern.
  # 'value = TRUE' returns the matching column names as a character vector.
  matching_names <- grep(to_search, tolower(names(mydf)), value = TRUE)

  # Return the matching column name(s) as a character vector.
  return(matching_names)
}

# Explanations:

# This function provides a flexible way to search for column names within a dataframe based on specified starting and ending patterns. It is useful for identifying columns that match a particular naming convention or pattern. 
#  
# The function is designed to search for column names within a dataframe that contain a specific pattern defined by `start_part` and `end_part`.
# 
# 1. **Function Definition:**
#    -`function (mydf, start_part, end_part) { .. }`: Defines a function with three arguments: `mydf` (the dataframe to search), `start_part` (the starting pattern of the column names to search for), and `end_part` (the ending pattern of the column names to search for).
# 
# 2. **Constructing Regular Expression:**
#    - `to_search <- paste0(start_part, ".*", end_part)`: Constructs a regular expression pattern by concatenating `start_part`, ".*" (which matches any characters zero or more times), and `end_part`. This pattern will be used to search for matching column names.
# 
# 3. **Searching for Matching Column Names:**
#    - `matching_names <- grep(to_search, tolower(names(mydf)), value = TRUE)`: Searches for column names in the dataframe `mydf` that match the regular expression pattern `to_search`. The `tolower` function is used to convert the column names to lowercase for case-insensitive matching. The `grep` function returns the matching column names as a vector.
# 
# 4. **Return Statement:**
#    - `return(matching_names)`: Returns the vector of matching column names found in the dataframe that satisfy the specified pattern.
# 
# >>>>


# put names into vars (needed, bc spaces and underscores placements vary from source to source)
contactdate_field_name <-
  auxfunctions::find_col_name(compl_corr_to_investigation_short, "contact", "date")[1]

contacttype_field_name <-
  auxfunctions::find_col_name(compl_corr_to_investigation_short, "contact", "type")[1]

contactphonenumber_field_name <-
  auxfunctions::find_col_name(compl_corr_to_investigation_short, ".*contact", "number.*")[1]

# Explanations:
# Define a function 'get_date_contacttype' that takes a dataframe 'compl_corr_to_investigation' as input.
# Perform several data manipulation steps to extract and organize relevant information.
# 1. Add a new column 'date__contacttype' by concatenating the values from 'contactdate_field_name' and 'contacttype'.
# 2. Select only the 'vessel_official_number' and 'date__contacttype' columns.
# 3. Arrange the dataframe by 'vessel_official_number' and 'date__contacttype'.
# 4. Keep distinct rows based on 'vessel_official_number' and 'date__contacttype'.
# 5. Group the dataframe by 'vessel_official_number'.
# 6. Summarize the data by creating a new column 'date__contacttypes' that concatenates all 'date__contacttype' values for each vessel separated by a comma.
# 7. Return the resulting dataframe.
get_date_contacttype <-
  function(my_df) {
  
    res <-
      my_df |>
      # add a new column date__contacttype with contactdate and contacttype
      dplyr::mutate(date__contacttype =
                      paste(
                        !!rlang::sym(contactdate_field_name),
                        !!rlang::sym(contacttype_field_name)
                      )) |>
      # use 2 columns only
      dplyr::select(vessel_official_number, date__contacttype) |>
      # sort
      dplyr::arrange(vessel_official_number, date__contacttype) |>
      dplyr::distinct() |>
      dplyr::group_by(vessel_official_number) |>
      # for each vessel id combine all date__contacttypes separated by comma in one cell
      dplyr::summarise(date__contacttypes =
                         paste(date__contacttype, collapse = ", "))
    
    return(res)
  }

# use the function
date__contacttype_per_id <-
  get_date_contacttype(compl_corr_to_investigation_short)

dim(date__contacttype_per_id)

dplyr::glimpse(date__contacttype_per_id)
```

##### add the new column back 

```{r add the new column back}
#### add the new column back ----
compl_corr_to_investigation__corr_date <-
  dplyr::left_join(compl_corr_to_investigation_short,
            date__contacttype_per_id) |>
  # Joining with `by = join_by(vessel_official_number)`
  # this columns are not longer needed
  dplyr::select(-dplyr::all_of(c(
    contactdate_field_name,
    contacttype_field_name
  )))
  
# check
compl_corr_to_investigation__corr_date |> 
  dplyr::glimpse()
```

#### add pims home port info 

```{r add pims home port info}
### add pims home port info ----
# compl_corr_to_investigation_short_dup_marked__hailing_port <-
compl_corr_to_investigation__corr_date__hailing_port <- 
  dplyr::left_join(
    compl_corr_to_investigation__corr_date,
    processed_pims_home_ports,
    dplyr::join_by(vessel_official_number)
  ) |> 
  dplyr::rename("hailing_port_city" = city_fixed,
         "hailing_port_state" = state_fixed)
```

#### add prepared addresses 

```{r add prepared addresses}
### add prepared addresses ----

prep_addresses_path <-
  file.path(current_project_path,
            stringr::str_glue("{current_project_basename}_prep_addresses.R"))

file.exists(prep_addresses_path)

# Manually check missing addresses
```

## From FHIER 

```{r From FHIER}
# From FHIER ----
```

### fewer fields 

```{r fewer fields}
## fewer fields ----
# <<<<
clean_names_and_addresses  <-  function(my_df) {
    my_df_cleaned <-
      my_df |>
      dplyr::mutate(
        dplyr::across(dplyr::where(is.character), ~ stringr::str_squish(.x)),
        dplyr::across(dplyr::where(is.character), ~ tidyr::replace_na(.x, "")),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, ", ;", ";")
        ),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, "\\s+[,;]", ",")
        ),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, ";,+", ";")
        ),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, ";;+", ";")
        ),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, ",,+", ",")
        ),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, "[,;] *\\bUN\\b *", "")
        ),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, "\\bUN\\b", "")
        ),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, "\\s*\\bUN\\b\\s*", "")
        ),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, "^[,;] ", "")
        ),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, "^[,;]$", "")
        ),
        dplyr::across(
          dplyr::where(is.character),
          ~ stringr::str_replace_all(.x, "[,;]$", "")
        ),
        dplyr::across(dplyr::where(is.character), 
                      ~ stringr::str_squish(.x))
      )

  return(my_df_cleaned)
}

# Explanations:

# - This function `clean_names_and_addresses` cleans the strings in character columns of a dataframe.
# - It uses `mutate` with `across` to apply multiple string cleaning operations to all character columns.
# - The `str_squish` function is used to remove leading, trailing, and extra internal whitespace from each string.
# - `replace_na` function replaces missing values with empty strings.
# - The `str_replace_all` function is used to perform multiple replacements using regular expressions to clean up commas, semicolons, and unwanted substrings like "UN".
# - Finally, it returns the cleaned dataframe. 
#  
# >>>>

# fhier_addresses are from get_data (For-hire Primary Physical Address List)
fhier_addr_short <-
  fhier_addresses |>
  dplyr::select(
    vessel_official_number,
    permit_holder_names,
    physical_address_1,
    physical_address_2,
    physical_city,
    physical_county,
    physical_state,
    physical_zip_code,
    phone_number,
    primary_email
  )

fhier_addr_short_clean <-
  fhier_addr_short |>
  auxfunctions::clean_names_and_addresses() |> 
  dplyr::distinct()

# nrow(fhier_addr_short_clean)

# don't combine address
# fhier_addr_short__comb_addr <- 
#   fhier_addr_short |> 
#   clean_names_and_addresses() |> 
#   mutate(
#     fhier_address =
#       str_glue(
#         "
#         {physical_address_1}, {physical_address_2}, {physical_city}, {physical_county}, {physical_state}, {physical_zip_code}
#       "
#       )
#   ) |>
#   select(
#     -c(
#       physical_address_1,
#       physical_address_2,
#       physical_city,
#       physical_county,
#       physical_state,
#       physical_zip_code
#     )
#   ) |>
#   clean_names_and_addresses() |> 
#   distinct()

# dim(fhier_addr_short__comb_addr)
# [1] 2390    5

# dim(fhier_addr_short_clean)
```

### add addresses from FHIER 

```{r add addresses from FHIER}
## add addresses from FHIER ----
compl_corr_to_investigation__corr_date__hailing_port__fhier_addr <-
  left_join(compl_corr_to_investigation__corr_date__hailing_port,
            fhier_addr_short_clean)
# Joining with `by = join_by(vessel_official_number)`

# View(compl_corr_to_investigation__corr_date__hailing_port__fhier_addr)

# check if no name, phone or email
compl_corr_to_investigation__corr_date__hailing_port__fhier_addr |>
  filter(
    is.na(contactrecipientname) |
      is.na(contactphone_number) |
      is.na(contactemailaddress)
  ) |> nrow()
# 0
```

### vessels with no addresses 

```{r vessels with no addresses}
## vessels with no addresses ----

# print_df_names(compl_corr_to_investigation__corr_date__hailing_port__fhier_addr)

### Explanation:

# This code snippet creates a dataframe `no_addr_vsl_ids` containing unique `vessel_official_number` values based on certain conditions.
# 
# 1. **Starting with the DataFrame:**
#    - `compl_corr_to_investigation__corr_date__hailing_port__fhier_addr |>`: Pipes the dataframe `compl_corr_to_investigation__corr_date__hailing_port__fhier_addr` into the next function.
# 
# 2. **Filtering Rows:**
#    - `dplyr::filter(physical_address_1 %in% is_empty)`: Uses the `filter` function from the `dplyr` package to keep rows where the `physical_address_1` column is empty.
#      - `physical_address_1 %in% is_empty`: This condition checks if the values in the `physical_address_1` column are empty.
# 
# 3. **Selecting Columns:**
#    - `dplyr::select(vessel_official_number)`: Selects only the `vessel_official_number` column from the filtered dataframe.
# 
# 4. **Removing Duplicate Rows:**
#    - `dplyr::distinct()`: Removes duplicate rows from the dataframe, ensuring that each `vessel_official_number` appears only once in the final result.
# 
# The resulting `no_addr_vsl_ids` dataframe contains unique `vessel_official_number` values where the corresponding `physical_address_1` column is empty in the `compl_corr_to_investigation__corr_date__hailing_port__fhier_addr` dataframe.
is_empty <- c(NA, "NA", "", "UN", "N/A")

no_addr_vsl_ids <- 
  compl_corr_to_investigation__corr_date__hailing_port__fhier_addr |> 
  dplyr::filter(physical_address_1 %in% is_empty) |> 
  dplyr::select(vessel_official_number) |> 
  dplyr::distinct()

dplyr::n_distinct(no_addr_vsl_ids$vessel_official_number)
# 109
# 71
```

## From Oracle db 

```{r From Oracle db}
# From Oracle db ----
# Explanation:

# This code snippet processes the `db_participants_address` dataframe by filtering rows based on the `official_number` and ensuring distinct rows in the resulting dataframe.
# 
# 1. **Starting with the DataFrame:**
#    - `db_participants_address |>`: Starts with the `db_participants_address` dataframe and pipes it into the next function.
# 
# 2. **Filtering Rows:**
#    - `dplyr::filter(official_number %in% no_addr_vsl_ids$vessel_official_number)`:
#      - `dplyr::filter(...)`: The `filter` function from the `dplyr` package is used to keep rows that meet certain conditions.
#      - `official_number %in% no_addr_vsl_ids$vessel_official_number`: This condition keeps only the rows where the `official_number` is found in the `vessel_official_number` column of the `no_addr_vsl_ids` dataframe.
#        - `%in%`: The `%in%` operator checks if elements of `official_number` are present in `no_addr_vsl_ids$vessel_official_number`.
# 
# 3. **Removing Duplicate Rows:**
#    - `dplyr::distinct()`: The `distinct` function from the `dplyr` package removes duplicate rows from the filtered dataframe, ensuring each row is unique.
# 
# The result is a new dataframe `db_participants_address__needed` that contains only the rows from `db_participants_address` where the `official_number` is present in the `no_addr_vsl_ids$vessel_official_number` column, and all duplicate rows are removed.
db_participants_address__needed <-
  db_participants_address |>
  dplyr::filter(official_number %in% no_addr_vsl_ids$vessel_official_number) |>
  dplyr::distinct()

dim(db_participants_address__needed)
# [1] 139  37

dplyr::n_distinct(db_participants_address__needed$official_number)
# 71
```

### keep fewer columns 

```{r keep fewer columns}
## keep fewer columns ----
col_names_to_keep <-
  c(
    "official_number",
    "entity_name",
    "primary_email",
    # "is_primary",
    "ph_area",
    "ph_number",
    "entity_name",
    "physical_city",
    "physical_county",
    "physical_state",
    "physical_zip_code",
    "mailing_address1",
    "mailing_address2",
    "mailing_city",
    "mailing_county",
    "mailing_country",
    "mailing_state",
    "mailing_zip_code"
  )

# Explanation:

# This code snippet processes the `db_participants_address__needed` dataframe by selecting specific columns, removing duplicate rows, and arranging the rows based on the `official_number` column.
# 
# 1. **Creating a Regular Expression Pattern for Column Names:**
#    - `my_cols_ends <- paste0(col_names_to_keep, '$', collapse = '|')`:
#      - `col_names_to_keep`: This variable contains a list or vector of column name prefixes you want to keep.
#      - `paste0(...)`: This function concatenates the elements of `col_names_to_keep` with a `$` at the end of each element, creating a regular expression pattern to match column names that end with any of the specified prefixes.
#      - `collapse = '|'`: The `collapse` parameter ensures that the elements are joined by a `|`, which is the OR operator in regular expressions.
# 
# 2. **Selecting Specific Columns:**
#    - `db_participants_address__needed |>`: Starts with the `db_participants_address__needed` dataframe and pipes it into the next function.
#    - `dplyr::select(tidyselect::matches(my_cols_ends))`: Uses the `select` function from `dplyr` and the `matches` function from `tidyselect` to select columns whose names match the regular expression pattern stored in `my_cols_ends`.
# 
# 3. **Removing Duplicate Rows:**
#    - `dplyr::distinct()`: Removes duplicate rows from the selected columns.
# 
# The result is a new dataframe `db_participants_address__needed_short1` that contains only the columns matching the specified pattern, with duplicates removed.

my_cols_ends <- paste0(col_names_to_keep, 
                  '$', 
                  collapse = '|')

db_participants_address__needed_short <-
  db_participants_address__needed |>
  dplyr::select(tidyselect::matches(my_cols_ends)) |>
  dplyr::distinct()

# check
nrow(compl_corr_to_investigation__corr_date__hailing_port__fhier_addr)
# 199
dplyr::n_distinct(compl_corr_to_investigation__corr_date__hailing_port__fhier_addr$vessel_official_number)
# 199
# one vessel per row, OK

# have to combine rows
dim(db_participants_address__needed_short)
# 106
dplyr::n_distinct(db_participants_address__needed_short$official_number)
# 71
```

### combine area and phone numbers 

```{r combine area and phone numbers}
## combine area and phone numbers ----

# Explanation:

# This code creates a new dataframe `db_participants_address__needed_short__phone0` by modifying the `db_participants_address__needed_short` dataframe. It adds two new columns (`erv_phone` and `erb_phone`) that concatenate existing columns.
# 
# 1. **Starting with the Original Dataframe:**
#    - `db_participants_address__needed_short |>`: Begins with the `db_participants_address__needed_short` dataframe and pipes it into the next function.
# 
# 2. **Adding New Columns:**
#    - `dplyr::mutate(...)`: The `mutate` function from the `dplyr` package is used to add or modify columns in the dataframe.
#      - `erv_phone = paste0(erv_ph_area, erv_ph_number)`: Creates a new column `erv_phone` by concatenating the `erv_ph_area` and `erv_ph_number` columns using `paste0`, which combines strings without any separator.
#      - `erb_phone = paste0(erb_ph_area, erb_ph_number)`: Similarly, creates a new column `erb_phone` by concatenating the `erb_ph_area` and `erb_ph_number` columns.
# 
# The result is a new dataframe `db_participants_address__needed_short__phone0` that contains all the original columns from `db_participants_address__needed_short` plus two new columns (`erv_phone` and `erb_phone`) that contain concatenated phone numbers.

db_participants_address__needed_short__phone0 <- 
  db_participants_address__needed_short |> 
  dplyr::mutate(erv_phone = paste0(erv_ph_area, erv_ph_number),
         erb_phone = paste0(erb_ph_area, erb_ph_number))
```

### make erv and erb combinations 

```{r make erv and erb combinations}
## make erv and erb combinations ----
# <<<<
list_sort_uniq  <-  function(my_lists) {
  # browser()
  res <-
    my_lists |>
    stringr::str_trim() |>
    unique() |>
    sort() |>
    list() |>
    purrr::flatten()
  return(res)
}

# Explanations:

# Overall, this function ensures that the elements in the input list are cleaned, unique, and sorted, providing a tidy and organized output.
#  
#  
# The `list_sort_uniq` function is designed to take a list of character vectors, clean them, remove duplicates, sort them, and return a single vector with unique, sorted elements.
# 
# 1. **Function Definition:**
#    - `list_sort_uniq <- function(my_lists) { .. }`: Defines a function named `list_sort_uniq` with one argument, `my_lists`, representing the list of character vectors to process.
# 
# 2. **Data Transformation Pipeline:**
#    - `my_lists |> ...`: Utilizes the pipe operator (`|>`) to pass the `my_lists` argument through a series of data transformation steps.
#    
# 3. **Cleaning and Transformation:**
#    - `str_trim()`: Removes leading and trailing whitespace from each element of the list.
#    - `unique()`: Removes duplicate elements from the list.
#    - `sort()`: Sorts the elements of the list in ascending order.
#    - `list()`: Converts the sorted vector into a list.
#    - `flatten()`: Flattens the resulting list into a single vector.
# 
# 4. **Return Result:**
#    - The function returns the processed vector stored in `res`.
# 
# >>>>

col_part_names <-
  c(
    "entity_name",
    "primary_email",
    # "ph_is_primary",
    # "ph_area",
    # "ph_number",
    "physical_city",
    "physical_county",
    "physical_state",
    "physical_zip_code",
    "mailing_address1",
    "mailing_address2",
    "mailing_city",
    "mailing_county",
    # "mailing_country",
    "mailing_state",
    "mailing_zip_code"
  )

# Explanation:

# 1. **Mapping Over Column Parts:**
#    - `col_part_names |> purrr::map(\(curr_col_part) { ... })`: It iterates over each element in `col_part_names` using the `map` function from the purrr package. For each column part (`curr_col_part`), it executes the code inside the curly braces `{ ... }`.
# 
# 2. **Generating New Column Names:**
#    - `new_col_name <- stringr::str_glue("db_{curr_col_part}")`: It creates a new column name by combining the prefix "db_" with the current column part (`curr_col_part`) using `str_glue` from the stringr package.
# 
# 3. **Grouping and Mutating Data:**
#    - `db_participants_address__needed_short__phone0 |> dplyr::group_by(official_number) |> ...`: It groups the dataframe `db_participants_address__needed_short__phone0` by the column `official_number` using `group_by` from dplyr. Then, it proceeds with further data manipulation operations.
# 
# 4. **Applying Purrr::pmap Function:**
#    - `purrr::pmap(dplyr::across(dplyr::ends_with(curr_col_part)), ...)`: It applies the `pmap` function from the purrr package to iterate over columns that end with the current column part (`curr_col_part`). Within the `pmap` call, a custom function (`auxfunctions::list_sort_uniq`) is applied to each corresponding set of columns.
# 
# 5. **Ungrouping and Selecting Columns:**
#    - `... |> dplyr::ungroup() |> dplyr::select(-official_number)`: After the mutation step, it ungroups the dataframe and removes the `official_number` column using `ungroup()` and `select()` functions from dplyr, respectively.
# 
# 6. **Binding Columns Together:**
#    - `dplyr::bind_cols(db_participants_address__needed_short__phone0, .)`: Finally, it binds the original dataframe `db_participants_address__needed_short__phone0` with the transformed columns obtained from the mapping operation using `bind_cols` from dplyr.
# 
# This code dynamically generates new columns in the dataframe based on the provided column parts, applies a custom function to each set of corresponding columns, and then binds the resulting columns back to the original dataframe.

tictoc::tic("map all pairs")
db_participants_address__needed_short__erv_erb_combined3 <-
  col_part_names |>
  purrr::map(\(curr_col_part)  {
    new_col_name <- stringr::str_glue("db_{curr_col_part}")
    # cat(new_col_name, sep = "\n")
    
    db_participants_address__needed_short__phone0 |>
      dplyr::group_by(official_number) |>
      dplyr::mutate(!!new_col_name := 
                      purrr::pmap(dplyr::across(dplyr::ends_with(curr_col_part)),
                                    ~ auxfunctions::list_sort_uniq(.)),
             .keep = "none" ) |>
      dplyr::ungroup() |>
      dplyr::select(-official_number)
    
  }) %>%
  dplyr::bind_cols(db_participants_address__needed_short__phone0, .)
tictoc::toc()
# map all pairs: 14.31 sec elapsed
```

#### shorten 

```{r shorten}
### shorten ----

# Explanation:

# This code processes the `db_participants_address__needed_short__erv_erb_combined3` dataframe to create a new dataframe named `db_participants_address__needed_short__erv_erb_combined_short` by selecting specific columns and ensuring the rows are distinct.
# 
# 1. **Dataframe Selection and Transformation:**
#    - `db_participants_address__needed_short__erv_erb_combined3 |>`: Starts with the input dataframe `db_participants_address__needed_short__erv_erb_combined3` and pipes it into the subsequent functions.
# 
# 2. **Selecting Specific Columns:**
#    - `dplyr::select(official_number, tidyselect::all_of(tidyselect::starts_with("db_")))`: Uses `dplyr::select` to retain only the `official_number` column and any columns whose names start with "db_".
#      - `tidyselect::all_of(tidyselect::starts_with("db_"))`: Uses the `tidyselect` package to identify all column names that start with "db_". The `all_of` function ensures that the selected columns exist within the dataframe.
# 
# 3. **Ensuring Unique Rows:**
#    - `dplyr::distinct()`: Ensures that the resulting dataframe contains only unique rows, removing any duplicate rows based on the selected columns.
# 
# The result is a new dataframe `db_participants_address__needed_short__erv_erb_combined_short` that contains only the `official_number` column and columns starting with "db_", with all duplicate rows removed.
db_participants_address__needed_short__erv_erb_combined_short <-
  db_participants_address__needed_short__erv_erb_combined3 |>
  dplyr::select(official_number, 
                tidyselect::all_of(tidyselect::starts_with("db_"))) |>
  dplyr::distinct()

dim(db_participants_address__needed_short__erv_erb_combined_short)
# 94 17

dplyr::n_distinct(db_participants_address__needed_short__erv_erb_combined_short$official_number)
# 71

# check
# db_participants_address__needed_short__erv_erb_combined_short |> 
#   dplyr::filter(official_number == "1235397") |>
#   dplyr::glimpse()
# $ db_physical_city     <list> ["SOUTH ISLANDIA"], ["ISLANDIA"]
```

### combine similar fields 

```{r combine similar fields}
## combine similar fields ----

# Explanations:
# 1. Iterate over each participant column using 'col_part_names'.
#    - 'map' applies the provided function to each element of the list.
# 2. Define the old and new column names based on the current participant column.
#    - 'str_glue' is used for string interpolation to create column names.
# 3. Group the DataFrame by 'official_number' using 'group_by'.
# 4. For each group, create a new column with unique sorted values for the current participant.
#    - 'list_sort_uniq' ensures unique values and sorts them.
# 5. Ungroup the DataFrame and remove the 'official_number' column.
#    - 'ungroup' removes grouping structure.
#    - 'select' is used to exclude the 'official_number' column and keep only the new column.
# 6. Bind the resulting columns to 'db_participants_address__needed_short__erv_erb_combined_short'.
#    - 'bind_cols' combines columns horizontally.
# 7. Select only the 'official_number' and columns ending with '_u'.
# 8. Keep only distinct rows in the final DataFrame using 'distinct'.
# 9. The resulting DataFrame is stored in 'db_participants_address__needed_short__erv_erb_combined_short__u'.

db_participants_address__needed_short__erv_erb_combined_short__u_temp <-
  col_part_names |>
  purrr::map(\(curr_col_part)  {
    # browser()
    old_col_name <- stringr::str_glue("db_{curr_col_part}")
    new_col_name <- stringr::str_glue("db_{curr_col_part}_u")
    cat(new_col_name, sep = "\n")
    
    db_participants_address__needed_short__erv_erb_combined_short |>
      dplyr::group_by(official_number) |>
      dplyr::mutate(!!new_col_name := list(paste(sort(unique(stringr::str_trim(purrr::list_flatten(!!sym(old_col_name))))))),
             .keep = "none" ) |>
      dplyr::ungroup() |>
      dplyr::select(-official_number)
  })

# glimpse(db_participants_address__needed_short__erv_erb_combined_short)

# Explanation:
# 
# This code creates a new dataframe by combining columns with suffix "_u" from two existing dataframes (`db_participants_address__needed_short__erv_erb_combined_short` and `db_participants_address__needed_short__erv_erb_combined_short__u_temp`). 
# 
# 1. **Binding Columns Together:**
#    - `dplyr::bind_cols(...)`: It binds columns from two dataframes together. The columns from `db_participants_address__needed_short__erv_erb_combined_short` and `db_participants_address__needed_short__erv_erb_combined_short__u_temp` are combined horizontally.
# 
# 2. **Selecting Columns:**
#    - `dplyr::select(official_number, dplyr::all_of(dplyr::ends_with("_u")))`: After binding columns, it selects the `official_number` column along with all columns that end with "_u" using `select` from dplyr. 
#    
# 3. **Removing Duplicate Rows:**
#    - `dplyr::distinct()`: It removes duplicate rows from the dataframe to ensure each row is unique.
# 
# This code essentially creates a new dataframe containing selected columns from two existing dataframes and ensures that there are no duplicate rows in the resulting dataframe.

db_participants_address__needed_short__erv_erb_combined_short__u <-
  dplyr::bind_cols(
    db_participants_address__needed_short__erv_erb_combined_short,
    db_participants_address__needed_short__erv_erb_combined_short__u_temp
  ) |>
  dplyr::select(official_number, dplyr::all_of(dplyr::ends_with("_u"))) |> 
  dplyr::distinct()

# check
# db_participants_address__needed_short__erv_erb_combined_short__u |>
#   filter(official_number == "1235397") |>
#   glimpse()
```

#### convert to characters 

```{r convert to characters}
### convert to characters ----
# Explanation:
# 
# This code modifies the dataframe `db_participants_address__needed_short__erv_erb_combined_short__u` by concatenating the elements of list-type columns into a single string separated by semicolons. Here's a detailed explanation:
# 
# 1. **Row-wise Operation:**
#    - `dplyr::rowwise()`: It sets the dataframe to be processed row-wise, meaning each operation will be applied independently to each row.
# 
# 2. **Mutating List-type Columns:**
#    - `dplyr::mutate_if(is.list, ~ paste(unlist(.), collapse = '; '))`: This line applies a mutation to each column of the dataframe that is of list type. 
#      - `is.list`: Checks if a column is of list type.
#      - `paste(unlist(.), collapse = '; ')`: For each list-type column, it converts the list elements into a single string by unlisting them and concatenating them together with a semicolon as the separator.
# 
# 3. **Ungrouping:**
#    - `dplyr::ungroup()`: It removes the grouping previously applied to the dataframe, returning it to its original state.
# 
# This code effectively transforms list-type columns in the dataframe into character vectors, concatenating their elements into a single string with semicolons as separators.

db_participants_address__needed_short__erv_erb_combined_short__u_no_c <-
  db_participants_address__needed_short__erv_erb_combined_short__u |>
  dplyr::rowwise() |>
  dplyr::mutate_if(is.list, ~ paste(unlist(.), collapse = '; ')) |>
  dplyr::ungroup()

# check
# db_participants_address__needed_short__erv_erb_combined_short__u_no_c |>
#   filter(official_number == "1235397") |>
#   glimpse()
# $ db_mailing_state_u     <chr> "NY"
# $ db_mailing_city_u      <chr> "ISLANDIA; SOUTH ISLANDIA"
```

### rename fields 

```{r rename fields}
## rename fields ----

# Explanation:
# 
# This code renames the columns in the dataframe `db_participants_address__needed_short__erv_erb_combined_short__u_no_c` by removing the suffix "_u" from their names.
# 
# 1. **Renaming Columns:**
#    - `dplyr::rename_with( ~ stringr::str_replace(.x, pattern = "_u$", replacement = ""))`: It renames the columns of the dataframe using a function provided by `rename_with`.
#      - `~`: It indicates the start of an anonymous function.
#      - `stringr::str_replace(.x, pattern = "_u$", replacement = "")`: For each column name (`x`), it applies the `str_replace` function from the `stringr` package to replace the pattern "_u" at the end of the column name with an empty string, effectively removing it.
# 
# This code removes the "_u" suffix from the column names in the dataframe.

db_participants_address__needed_short__erv_erb_combined_short__u_ok <-
  db_participants_address__needed_short__erv_erb_combined_short__u_no_c |>
  dplyr::rename_with(~ stringr::str_replace(.x, pattern = "_u$", 
                                            replacement = ""))
```

## Join FHIER and Oracle db addresses 

```{r Join FHIER and Oracle db addresses}
# Join FHIER and Oracle db addresses ----
compl_corr_to_investigation__corr_date__hailing_port__fhier_addr__db_addr <-
  compl_corr_to_investigation__corr_date__hailing_port__fhier_addr |>
  dplyr::left_join(
    db_participants_address__needed_short__erv_erb_combined_short__u_ok,
    dplyr::join_by(vessel_official_number == official_number)
  )

# check
# compl_corr_to_investigation__corr_date__hailing_port__fhier_addr__db_addr |>
#   filter(vessel_official_number == "1235397") |>
#   glimpse()
# $ db_mailing_state       <chr> "NY"
# $ db_mailing_city        <chr> "ISLANDIA; SOUTH ISLANDIA"

cat("Result: ",
    "compl_corr_to_investigation__corr_date__hailing_port__fhier_addr__db_addr",
    sep = "\n")


# result: compl_corr_to_investigation__corr_date__hailing_port__fhier_addr__db_addr
```

### 3. mark vessels already in the know list 

```{r 3 mark vessels already in the know list}
## 3. mark vessels already in the know list ----
# The first column (report created) indicates the vessels that we have created a case for. My advice would be not to exclude those vessels. EOs may have provided compliance assistance and/or warnings already. If that is the case and they continue to be non-compliant after that, they will want to know and we may need to reopen those cases.

vessels_to_mark_ids <-
  prev_result |>
  dplyr::select(vessel_official_number)

dim(vessels_to_mark_ids)
```

##### mark these vessels 

```{r mark these vessels}
#### mark these vessels ----
# Explanations:
# Create a new column 'duplicate_w_last_time' in the dataframe 'compl_corr_to_investigation_short'.
# This column is marked with "duplicate" for rows where 'vessel_official_number' is present in the list of vessel IDs to mark as duplicates ('vessels_to_mark_ids').
# For all other rows, it is marked as "new".
compl_corr_to_investigation__corr_date__hailing_port__fhier_addr__db_addr__dup_marked <-
  compl_corr_to_investigation__corr_date__hailing_port__fhier_addr__db_addr |>
  dplyr::mutate(
    duplicate_w_last_time =
      dplyr::case_when(
        vessel_official_number %in%
          vessels_to_mark_ids$vessel_official_number ~ "duplicate",
        .default = "new"
      )
  )

dim(compl_corr_to_investigation__corr_date__hailing_port__fhier_addr__db_addr__dup_marked)
```

#### check 

```{r check}
### check ----
dplyr::n_distinct(compl_corr_to_investigation__corr_date__hailing_port__fhier_addr__db_addr__dup_marked$vessel_official_number)

compl_corr_to_investigation__corr_date__hailing_port__fhier_addr__db_addr__dup_marked |>
  dplyr::count(duplicate_w_last_time)
# 1 duplicate               108
# 2 new                      48
```

### 4. how many are duals? 

```{r 4 how many are duals}
## 4. how many are duals? ----
# Explanations:
# Create a new dataframe 
# Use the 'mutate' function to add a new column 'permit_region' based on conditions.
# If 'permitgroup' contains any of the specified patterns ("RCG", "HRCG", "CHG", "HCHG"),
# set 'permit_region' to "dual". Otherwise, set 'permit_region' to "sa_only".
# If none of the conditions are met, set 'permit_region' to "other".
# The resulting dataframe includes the original columns from 'compl_corr_to_investigation_short_dup_marked'
# along with the newly added 'permit_region' column.

compl_corr_to_investigation_short_dup_marked__permit_region <-
  compl_corr_to_investigation__corr_date__hailing_port__fhier_addr__db_addr__dup_marked |> 
  # compl_corr_to_investigation_short_dup_marked__permit_region__fhier_names__fhier_addr__mv_cols |>
  dplyr::mutate(permit_region =
           dplyr::case_when(
             grepl("RCG|HRCG|CHG|HCHG", permitgroup) ~ "dual",
             !grepl("RCG|HRCG|CHG|HCHG", permitgroup) ~ "sa_only",
             .default = "other"
           ))

# Explanations:
# Use the 'select' function to extract the columns 'vessel_official_number' and 'permit_region'
# from the dataframe 'compl_corr_to_investigation_short_dup_marked__permit_region'.
# Use the 'distinct' function to keep only unique combinations of 'vessel_official_number' and 'permit_region'.
# Use the 'count' function to count the occurrences of each unique 'permit_region'.
# The resulting count provides the frequency of each 'permit_region'.
region_counts <-
  compl_corr_to_investigation_short_dup_marked__permit_region |>
  dplyr::select(vessel_official_number, permit_region) |>
  dplyr::distinct() |>
  dplyr::count(permit_region)

dplyr::n_distinct(compl_corr_to_investigation_short_dup_marked__permit_region$vessel_official_number)
```

#### dual permitted cnts 

```{r dual permitted cnts}
### dual permitted cnts ----

region_counts$n[[1]] / (region_counts$n[[2]] + region_counts$n[[1]]) * 100
```

## Print out results 

```{r Print out results}
# Print out results ----
```

### add additional columns in front 

```{r add additional columns in front}
## add additional columns in front ----

additional_column_name1 <-
  stringr::str_glue(
    "Confirmed Egregious? (permits must still be active till {permit_expired_check_date}, missing past 6 months, and (1) they called/emailed us (incoming), or (2) at least 2 contacts (outgoing) with at least 1 call/other (voicemail counts) and at least 1 email)"
  )

# Explanation:
# 
# This code adds new columns to the dataframe `compl_corr_to_investigation_short_dup_marked__permit_region`. Here's what each part does:
# 
# 1. **Add Columns Function:**
#    - `tibble::add_column()`: This function from the `tibble` package is used to add new columns to a dataframe.
# 
# 2. **Column Specifications:**
#    - `!!(additional_column_name1) := NA`: Adds a new column named `additional_column_name1` filled with NA values.
#      - `!!`: This is a tidy evaluation feature that allows the use of non-standard evaluation. It evaluates the expression `additional_column_name1` dynamically.
#      - `:= NA`: Assigns NA values to the new column.
#    - `Notes = NA`: Adds another new column named "Notes" filled with NA values.
#    - `.before = 2`: Specifies that the new columns should be inserted before the second column in the dataframe.
# 
# This code effectively adds two new columns, "additional_column_name1" and "Notes", filled with NA values, to the dataframe.
compl_corr_to_investigation_short_dup_marked__permit_region__add_columns <-
  compl_corr_to_investigation_short_dup_marked__permit_region |>
  tibble::add_column(
    !!(additional_column_name1) := NA,
    Notes = NA,
    .before = 2
  )

# print_df_names(compl_corr_to_investigation_short_dup_marked__permit_region__add_columns)

# remove the "year" column, its value is the same for all rows
# compl_corr_to_investigation_short_dup_marked__permit_region__add_columns |> 
#   select(year) |> 
#   distinct()
# 1 2023, 2024

compl_corr_to_investigation_short_dup_marked__permit_region__add_columns <- 
  compl_corr_to_investigation_short_dup_marked__permit_region__add_columns |>
  select(-year)

out_file_name <-
  stringr::str_glue("egregious_violators_to_investigate_{lubridate::today()}.csv")

result_path <- 
  file.path(my_paths$outputs,
            current_project_basename,
            out_file_name)

compl_corr_to_investigation_short_dup_marked__permit_region__add_columns |>
readr::write_csv(result_path)

cat("Result:",
    "compl_corr_to_investigation_short_dup_marked__permit_region__add_columns",
    "and",
    out_file_name,
    sep = "\n")
```

